\lecture{25}{30 Nov.\ 10:30}{Hardness of Max-Cut}
\section{Hardness of Max-Cut}
In this section, we will see that if we assume the \hyperref[conj:unique-game]{unique game conjecture}, then \autoref{thm:max-cut} is actually the best we can do, i.e., we have the following.

\begin{theorem}[\cite{KKMO04}]\label{thm:max-cut-hardness}
	Assuming the \hyperref[conj:unique-game]{unique game conjecture}, for every \(\epsilon > 0 \), it's \(\NP\)-hard to approximate \hyperref[prb:max-cut]{max cut}  within a factor of \(\alpha _{\GW }+\epsilon \).
\end{theorem}

\begin{prev}
	Recall that the \hyperref[not:GW]{Goemans-Williamson constant} \(\alpha _{\GW }\) is defined as
	\[
		\alpha _{\GW } \coloneqq \max _{a\in [-1, 1]}\frac{\arccos (a) / \pi }{(1 - a) / 2} \approx 0.878,
	\]
	which is the approximation ratio achieved by \autoref{algo:max-cut-randomized-rounding}.
\end{prev}

Just like \hyperref[prb:max-3LIN]{3LIN}, we design a \hyperref[def:dictation]{dictator-ship} test, namely to create an instance (a graph) \(\mathcal{G} =(\mathcal{V} , \mathcal{E} )\) for \(\mathcal{V} \coloneqq \left\{ \pm 1 \right\} ^R\) such that
\begin{itemize}
	\item \emph{dictator cuts} (for some \(i\in [R]\), \(S = \{ x\in \left\{ \pm 1 \right\} ^R \colon x_i = 1 \} \)) get good value (cut many edges);
	\item cuts \emph{far from dictator} get small value (don't cut many edges).
\end{itemize}

Such a test is done is again done via noisy hypercube construction.

\subsection{Noisy Hypercube}\label{subsec:noisy-hypercube}
To start with, let \(\rho \in [-1, +1]\), let \(\mathcal{G} _{\rho} = (\mathcal{V} , \mathcal{E} )\) with edge-weight such that the total weight sums up to \(1\). This allows us to sample an edge by first sample \(x\in \left\{ \pm 1 \right\} ^R\) uniformly, and for all \(i\in [R]\), let
\[
	y_i = \begin{dcases}
		x_i,  & \text{ w.p.\ } (1 + \rho ) / 2 ; \\
		-x_i, & \text{ w.p.\ } (1 - \rho ) / 2,
	\end{dcases}
\]
and output \((x, y)\in \mathcal{E} \).

\begin{notation}
	For any \((x, y)\in \mathcal{E} \) in \(\mathcal{G} _\rho \), we denote \(x \underset{\rho }{\sim } y\).
\end{notation}

We see that for all \(i\in [R]\), \(\mathbb{E}_{x \underset{\rho }{\sim }y }\left[x_i y_i \right] = 1\cdot (1+\rho )/2 - 1\cdot (1 - \rho ) / 2 = \rho \).

\begin{note}
	At the end, we'll try \(\rho < 0\).
\end{note}

Let \(T_\rho \in \mathbb{R} ^{2^R \times 2^R}\) be the normalized adjacency matrix of \(\mathcal{G} _\rho \) such that
\[
	T_{\rho } (x, y)
	= \Pr_{}(\text{\(y\) is sampled} \mid \text{\(x\) is sampled} )
	= \Pr_{}(\text{\((x, y)\) is sampled} ) \cdot 2^R.
\]
We see that the sum of entries is \(2^R\), and rows and columns sum up to \(1\). Given a cut \((S, T)\), consider \(f\colon \mathcal{V} \to \left\{ \pm 1 \right\} \) such that
\[
	f(x) = \begin{dcases}
		1,  & \text{ if } x\in S ; \\
		-1, & \text{ if } x\in T .
	\end{dcases}
\]

\begin{claim}
	Given \((S, T)\), the total weight of edges cut by \((S, T)\) in \(\mathcal{G} _{\rho }\) is equal to
	\[
		\frac{1}{2} (1 - \left\langle f, T_\rho f \right\rangle ).
	\]
\end{claim}
\begin{explanation}
	The total weigh of cut edges is equal to
	\[
		\mathbb{E}_{x \underset{\rho }{\sim }y }\left[ \frac{1}{2} (1 - f(x) f(y)) \right]
		= \sum_{(x, y)\in \mathcal{E} } w(x, y)\cdot \frac{1}{2}(1 - f(x) f(y)).
	\]
	Then, since
	\[
		\mathbb{E}_{x \underset{\rho }{\sim } y}\left[f(x) f(y) \right]
		= \sum_{x\in \mathcal{V} } \sum_{y\in \mathcal{V} } \frac{1}{2^R} \Pr_{}(y\mid x) f(x) f(y)
		= \frac{1}{2^R} \sum_{x\in \mathcal{V} } f(x) \underbrace{\sum_{y\in \mathcal{V} } \Pr_{}(y\mid x) f(y)}_{(T_\rho f)(x) = \mathbb{E}_{x \underset{\rho }{\sim } y}\left[ f(y)\right] }
		= \left\langle f, T_{\rho } f \right\rangle,
	\]
	so we're done.
\end{explanation}

\begin{definition}[Stability]\label{def:stability}
	The \emph{stability} of \(f\) given \(\rho \) is defined as
	\[
		\Stab_\rho (f) \coloneqq \left\langle f, T_\rho f \right\rangle = \mathbb{E}_{x \underset{\rho }{\sim } y}\left[f(x)f(y) \right].
	\]
\end{definition}

\begin{remark}
	If \(f\) is \(\pm 1\)-valued, then \(\Stab_\rho (f) = 2\Pr_{x \underset{\rho }{\sim }y }(f(x) = f(y)) -1 \). So if \(f(x) \approx f(y)\) for \(y\) being the neighbor of \(x\) (i.e., \(x \underset{\rho }{\sim }y \)), this quantity is stable and close to \(1\).
\end{remark}

To study \(\Stab_\rho (f)\), consider \(S \subseteq [R]\) and \(\chi _S\), we have
\[
	\Stab_\rho (\chi _S)
	= \mathbb{E}_{x \underset{\rho }{\sim }y }\left[x^S y^S \right]
	= \prod_{i\in S} \mathbb{E}_{}\left[x_i y_i \right]
	= \rho ^{\vert S \vert }
\]
since \(\chi _S(x) = x^S = \prod_{i\in S}^{} x_i\). Now, since
\[
	f = \sum_{S \subseteq [R]} \hat{f} (S) \chi _S,
\]
we have
\[
	\left\langle f, T_{\rho }f \right\rangle
	= \sum_{S, T} \hat{f} (S) \hat{f} (T) \left\langle \chi _S, T_\rho \chi _T \right\rangle
	= \sum_{S\subseteq [R]} \hat{f} (S)^2 \cdot \rho ^{\vert S \vert }
\]
since \(T_\rho \chi _T = \rho ^{\vert T \vert }\cdot \chi _T\). So if \(f = \chi _i\) for some \(i\in [R]\), the cut value is equal to
\[
	\frac{1}{2} - \frac{1}{2} \Stab_\rho (f) = \frac{1}{2} - \frac{1}{2}\rho
\]
for \(\rho < 0\), so a dictator cuts indeed get good value.

\begin{problem*}
	How we define \emph{far from dictators} and prove they have small values?
\end{problem*}

\subsection{Majority is The Stablest Theorem}
Consider \(f\colon \left\{ \pm 1 \right\} ^R \to \left\{ \pm 1 \right\} \) as voting rules:
\begin{itemize}
	\item \(R\) people;
	\item \(x\in \left\{ \pm 1 \right\} ^R\) corresponds to a voting outcome;
	\item Society goes with \(f(x)\).
\end{itemize}

\begin{eg}[Majority]
	We can take \(f(x)\) to be the majority vote, i.e.,
	\[
		f(x) = \Maj(x) \coloneqq \sgn(x_1 + \dots + x_R ).
	\]
\end{eg}

\begin{eg}[Dictator]
	We can also take \(f(x)\) to be following a dictator \(i\), i.e.,
	\[
		f(x) = \operatorname{Dictator}_i(x) \coloneqq x_i.
	\]
\end{eg}

\begin{definition}[Influence]\label{def:influence}
	Given \(f\colon \left\{ \pm 1 \right\} ^R \to \left\{ \pm 1 \right\} \), for an \(i\in [R]\), the \emph{influence} \(\Inf_i(f)\) of \(i\) is defined as
	\[
		\Inf_i(f)
		\coloneqq \Pr_{x}(f(x) \neq f(x\oplus e_i))
		= \sum_{S\ni i}\hat{f} (S)^2.
	\]
\end{definition}

But one problem of \autoref{def:influence} is that the total \hyperref[def:influence]{influence} is not the same for all \(f\), since
\[
	\operatorname{I}(f)
	\coloneqq \sum_{i\in [R]} \Inf_i(f)
	= \sum_{S \subseteq [R]} \hat{f} (S)^2 \vert S \vert .
\]

\begin{note}
	Indeed, the total \hyperref[def:influence]{influence} can vary a lot, since \(\operatorname{I}(\chi _i) = 1\) while \(\operatorname{I} (\chi _{[R]})=\vert R \vert \).
\end{note}

To fix this issue, we define the so-called
\begin{definition}[\(\delta \)-noisy influence]\label{def:noisy-influence}
	Given \(f\colon \left\{ \pm 1 \right\} ^R \to \left\{ \pm 1 \right\} \) and \(\delta \in (0, 1)\), for an \(i\in [R]\), the \emph{\(\delta \)-noisy influence} \(\Inf_i^\delta (f)\) of \(i\) is defined as
	\[
		\Inf_i^\delta (f)
		\coloneqq \sum_{S\ni i}\hat{f} (S)^2 (1 - \delta )^{\vert S \vert - 1}.
	\]
\end{definition}

In this case, the total \hyperref[def:noisy-influence]{\(\delta \)-noisy influence} \(\operatorname{I}^\delta (f)\) is given by
\[
	\operatorname{I}^\delta (f) = \sum_{S \subseteq [R]} \hat{f} (S)^2 \vert S  (1 - \delta )\vert ^{\vert S \vert -1}.
\]

\begin{claim}
	We have \(\operatorname{I}^\delta (f) \leq 1 / \delta \lVert f \rVert _2^2\).
\end{claim}
\begin{explanation}
	We simply maximize \(\vert S \vert (1 - \delta )^{\vert S \vert -1}\) for every \(S\), where we simply have \(1 / \delta \).
\end{explanation}

Now, consider the majority vote \(\Maj\colon \left\{ \pm 1 \right\} ^R \to \left[ \pm 1 \right] \), we have
\[
	\Inf_i(\Maj) = \Theta (1 / \sqrt{R} ),\quad
	\operatorname{I}(\Maj) = \Theta (\sqrt{R} );
\]
while
\[
	\Inf_i^\delta (\Maj) = \Theta (1),\quad
	\operatorname{I}^\delta (\Maj) = \Theta (1 / R).
\]

Now, we say that \(i\) is \emph{influential} if \(\Inf_i^\delta (f) \geq \Omega (1).\)

\begin{claim}
	As \(R \to \infty \), \(\Stab_\rho (\Maj) \to 2 / \pi \cdot \arcsin \rho\).
\end{claim}
\begin{explanation}
	Firstly, let
	\[
		X = \frac{x_1 + \dots  + x_R}{\sqrt{R} },\quad
		Y = \frac{y_1 + \dots  + y_R}{\sqrt{R} },
	\]
	we have \(\mathbb{E}_{}\left[X \right] = \mathbb{E}_{}\left[Y \right] = 0\), \(\mathbb{E}_{}\left[X^2 \right] = \mathbb{E}_{}\left[Y^2 \right] = 1\) with \(\mathbb{E}_{}\left[XY \right] = \rho \), we have
	\[
		\begin{split}
			\Stab_\rho (\Maj)
			 & = \mathbb{E}_{x \underset{\rho }{\sim } y}\left[ \sgn (x_1 + \dots + x_R ) \cdot \sgn (y_1 + \dots +y_R )\right] \\
			 & = \mathbb{E}_{X, Y}\left[\sgn (X) \sgn (Y) \right]                                                               \\
			 & \approx \mathbb{E}_{g, h}\left[\sgn (g)\sgn (h) \right]
		\end{split}
	\]
	where \(g, h \sim \mathcal{N} (0, 1)\) are \(\rho \)-correlated by \(2\)-dimensional central limit theorem. Notice that we interpret the term \(\rho \)-correlated as \(g=\left\langle a, t \right\rangle = t_1\), \(h=\left\langle b, t \right\rangle \) where we sample \(t \coloneqq (t_1, t_2)\sim \mathcal{N} (0, 1)^2\).\footnote{We implicitly rotate \(t_1\) to the \(x\)-axis from \(g=\left\langle a, t \right\rangle = t_1\).} In this case, we have the same geometric interpretation as in \autoref{lma:lec14-2}, we have
	\[
		\Pr_{}(\sgn (g) = \sgn (h)) = \frac{\pi - \arccos \rho }{\pi }.
	\]
\end{explanation}

Finally, we need the following theorem, which is introduced in~\cite{KKMO04}, and confirmed in~\cite{MOO05} later.

\begin{theorem}[Majority is the stablest~\cite{MOO05, KKMO04}]\label{thm:MIST}
	Given \(0 < \rho < 1\) and \(\epsilon > 0\), and \(f\colon \left\{ \pm 1 \right\} ^n \to [-1, 1]\) with \(\mathbb{E}_{}\left[f \right] = 0\). Suppose \(\Inf_i^{1 / \log (1 / \epsilon )}(f)\leq \epsilon \), then
	\[
		\Stab_\rho (f) \leq \left( \frac{2}{\pi } \right) \arcsin \rho + O\left( \frac{\log \log 1 / \epsilon }{\log 1 / \epsilon } \right).
	\]
\end{theorem}