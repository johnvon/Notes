\lecture{25}{30 Nov. 10:30}{Hardness of Max-Cut}
\section{Hardness of Max-Cut}
In this section, we will see that if we assume the \hyperref[conj:unique-game]{unique game conjecture}, then \autoref{thm:max-cut} is actually the best we can do, i.e., we have the following.

\begin{theorem}[\cite{1366234}]\label{thm:max-cut}
	Assuming \hyperref[conj:unique-game]{unique game conjecture}, for every \(\epsilon > 0 \), it's \(\NP\)-hard to approximate \hyperref[prb:max-cut]{max cut}  within a factor of \(\alpha _{\mathrm{GW} }+\epsilon \).
\end{theorem}

\begin{prev}
	Recall that
	\[
		\alpha _{\mathrm{GW} } \coloneqq \max _{a\in [-1, 1]}\frac{\arccos (a) / \pi }{(1 - a) / 2} \approx 0.878
	\]
	is the approximation ratio achieved by \autoref{algo:max-cut-randomized-rounding}.
\end{prev}

Just like \hyperref[prb:max-3LIN]{3LIN}, we design a \hyperref[not:dictation]{dictator-ship} test, namely for \(\mathcal{\MakeUppercase{g}} =(\mathcal{\MakeUppercase{v}} , \mathcal{\MakeUppercase{e}} )\) for \(\mathcal{\MakeUppercase{v}} \coloneqq \left\{ \pm 1 \right\} ^R\),
\begin{itemize}
	\item \emph{dictator cuts} get good value;
	\item \emph{cuts far from dictator cuts} get small value.
\end{itemize}

\subsection{Noisy Hypercube}
To start with, let \(\rho \in [-1, +1]\), let \(\mathcal{\MakeUppercase{g}} _{\rho} = (\mathcal{\MakeUppercase{v}} , \mathcal{\MakeUppercase{e}} )\) with edge-weight such that the total weight sums up to \(1\). This allows us to sample an edge by first sample \(x\in \left\{ \pm 1 \right\} ^R\) uniformly, and for all \(i\in [R]\), let
\[
	y_i = \begin{dcases}
		x_i,  & \text{ w.p.\ } (1 + \rho ) / 2 ; \\
		-x_i, & \text{ w.p.\ } (1 - \rho ) / 2,
	\end{dcases}
\]
and output \((x, y)\in \mathcal{\MakeUppercase{e}} \).

\begin{notation}
	For any \((x, y)\in \mathcal{\MakeUppercase{e}} \) in \(\mathcal{\MakeUppercase{g}} _\rho \), we denote \(x \underset{\rho }{\sim } y\).
\end{notation}

We see that for all \(i\in [R]\), \(\mathbb{E}_{x \underset{\rho }{\sim }y }\left[x_i y_i \right] = 1\cdot (1+\rho )/2 - 1\cdot (1 - \rho ) / 2 = \rho \).

\begin{note}
	At the end, we'll try \(\rho < 0\).
\end{note}

Let \(T_\rho \in \mathbb{\MakeUppercase{r}} ^{2^R \times 2^R}\) be the normalized adjacency matrix of \(\mathcal{\MakeUppercase{g}} _\rho \) such that
\[
	T_{\rho } (x, y)
	= \Pr_{}(\text{\(y\) is sampled} \mid \text{\(x\) is sampled} )
	= \Pr_{}(\text{\((x, y)\) is sampled} ) \cdot 2^R.
\]
We see that the sum of entries is \(2^R\), and rows and columns sum up to \(1\). Given a cut \((S, T)\), consider \(f\colon \mathcal{\MakeUppercase{v}} \to \left\{ \pm 1 \right\} \) such that
\[
	f(x) = \begin{dcases}
		1,  & \text{ if } x\in S ; \\
		-1, & \text{ if } x\in T .
	\end{dcases}
\]

\begin{claim}
	Given \((S, T)\), the total weight of edges cut by \((S, T)\) in \(\mathcal{\MakeUppercase{g}} _{\rho }\) is equal to
	\[
		\frac{1}{2} (1 - \left\langle f, T_\rho f \right\rangle ).
	\]
\end{claim}
\begin{explanation}
	The total weigh of cut edges is equal to
	\[
		\mathbb{E}_{x \underset{\rho }{\sim }y }\left[ \frac{1}{2} (1 - f(x) f(y)) \right]
		= \sum_{(x, y)\in \mathcal{\MakeUppercase{e}} } w(x, y)\cdot \frac{1}{2}(1 - f(x) f(y)).
	\]
	Then, since
	\[
		\mathbb{E}_{x \underset{\rho }{\sim } y}\left[f(x) f(y) \right]
		= \sum_{x\in \mathcal{\MakeUppercase{v}} } \sum_{y\in \mathcal{\MakeUppercase{v}} } \frac{1}{2^R} \Pr_{}(y\mid x) f(x) f(y)
		= \frac{1}{2^R} \sum_{x\in \mathcal{\MakeUppercase{v}} } f(x) \underbrace{\sum_{y\in \mathcal{\MakeUppercase{v}} } \Pr_{}(y\mid x) f(y)}_{(T_\rho f)(x) = \mathbb{E}_{y \underset{\rho }{\sim } y}\left[ f(y)\right] }
		= \left\langle f, T_{\rho } f \right\rangle,
	\]
	so we're done.
\end{explanation}

\begin{definition}[Stability]\label{def:stability}
	The \emph{stability} of \(f\) given \(\rho \) is defined as
	\[
		\mathop{\mathrm{Stab}}_\rho (f) \coloneqq \left\langle f, T_\rho f \right\rangle = \mathbb{E}_{x \underset{\rho }{\sim } }\left[y(x)f(y) \right].
	\]
\end{definition}

\begin{remark}
	If \(f\) is \(\pm 1\)-valued, then \(\mathop{\mathrm{Stab}}_\rho (f) = 2\Pr_{x \underset{\rho }{\sim }y }(f(x) = f(y)) -1 \). So if \(f(x) \approx f(y)\) for \(y\) being the neighbor of \(x\) (i.e., \(x \underset{\rho }{\sim }y \)), this quantity is stable and close to \(1\).
\end{remark}

To study \(\mathop{\mathrm{Stab}}_\rho (f)\), consider \(S \subseteq [R]\) and \(\chi _S\), we have
\[
	\mathop{\mathrm{Stab}}_\rho (\chi _S)
	= \mathbb{E}_{x \underset{\rho }{\sim }y }\left[x^S y^S \right]
	= \prod_{i\in S} \mathbb{E}_{}\left[x_i y_i \right]
	= \rho ^{\vert S \vert }
\]
since \(\chi _S(x) = x^S = \prod_{i\in S}^{} x_i\). Now, since
\[
	f = \sum_{S \subseteq [R]} \hat{f} (S) \chi _S,
\]
we have
\[
	\left\langle f, T_{\rho }f \right\rangle
	= \sum_{S, T} \hat{f} (S) \hat{f} (T) \left\langle \chi _S, T_\rho \chi _T \right\rangle
	= \sum_{S\subseteq [R]} \hat{f} (S)^2 \cdot \rho ^{\vert S \vert }
\]
since \(T_\rho \chi _T = \rho ^{\vert T \vert }\cdot \chi _T\). So if \(f = \chi _i\) for some \(i\in [R]\), the cut value is equal to
\[
	\frac{1}{2} - \frac{1}{2} \mathop{\mathrm{Stab}}_\rho (f) = \frac{1}{2} - \frac{1}{2}\rho
\]
for \(\rho < 0\), so a dictator cuts indeed get good value.

\begin{problem*}
	How we define \emph{far from dictators} and prove they have small values?
\end{problem*}

\subsection{Majority is The Stablest Theorem}
Consider \(f\colon \left\{ \pm 1 \right\} ^R \to \left\{ \pm 1 \right\} \) as voting rules:
\begin{itemize}
	\item \(R\) people;
	\item \(x\in \left\{ \pm 1 \right\} ^R\) corresponds to a voting outcome;
	\item Society goes with \(f(x)\).
\end{itemize}

\begin{eg}[Majority]
	We can take \(f(x)\) to be the majority vote, i.e.,
	\[
		f(x) = \mathop{\mathrm{Maj}}(x) \coloneqq \sgn(x_1 + \ldots + x_R ).
	\]
\end{eg}

\begin{eg}[Dictator]
	We can also take \(f(x)\) to be following a dictator \(i\), i.e.,
	\[
		f(x) = \mathop{\mathrm{Dictator}}\nolimits_i(x) \coloneqq x_i.
	\]
\end{eg}

\begin{definition}[Influence]\label{def:influence}
	Given \(f\colon \left\{ \pm 1 \right\} ^R \to \left\{ \pm 1 \right\} \), for an \(i\in [R]\), the \emph{influence} \(\mathop{\mathrm{Inf}}\nolimits_i(f)\) of \(i\) is defined as
	\[
		\mathop{\mathrm{Inf}}\nolimits_i(f)
		\coloneqq \Pr_{x}(f(x) \neq f(x\oplus e_i))
		= \sum_{S\ni i}\hat{f} (S)^2.
	\]
\end{definition}

But one problem of \autoref{def:influence} is that the total \hyperref[def:influence]{influence} is not the same for all \(f\), since
\[
	\mathop{\mathrm{I}}(f)
	\coloneqq \sum_{i\in [R]} \mathop{\mathrm{Inf}}\nolimits_i(f)
	= \sum_{S \subseteq [R]} \hat{f} (S)^2 \vert S \vert .
\]

\begin{note}
	Indeed, the total \hyperref[def:influence]{influence} can vary a lot, since \(\mathop{\mathrm{I}}(\chi _i) = 1\) while \(I(\chi _{[R]})=\vert R \vert \).
\end{note}

To fix this issue, we define the so-called
\begin{definition}[\(\delta \)-noisy influence]\label{def:noisy-influence}
	Given \(f\colon \left\{ \pm 1 \right\} ^R \to \left\{ \pm 1 \right\} \) and \(\delta \in (0, 1)\), for an \(i\in [R]\), the \emph{\(\delta \)-noisy influence} \(\mathop{\mathrm{Inf}}_i^\delta (f)\) of \(i\) is defined as
	\[
		\mathop{\mathrm{Inf}}\nolimits_i^\delta (f)
		\coloneqq \sum_{S\ni i}\hat{f} (S)^2 (1 - \delta )^{\vert S \vert - 1}.
	\]
\end{definition}

In this case, the total \hyperref[def:noisy-influence]{\(\delta \)-noisy influence} \(\mathop{\mathrm{I}}^\delta (f)\) is given by
\[
	\mathop{\mathrm{I}}\nolimits^\delta (f) = \sum_{S \subseteq [R]} \hat{f} (S)^2 \vert S  (1 - \delta )\vert ^{\vert S \vert -1}.
\]

\begin{claim}
	We have \(\mathop{\mathrm{I}}^\delta (f) \leq 1 / \delta \lVert f \rVert _2^2\).
\end{claim}
\begin{explanation}
	We simply maximize \(\vert S \vert (1 - \delta )^{\vert S \vert -1}\) for every \(S\), where we simply have \(1 / \delta \).
\end{explanation}

Now, consider the majority vote \(\mathop{\mathrm{Maj}}\colon \left\{ \pm 1 \right\} ^R \to \left[ \pm 1 \right] \), we have
\[
	\mathop{\mathrm{Inf}}\nolimits_i(\mathop{\mathrm{Maj}}) = \Theta (1 / \sqrt{R} ),\quad
	\mathop{\mathrm{I}}(\mathop{\mathrm{Maj}}) = \Theta (\sqrt{R} );
\]
while
\[
	\mathop{\mathrm{Inf}}\nolimits_i^\delta (\mathop{\mathrm{Maj}}) = \Theta (1),\quad
	\mathop{\mathrm{I}}\nolimits^\delta (\mathop{\mathrm{Maj}}) = \Theta (1 / R).
\]

Now, we say that \(i\) is \emph{influential} if \(\mathop{\mathrm{Inf}}_i^\delta (f) \geq \Omega (1).\)

\begin{claim}
	As \(R \to \infty \), \(\mathop{\mathrm{Stab}}_\rho (\mathop{\mathrm{Maj}}) \to 2 / \pi \cdot \arcsin \rho\).
\end{claim}
\begin{explanation}
	Firstly, let
	\[
		X = \frac{x_1 + \ldots  + x_R}{\sqrt{R} },\quad
		Y = \frac{y_1 + \ldots  + y_R}{\sqrt{R} },
	\]
	we have \(\mathbb{E}_{}\left[X \right] = \mathbb{E}_{}\left[Y \right] = 0\), \(\mathbb{E}_{}\left[X^2 \right] = \mathbb{E}_{}\left[Y^2 \right] = 1\) with \(\mathbb{E}_{}\left[XY \right] = \rho \), we have
	\[
		\begin{split}
			\mathop{\mathrm{Stab}}\nolimits_\rho (\mathop{\mathrm{Maj}})
			&= \mathbb{E}_{x \underset{\rho }{\sim } y}\left[ \sgn (x_1 + \ldots + x_R ) \cdot \sgn (y_1 + \ldots +y_R )\right]\\
			&= \mathbb{E}_{X, Y}\left[\sgn (X) \sgn (Y) \right]\\
			&\approx \mathbb{E}_{g, h}\left[\sgn (g)\sgn (h) \right]
		\end{split}
	\]
	where \(g, h \sim \mathcal{\MakeUppercase{n}} (0, 1)\) are \(\rho \)-correlated by \(2\)-dimensional central limit theorem. Notice that we interpret the term \(\rho \)-correlated as \(g=\left\langle a, t \right\rangle = t_1\), \(h=\left\langle b, t \right\rangle \) where we sample \(t \coloneqq (t_1, t_2)\sim \mathcal{\MakeUppercase{n}} (0, 1)^2\).\footnote{We implicitly rotate \(t_1\) to the \(x\)-axis from \(g=\left\langle a, t \right\rangle = t_1\).} In this case, we have the same geometric interpretation as in \autoref{lma:lec14-2}, we have
	\[
		\Pr_{}(\sgn (g) = \sgn (h)) = \frac{\pi - \arccos \rho }{\pi }.
	\]
\end{explanation}

Finally, we need the following theorem, which is introduced in~\cite{1366234}, and confirmed in~\cite{https://doi.org/10.48550/arxiv.math/0503503} later.

\begin{theorem}[Majority is the Stablest~\cite{https://doi.org/10.48550/arxiv.math/0503503, 1366234}]\label{thm:MIST}
	Given \(0 < \rho < 1\) and \(\epsilon > 0\), and \(f\colon \left\{ \pm 1 \right\} ^n \to [-1, 1]\) with \(\mathbb{E}_{}\left[f \right] = 0\). Suppose \(\mathop{\mathrm{Inf}}_i^{1 / \log (1 / \epsilon )}(f)\leq \epsilon \), then
	\[
		\mathop{\mathrm{Stab}}\nolimits_\rho (f) \leq \left( \frac{2}{\pi } \right) \arcsin \rho + O\left( \frac{\log \log 1 / \epsilon }{\log 1 / \epsilon } \right).
	\]
\end{theorem}