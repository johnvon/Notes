\lecture{7}{21 Sep. 10:30}{\(k\)-Median and LMP Approximation}
\section{\(k\)-Median}

Let's look at another \hyperref[prb:clustering]{clustering} problem.

\begin{problem}[\(k\)-median]\label{prb:k-median}
Given a \hyperref[def:metric]{metric space} \((X, d)\) and \(P, Q\subseteq X\) with \(k\in \mathbb{\MakeUppercase{n}} \), find \(Q^\prime \subseteq Q\) with \(\left\vert Q^\prime  \right\vert = k\) which minimizes \(\sum_{i\in P} \min _{j\in Q^\prime } d(i, j)\).
\end{problem}

The natural linear programming for \autoref{prb:k-median} is the following. Consider \(\left\{ x_{ij}  \right\}_{i\in P, j\in Q} \) and \(\left\{ y_j \right\}_{j\in Q} \), then
\begin{align*}
	\min~ & \sum_{ij} x_{ij} d(i, j)                                                \\
	      & \sum_{j} x_{ij} \geq 1   & \forall i\in P         &  & (\alpha _i)      \\
	      & x_{ij} \leq y_j          & \forall i\in P, j\in Q &  & (\beta _{ij})    \\
	      & \sum_{j} y_j \leq k      &                        &  & (f)\footnotemark \\
	      & x, y\geq 0
\end{align*}
\footnotetext{Notice that compare to \autoref{prb:facility-location}, \(f\) here is a variable but not a given facility cost! The reason why we do this will be clear soon.}

\begin{intuition}
	We interpret \(x_{ij} \) as follows: if \(x_{ij} = 1\), then \(i\) belongs to \(j\). And \(y_j=1\) if \(j\) is the actual median we choose (i.e., in \(Q^\prime \)). As for constraints, both \(\sum_{j} x_{ij} \geq 1\) and \(\sum_{j} y_j \leq k\) are clear, while for \(x_{ij} \leq y_j\), we see that it can't be the case that \(x_{ij} = 1\) while \(y_j = 0\), i.e., we can't have the case that \(x_{ij} \) belongs to \(j\) while \(j\) isn't even in \(Q^\prime \).
\end{intuition}

The dual is then
\begin{align*}
	\max~ & \sum_{i} \alpha _i - kf                                    \\
	      & \sum_{i} \beta _{ij} \leq d(i, j) & \forall i\in P, j\in Q \\
	      & \sum_{i} \beta _{ij} \leq f       & \forall j\in Q         \\
	      & \alpha , \beta \geq 0
\end{align*}

\begin{note}
	Notice that this is exactly the dual as \autoref{prb:facility-location}, except that we now have an additional \(-kf\) term in the objective function. Although \(f\) is not included in the statement of \autoref{prb:k-median}, by denoting one of the dual variable \(f\), we get a similar formulation compare to \autoref{prb:facility-location}.
\end{note}

Due to the similarity between \autoref{prb:k-median} and \autoref{prb:facility-location}, we can try to use \autoref{algo:facility-location-greedy-III} which solves \autoref{prb:facility-location} with \(2\)-\hyperref[def:LMP]{LMP} guarantee. But note that in \autoref{prb:facility-location}, we need to specify \(f\). Suppose we guessed \(f\), and we run a \(\gamma\)-\hyperref[def:LMP]{LMP approximation} algorithm and somehow get \(k^\prime = k\). Then we have
\[
	\frac{\mathop{\mathrm{conn}}(\mathrm{ALG})}{\gamma } \leq \sum_{i} \alpha _i - kf \leq \OPT_{\text{\(k\)-med.}},
\]
i.e., this is a \(\gamma \)-approximation algorithm. So now, the task is to guess \(f\) such that the algorithm gives exactly \(k\) centers.

\subsection{Bipoint Rounding}
Turns out that we don't have ideas about the relation between \(k\) and \(f\), the only thing we know is that if \(f\to \infty \), \(k\) decreases, other than that it behaves quite arbitrary.

\begin{remark}
	The relation between \(k\) and \(f\) indeed highly depends on what algorithm we use. But at least for \autoref{algo:facility-location-greedy-III}, nobody knows anything in this case.
\end{remark}

Given this fact, just randomly guess one \(f\) doesn't work. A new idea is then to maintain two solutions (or interval) \([f^2, f^1]\) such that \(f^2 \leq f^1\), where
\begin{itemize}
	\item At \(f^2\), the algorithm opens \(k^2 \geq k\) facilities.
	\item At \(f^1\), the algorithm opens \(k^1 \leq k\) facilities.
\end{itemize}
Then, by binary search, we can get \(f_2 \leq f_1\) such that
\[
	\left\vert f^1 - f^2 \right\vert \leq \frac{\epsilon \OPT}{n}.
\]
Now, if \(a\in [0, 1]\) and \(b \coloneqq 1 - a\), we have \(k \coloneqq ak^1 + bk^2\) where \(k^1 \leq k \leq k^2\). Denote \(C^i\) as the connection cost with \(f^i\) such that \(C^1 \geq C^2\), we have
\[
	\begin{dcases}
		C^1 + \gamma k^1 f^1 \leq \gamma \sum_{i} \alpha _i^1, & \quad (\times a) \\
		C^2 + \gamma kr2 f^2 \leq \gamma \sum_{i} \alpha _i^2, & \quad (\times b)
	\end{dcases}
\]
hence,
\[
	aC^1 + bC^2
	\leq \gamma \left( a \sum_{i} \alpha _i^1 + b \sum_{i} \alpha _i^2 - ak^1 f^1 - bk^2 f^2 \right)
	\leq \gamma \underbrace{\left( \sum_{i} \alpha _i - kf \right)}_{\leq \OPT_{\text{\(k\)-med.}}} + \underbrace{\vphantom{\left( \sum_{i} \alpha _i - kf \right)}\gamma k \left\vert f^1 - f^2 \right\vert}_{\epsilon \OPT_{\text{\(k\)-med.}}},
\]
where we set \(\alpha \coloneqq a \alpha ^1 + b \alpha ^2\) and \(f \coloneqq \max (f^1, f^2)\).

\begin{note}
	\((\alpha , f)\) is dual-feasible for \autoref{prb:k-median}.
\end{note}

\begin{definition}[Bipoint solution]\label{def:bipoint-solution}
	Given \(F^1\), \(F^2\) with \(\left\vert F^1 \right\vert = k^1\), \(\left\vert F^2 \right\vert = k^2\) and \(k=ak^1 + bk^2\) for \(a, b\in [0, 1]\), \(a + b = 1\). If the connection cost of \(aF^1 + b F^2\) satisfies
	\[
		a\cdot \mathop{\mathrm{conn}}(F^1) + b\cdot \mathop{\mathrm{conn}}(F^2) \leq \gamma \cdot \OPT_{\text{\(k\)-med.}},
	\]
	we then call such a solution a \emph{bipoint solution}.
\end{definition}

\begin{remark}[\(\delta \)-bipoint rounding]
	For \(F^1\) and \(F^2\), output \(F\) with \(\left\vert F \right\vert = k\) such that
	\[
		\mathop{\mathrm{conn}}(F) \leq \delta \cdot (a C^1 + b C^2).
	\]
\end{remark}

Back to \autoref{prb:k-median}, we see that we can actually get a \(2\)-bipoint rounding as follows. For \(j\in F^1\), let \(\pi (j)\) be the closest facility in \(F^2\) to \(j\). Let \(F^{\ast} = \left\{ j^\prime \in F^2 \colon j^\prime =\pi (j) \text{ for some }j\in F^1  \right\} \). We see that \(\left\vert F^{\ast}  \right\vert \leq k^1\) since if \(\left\vert F^{\ast}  \right\vert < k^1\), then we add arbitrary centers so that \(\left\vert F^{\ast}  \right\vert = k^1\).

To open the facilities as what we want, consider the following rounding algorithm.
\begin{enumerate}[(a)]
	\item Open \(F^1\) with probability \(a\), otherwise open \(F^{\ast} \).
	\item Randomly choose \(k - k_1\) far from \(F^2 \setminus F^{\ast} \).
\end{enumerate}

Then,
\begin{itemize}
	\item \(j\in F^1\), \(\Pr(\text{\(j\) open} )= a\)
	\item \(j\in F^{\ast}\), \(\Pr(\text{\(j\) open} )= b\)
	\item \(j\in F^2 \setminus F^{\ast}\), \(\Pr(\text{\(j\) open} )= \frac{k-k_1}{k_2 - k_1}= b\)
\end{itemize}

Now, fix \(i\in P\) and let \(d_j \coloneqq d(i, j)\), then we have
\begin{table}[H]
	\centering
	\begin{tabular}{c|c|c}
		\toprule
		                               & distance       & probability                  \\
		\midrule
		\(j^2\) open                   & \(d_2\)        & \(b\)                        \\
		\(j^2\) not open, \(j^1\) open & \(d_1\)        & \(\geq (a-b)^+ \eqqcolon M\) \\
		none of \(j^1, j^2\) open      & \(2d_1 + d_2\) & \(\leq 1 - b - M\)           \\
		\bottomrule
	\end{tabular}
\end{table}

Then, the expected cost is just
\[
	\mathbb{E}\left[\text{\(i\)'s connected cost}\right] \leq b d_2 + Md_1 + (1 - b - M) (2d_1 + d_2).
\]

\begin{itemize}
	\item If \(b \geq a\), then \(b \geq 1 / 2\), \(M = 0\) and
	      \[
		      \mathbb{E}\left[\text{\(i\)'s connected cost}\right] \leq b - d_2 + (1 - b)	(2 d_1 + d_2) = 2ad_1 + d_2 \leq 2(ad_1 + bd_2).
	      \]
	\item If \(a > b\), then \(a > 1 / 2\), \(M = a - b\) and
	      \[
		      \mathbb{E}\left[\text{\(i\)'s connected cost}\right] \leq bd_2 ( (a - b))d_1 + b(2d_1 + d_2) = d_1(a+b)+d_2(b+d) \leq 2(ad_1 + bd_2).
	      \]
\end{itemize}

\begin{remark}[SOTA]
	The SOTA result
\end{remark}