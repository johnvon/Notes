\lecture{16}{26 Oct. 10:30}{Lasserre Hierarchy Continued}
After defining \(\mathop{\mathrm{Lass}}_R(\mathcal{\MakeUppercase{p}} )\), we now analyze what kind of properties this hierarchy has.

\begin{remark}
	Up to this time, we have seen the following.
	\begin{enumerate}[(a)]
		\item \(\mathop{\mathrm{Lass}}_R(\mathcal{\MakeUppercase{p}})\) is a convex program with \(2^R n^{O(R)}\) variables and constraints.
		\item We can solve this in \(n^{O(R)}\) time.
		\item \(\mathop{\mathrm{Lass}}_2(\mathcal{\MakeUppercase{p}})\) is equivalent to a basic \hyperref[def:SDP]{SDP}, and \(\mathop{\mathrm{Lass}}_n(\mathcal{\MakeUppercase{p}})\) is equivalent to an IP.
	\end{enumerate}
\end{remark}


\subsection{Probabilistic Consequences}
Consider
\begin{itemize}
	\item \(\mathop{\mathrm{Var}}_{\widetilde{P} _i}\left[ X_i \right] = \mathbb{E}_{\widetilde{P} _i}\left[ X_i ^{2} \right] - \mathbb{E}_{\widetilde{P} _i}\left[ X_i \right]^{2}  \)
	\item \(\mathop{\mathrm{Cov}}_{\widetilde{P} _{ij}}\left[ X_i, X_j\right] = \mathbb{E}_{\widetilde{P} _{ij}}\left[ X_i X_j\right] - \mathbb{E}_{\widetilde{P} _{i}}\left[ X_i \right] \mathbb{E}_{\widetilde{P} _{j}}\left[ X_j\right]  \).
\end{itemize}
We see that we can do a conditioning: let \(\widetilde{P} \coloneqq \{ \widetilde{P} _{A} \} _{\left\vert A \right\vert \leq R}\) be an \(R\)-\hyperref[def:local-distribution]{local distribution}. Now, fix \(S \subseteq [n]\), \(\left\vert S \right\vert = t\), let \(\alpha _S = \left\{ 0, 1 \right\} ^{\left\vert S \right\vert }\), condition on \(\widetilde{P} \), we get
\[
	\widetilde{P} ^\prime = \widetilde{P} \mid X_S \gets \alpha _S,
\]
where \(X_S\gets \alpha _S\) means \(X_i \gets \alpha _i\) for all \(i\in S\).
\begin{remark}
	\(\widetilde{P} ^\prime \) is a \((R-t)\)-\hyperref[def:local-distribution]{local distribution}.
\end{remark}
\begin{explanation}
	For all \(A \subseteq [n]\), \(\left\vert A \right\vert \leq R-t\), we have
	\[
		\widetilde{P} ^\prime _A (X_A = \theta _A) = \frac{\widetilde{P} (X_A = \theta _A, X_S = \alpha _S)}{\widetilde{P} (X_S = \alpha _S)}.
	\]
\end{explanation}

Apart from this, we also see that
\begin{enumerate}[(a)]
	\item \(\widetilde{P} ^\prime \) is \((R-t)\)-wise locally consistent.
	\item If \(\widetilde{P} \) was \(\mathop{\mathrm{Lass}}_R(\mathcal{\MakeUppercase{p}})\), \(\widetilde{P} ^\prime \) is feasible for \(\mathop{\mathrm{Lass}}_{R-t} (\mathcal{\MakeUppercase{p}})\).
\end{enumerate}

\begin{lemma}[Conditioning reduces variance]\label{lma:conditioning-reduces-variance}
	For all \(i, j\in [n]\),
	\[
		\mathop{\mathrm{Var}}\nolimits_{\widetilde{P} _i}\left[X_i \right] - \mathbb{E}_{X_j\sim \widetilde{P} _j}\left[\mathop{\mathrm{Var}}\nolimits_{\widetilde{P} _{ij}}\left[X_i \mid X_j \right]  \right] \geq 4\mathop{\mathrm{Cov}}\nolimits_{\widetilde{P} _{ij}}\left[ X_i, X_j\right] ^{2} .
	\]
\end{lemma}
\begin{proof}
	From of law of total variance, we have
	\[
		\mathop{\mathrm{Var}}\nolimits_{\widetilde{P} _i}\left[X_i \right]  - \mathbb{E}_{X_j}\left[\mathop{\mathrm{Var}}\nolimits_{\widetilde{P} }\left[X_i \mid X_j \right]  \right] = \mathop{\mathrm{Var}}\nolimits_{\widetilde{P} _{j}}\left[\mathbb{E}_{\widetilde{P} _j}\left[X_i \mid X_j \right]  \right] .
	\]
	Now, let \(P_i = \widetilde{P} _i(X_i = 1)\), \(P_j= \widetilde{P} _j(X_j = 1)\), \(P_{ij}= \widetilde{P} _{ij}(X_i=1, X_j=1) \), we have
	\[
		\begin{split}
			\mathop{\mathrm{Var}}\nolimits_{X_j}\left[\mathbb{E}_{\widetilde{P} }\left[X_i \mid X_j \right] \right]
			&= \mathbb{E}_{X_j}\left[\mathbb{E}_{\widetilde{P} }\left[X_i \mid X_j \right] ^{2} \right] - \left( \mathbb{E}_{X_j}\left[ \mathbb{E}_{}\left[X_i\mid X_j \right] \right]  \right)^{2} \\
			&=\widetilde{P} _j(X_j = 1)\cdot \frac{\mathbb{E}_{\widetilde{P} }\left[X_i X_j \right]^{2}}{\widetilde{P} _j(X_j = 1)^{2}} + \widetilde{P} _j(X_j = 0)\cdot \frac{\mathbb{E}_{\widetilde{P} }\left[X_i (1-X_j) \right]^{2}}{\widetilde{P} _j(X_j = 0)^{2}} - \mathbb{E}_{\widetilde{P} }\left[ X_i\right] ^{2} \\
			&=\frac{P_{ij} ^{2} }{P_j} + \frac{(P_i - P_{ij} )^{2} }{1-P_j}- P_i ^{2} \\
			&=\frac{1}{P_j(1-P_j)} \left( P_{ij}^{2} (1-P_j) + (P_i - P_{ij} )^{2} \cdot P_j - P_i ^{2} P_j(1-P_j)  \right)\\
			&= \frac{(P_{ij} - P_i P_j)^{2} }{P_j(1-P_j)}\\
			&= \frac{\left( \mathbb{E}_{\widetilde{P} }\left[ X_i X_j\right] - \mathbb{E}_{\widetilde{P} _i}\left[ X_i\right] \mathbb{E}_{\widetilde{P} _j}\left[X_j \right] \right) ^{2} }{\mathbb{E}_{}\left[X_j^2 \right] - \mathbb{E}_{}\left[X_j \right] ^{2} }\\
			&= \frac{\mathop{\mathrm{Cov}}\nolimits_{\widetilde{P} }\left[X_i, X_j \right]^{2}  }{\mathop{\mathrm{Var}}\nolimits_{\widetilde{P} _i}\left[X_j \right] }.
		\end{split}
	\]
	Since \(X_i\) are \(0\)-\(1\) variable, the variance in the denominator is less than \(1 / 4\), hence we finally have
	\[
		\mathop{\mathrm{Var}}\nolimits_{\widetilde{P} _i}\left[X_i \right]  - \mathbb{E}_{X_j}\left[\mathop{\mathrm{Var}}\nolimits_{\widetilde{P} }\left[X_i \mid X_j \right]  \right] = \mathop{\mathrm{Var}}\nolimits_{X_j}\left[ \mathbb{E}_{\widetilde{P} }\left[X_i \mid X_j \right] \right] \geq 4\cdot \mathop{\mathrm{Cov}}\nolimits_{\widetilde{P} }\left[X_i, X_j \right]^{2} .
	\]
\end{proof}

\begin{corollary}\label{col:lec16}
	Suppose \(\widetilde{P} =\{ \widetilde{P} _A \}_{\left\vert A \right\vert \leq R} \) is an \(R\)-\hyperref[def:local-distribution]{local distribution} which is \(\mathop{\mathrm{Lass}}_R(\mathcal{\MakeUppercase{p}})\) feasible, then
	\[
		\mathbb{E}_{j\sim [n]} \mathbb{E}_{X_j\sim \widetilde{P} _j}\left[ \mathbb{E}_{i\sim [n]}\left[\mathop{\mathrm{Var}}\nolimits_{\widetilde{P} _i}\left[X_i \right]\right] - \mathbb{E}_{i\sim [n]}\left[\mathop{\mathrm{Var}}\nolimits_{\widetilde{P} _{ij}}\left[X_i \mid X_j \right]\right] \right] \geq 4 \mathbb{E}_{i, j\sim [n]}\left[\mathop{\mathrm{Cov}}\nolimits_{\widetilde{P} _{ij}}\left[ X_i, X_j\right]^{2} \right].
	\]
	Furthermore, given \(a\in \mathbb{\MakeUppercase{r}} ^+\), either one of the following will happen.
	\begin{enumerate}[(a)]
		\item \(\mathbb{E}_{i, j\sim [n]}\left[\mathop{\mathrm{Cov}}\nolimits_{\widetilde{P} _{ij}}\left[ X_i, X_j\right]^{2} \right] \leq a\).
		\item \(\exists j\in [n]\), \(\theta _j\in \left\{ 0, 1 \right\} \), \(\widetilde{P} ^\prime \coloneqq \widetilde{P} \mid X_j \gets \theta _j\) satisfies \(\mathbb{E}_{i\sim [n]}\left[\mathop{\mathrm{Var}}\nolimits_{\widetilde{P} _i}\left[X_i \right]\right] - \mathbb{E}_{i\sim [n]}\left[\mathop{\mathrm{Var}}\nolimits_{\widetilde{P}^\prime }\left[X_i \right]\right] \geq 4a\).
	\end{enumerate}
\end{corollary}
\begin{proof}
	We first prove the first statement. \autoref{lma:conditioning-reduces-variance} gives a point-wise inequality, taking the expectation on both sides with the \href{https://en.wikipedia.org/wiki/Dominated_convergence_theorem}{dominanted convergence theorem}, we have
	\[
		\mathbb{E}_{i, j\sim [n]}\left[\mathop{\mathrm{Var}}\nolimits_{}\left[X_i \right] - \mathbb{E}_{X_j}\left[\mathop{\mathrm{Var}}\nolimits_{}\left[X_i \mid X_j \right] \right] \right] \geq 4 \mathbb{E}_{i, j\sim [n]}\left[\mathop{\mathrm{Cov}}\nolimits_{}\left[X_i, X_j \right] ^{2} \right],
	\]
	with the fact that
	\[
		\mathbb{E}_{i, j\sim [n]}\left[\mathop{\mathrm{Var}}\nolimits_{}\left[X_i \right] - \mathbb{E}_{X_j}\left[\mathop{\mathrm{Var}}\nolimits_{}\left[X_i \mid X_j \right] \right] \right] = \mathbb{E}_{j\sim [n]}\mathbb{E}_{X_j}\left[\mathbb{E}_{i\sim [n]}\left[\mathop{\mathrm{Var}}\nolimits_{}\left[X_i \right] \right] - \mathbb{E}_{i\sim [n]}\left[\mathop{\mathrm{Var}}\nolimits_{}\left[X_i\mid X_j \right] \right] \right],
	\]
	hence conclude the first part. A probabilistic argument proves the \emph{either-or} statement.
\end{proof}

\begin{remark}
	\autoref{col:lec16} says that either we have a small covariance, or we can reduce it by a lot.
\end{remark}

\begin{theorem}\label{thm:lec16}
	Suppose \(\widetilde{P} =\{ \widetilde{P} _A \}_{\left\vert A \right\vert \leq R} \) is an \(R\)-\hyperref[def:local-distribution]{local distribution} which is \(\mathop{\mathrm{Lass}}_R(\mathcal{\MakeUppercase{p}})\) feasible and \(R \geq 1 / \epsilon ^4 + 2\), then there exists \(S \subseteq [n]\) such that \(\left\vert S \right\vert \leq 1 / \epsilon ^4\), \(\alpha _S \in \left\{ 0, 1 \right\} ^{\left\vert S \right\vert }\), and \(\widetilde{P} ^\prime \coloneqq \widetilde{P} \mid X_S\gets \alpha _S\), we have
	\[
		\mathbb{E}_{ij\sim [n]}\left[\mathop{\mathrm{Cov}}\nolimits_{\widetilde{P} ^\prime }\left[X_i, X_j \right]^{2} \right] \leq \frac{\epsilon ^4}{4}.
	\]
	Moreover, \(S\) and \(\alpha _S\) can be found in \(\poly(n, 1 / \epsilon )\).
\end{theorem}
\begin{proof}
	We actually have a constructive proof, i.e., we directly give an algorithm which runs in \(\poly(n, 1 / \epsilon )\) and find the desired \(i, j\).

	\begin{algorithm}[H]\label{algo:thm-lec16}
		\DontPrintSemicolon
		\caption{\autoref{thm:lec16} -- Construction}
		\KwData{\(\widetilde{P} \), \(\epsilon > 0\)}
		\KwResult{\(\widetilde{P} ^\prime\) with expected covariance smaller than \(\epsilon ^4 / 4\), \(S\)}
		\BlankLine
		\(\ell \gets 0\), \(\widetilde{P} ^{(\ell )}\gets \widetilde{P} \), \(S\gets \varnothing \)\;
		\For(){\(\ell = 0, 1, \ldots  , 1 / \epsilon ^4\) }{
			\uIf(\label{algo:thm-lec16-if}){\(\mathbb{E}_{i, j}\left[\mathop{\mathrm{Cov}}\nolimits_{\widetilde{P} ^{(\ell )}}\left[X_i, X_j \right] ^{2} \right] \leq \epsilon ^4 / 4\)}{
				\Return{\(\widetilde{P} ^{(\ell )}\)}\Comment*[r]{\(\widetilde{P} ^{(\ell )}= \widetilde{P} \mid X_S\gets \alpha _S\), \(S\)}
			}\Else(\label{algo:thm-lec16-else}){
				Find \(j_{\ell +1}\in [n]\setminus S\), \(\theta _{\ell +1}\in \left\{ 0, 1 \right\} \) \Comment*[r]{Guaranteed in \autoref{col:lec16}}
				\(\widetilde{P} ^{(\ell +1)}\gets \widetilde{P} ^{(\ell )}\mid X_{j_{\ell +1}}\gets \theta _{\ell +1}\)\;
				\(S\gets S \cup \left\{ j _{\ell +1}\right\} \)\;
			}
		}
	\end{algorithm}
	To analyze \autoref{algo:thm-lec16}, observe that if \autoref{algo:thm-lec16} returns, then we have a desired property, so we only need to ensure it'll meet the condition in \autoref{algo:thm-lec16-if} in \(1 / \epsilon ^4\) iterations. Now, for a \hyperref[def:local-distribution]{local distribution} \(Q\), let \(\mathop{\mathrm{Var}}\nolimits_{}\left[Q \right] \coloneqq \mathbb{E}_{i\sim [n]}\left[\mathop{\mathrm{Var}}\nolimits_{Q}\left[X_i \right]  \right] \) and \(\mathop{\mathrm{Cov}}\nolimits_{}\left[Q \right] \coloneqq \mathbb{E}_{i, j\sim [n]}\left[\mathop{\mathrm{Cov}}\nolimits_{Q}\left[X_i, X_j \right]  \right] \). We see that we only fail if in every iteration, we reach \autoref{algo:thm-lec16-else}, i.e., \(\mathop{\mathrm{Cov}}\nolimits_{}[ \widetilde{P} ^{(\ell )}] \leq \epsilon ^4 / 4\) for all \(\ell \). But from \autoref{col:lec16}, we know that the \(\widetilde{P} ^{(\ell +1)}\) we find will have the property that
	\[
		\mathop{\mathrm{Var}}\nolimits_{}[\widetilde{P} ^{(\ell -1 )} ] - \mathop{\mathrm{Var}}\nolimits_{}[\widetilde{P} ^{(\ell)}] \geq 4\cdot \epsilon ^4 / 4 = \epsilon ^4
		\implies \mathop{\mathrm{Var}}\nolimits_{}[ \widetilde{P} ^{(\ell )} ] \leq \mathop{\mathrm{Var}}\nolimits_{}[ \widetilde{P} ^{(\ell -1)}] - \epsilon ^4.
	\]
	By telescoping, we have
	\[
		\mathop{\mathrm{Var}}\nolimits_{}[ \widetilde{P} ^{(1 / \epsilon ^4)} ]
		\leq \mathop{\mathrm{Var}}\nolimits_{}[ \widetilde{P}^{(0)} ] - \frac{1}{\epsilon ^4}\cdot \epsilon ^4
		= \mathop{\mathrm{Var}}\nolimits_{}[ \widetilde{P} ] - 1
		\leq \frac{1}{4} - 1 < 0,
	\]
	a contradiction, and hence we must terminate, finishing the proof.
\end{proof}

\begin{remark}
	\autoref{thm:lec16} says that suppose we have a \hyperref[def:local-distribution]{local distribution} over \(n\) variables with sufficient large locality. Then turns out that there's a small subset of variables, if we fix them, they'll almost determine all other variables.
\end{remark}

Finally, we have the following algorithm.

\begin{algorithm}[H]\label{algo:max-cut-PTAS}
	\DontPrintSemicolon
	\caption{\hyperref[prb:max-cut]{Max Cut} -- \href{https://en.wikipedia.org/wiki/Polynomial-time_approximation_scheme}{PTAS}}
	\KwData{A dense graph \(\mathcal{\MakeUppercase{g}} =(\mathcal{\MakeUppercase{v}} , \mathcal{\MakeUppercase{e}} )\) with \(\left\vert \mathcal{\MakeUppercase{e}}  \right\vert \geq \epsilon n^{2} \), \(\epsilon > 0\)}
	\KwResult{A cut \(S\)}
	\SetKwFunction{rand}{rand}
	\SetKwFunction{Ber}{Ber}
	\SetKwFunction{Sol}{Solve}
	\SetKwFunction{Red}{\hyperref[algo:thm-lec16]{Reduce-Variance}}
	\BlankLine
	\(R\gets 1 / \epsilon ^4 + 2\)\;
	\(\widetilde{P} \coloneqq \{ \widetilde{P} _A \}_{\left\vert A \right\vert \leq R} \gets\)\Sol{\(\mathop{\mathrm{Lass}}_R(\mathcal{\MakeUppercase{p}})\)}\;
	\(\widetilde{P} ^\prime \gets\)\Red{\(\widetilde{P}\)}\Comment*[r]{\(\mathbb{E}_{ij\sim [n]}\left[\mathop{\mathrm{Cov}}\nolimits_{\widetilde{P} ^\prime }\left[X_i, X_j \right]^{2} \right] \leq \epsilon ^4 / 4\)}
	\;
	\label{algo:PTAS-max-cut-comment}\Comment{Rounding}
	\For(){\(i\in \mathcal{\MakeUppercase{v}} \)}{
		\(\lambda _i\gets\)\Ber{\(\widetilde{P} ^\prime (X_i = 1)\)}\;
	}
	\(S\gets \left\{ i\in \mathcal{\MakeUppercase{v}} \colon \lambda _i = 1 \right\} \)\;
	\Return{\(S\)}\;
\end{algorithm}

\begin{remark}
	The rounding method in \autoref{algo:max-cut-PTAS} (i.e., \autoref{algo:PTAS-max-cut-comment}) is ridiculously simple compare to \autoref{algo:max-cut-randomized-rounding}! \(\widetilde{P} ^\prime \) basically tells you everything.
\end{remark}

We indeed have the following guarantee.

\begin{theorem}[\href{https://en.wikipedia.org/wiki/Polynomial-time_approximation_scheme}{PTAS} for max cut]\label{thm:PTAS-for-max-cut}
	For any \(\epsilon >0\), given a graph \(\mathcal{\MakeUppercase{g}} =(\mathcal{\MakeUppercase{v}} , \mathcal{\MakeUppercase{e}} )\) such that \(\left\vert \mathcal{\MakeUppercase{e}}  \right\vert \geq \epsilon n^{2} \), there exists a \((1 - 4\epsilon )\)-approximation algorithm runs in \(n^{O(1 / \epsilon ^4)}\)-time.
\end{theorem}

We see that as long as the graph is dense enough, we can spend more and more time to get a better approximation, which is the whole point of Lasserre hierarchy.