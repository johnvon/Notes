\lecture{23}{21 Nov. 10:30}{Noisy BLR Test and Unique Games Conjecture}
\subsection{BLR Test}
To do this, we consider creating a \emph{weighted} \hyperref[prb:max-3LIN]{3LIN} instance.
\begin{enumerate}[(a)]
	\item Each equation has weight.
	\item Weights sum to \(1\).
	\item \(\mathop{\mathrm{OBJ}}(f) = \sum_{C_i \text{ is satisfied}} w(C_i)\).
\end{enumerate}
In this way, we can interpret one instance as the probability distribution over equations. Rather than construct the equation directly, we can specify how we are going to sample equations via specifying a probability distribution!

\begin{remark}
	Mathematically, given a distribution \(P\) on \(\left\{ x_i\cdot x_j \cdot x_k = b \right\} \), we have
	\[
		w(w_{i} \cdot w_{j} \cdot w_{k} = b) = \Pr_{P}(\text{\(x_i \cdot x_j \cdot x_k = b\) is sampled} ).
	\]
\end{remark}

Now, the construction is the following, where we specify how we sample one equation (hence this gives the probability distribution).
\begin{itemize}
	\item Variables: \(\left\{ \pm 1 \right\} ^n\) (hence, the assignment is a \hyperref[def:boolean-function]{boolean function} \(f\colon \left\{ \pm 1 \right\} ^n \to  \left\{ \pm 1 \right\} \)).
	\item Sample \(x\in \left\{ \pm 1 \right\} ^n\) and \(y\in \left\{ \pm 1 \right\} ^n\) independently, let \(z = \left\langle x, y \right\rangle \).
	\item Output \(f(x) \cdot f(y)\cdot f(z) = 1\).
\end{itemize}

\begin{note}
	Notice that this construction is deterministic, i.e., we can specify the weight of each possible equations directly. But we just use the probabilistic language.
\end{note}

To see why this is what we want, notice that if \(f = \chi _S\) for some \(S \subseteq [n]\), then for all \(x, y\in \left\{ \pm 1 \right\} ^n\),
\[
	f(x) f(y) f(z) = x^S y^S (x\cdot y)^S = 1,
\]
hence \(\mathop{\mathrm{OBJ}}(f) = 1\)! On the other hand, if \(f\) has \(k\) nonzero Fourier coefficients (i.e., \(f\) is far from any \(\chi _S\)) of value \(1 / \sqrt{k} \), then \(\mathop{\mathrm{OBJ}}(f) \approx 1 / 2\). This construction is known as BLR test~\cite{10.1145/100216.100225}.

\begin{theorem}[\cite{10.1145/100216.100225}]\label{thm:BLR}
	\(\mathop{\mathrm{OBJ}}(f) = 1 / 2 + 1 / 2 \cdot \sum_{S \subseteq [n]} \hat{f} (S)^3 \).
\end{theorem}
\begin{proof}
	Observe that
	\[
		\begin{split}
			\mathop{\mathrm{OBJ}}(f)
			&= \mathbb{E}_{x, y}\left[\mathbbm{1}\left[ f(x)f(y)f(z) = 1 \right] \right]\\
			&= \mathbb{E}_{x, y}\left[\frac{1}{2} + \frac{1}{2} f(x)f(y)f(z) = 1 \right]
			= \frac{1}{2} + \frac{1}{2} \mathbb{E}_{x, y}\left[f(x)f(y)f(z) \right].
		\end{split}
	\]
	Now, we decompose \(f(x), f(y), f(z)\) with respect to the Fourier basis, i.e., \(f(x) = \sum_{S} \hat{f} (s)\chi _S(x)\) we have
	\[
		\begin{split}
			\mathop{\mathrm{OBJ}}(f)
			&= \frac{1}{2} + \frac{1}{2} \mathbb{E}_{x, y}\left[\sum_{S, T, U \subseteq [n]} \hat{f} (S)\hat{f} (T)\hat{f} (U) \cdot \chi _S(x) \chi _T(y) \chi _U(z)\right]\\
			&= \frac{1}{2} + \frac{1}{2} \sum_{S, T, U \subseteq [n]} \hat{f} (S)\hat{f} (T)\hat{f} (U) \cdot \mathbb{E}_{x, y}\left[\chi _S(x) \chi _T(y) \chi _U(z)\right].
		\end{split}
	\]
	Since for fixed \(S, T, U\),
	\[
		\begin{split}
			\mathbb{E}_{x, y}\left[\chi _S(x) \chi _T(y) \chi _U(z)\right]
			&= \mathbb{E}_{x, y}\left[x^S\cdot y^T\cdot (x\cdot y)^U \right]\\
			&= \mathbb{E}_{x, y}\left[x^{S \Delta U} y^{T;\Delta U} \right]\\
			&= \mathbb{E}_{x}\left[x^{S \Delta U} \right] \mathbb{E}_{y}\left[y^{T \Delta U} \right]
			= \begin{dcases}
				1, & \text{ if } S=U \text{ and } T=U ; \\
				0, & \text{ otherwise} .
			\end{dcases}
		\end{split}
	\]
	Hence,
	\[
		\mathop{\mathrm{OBJ}}(f)
		= \frac{1}{2} + \frac{1}{2} \sum_{S \subseteq [n]} \hat{f} (S)^3 .
	\]
\end{proof}

We see that if we only require \(f\) to have high value when it is corresponding to \(\chi _S\), then we're done. But actually, what we want is when \(S\) is a singleton set, and hence we need more works.

\subsection{Noisy BLR Test}
In particular, we want to eliminate the case that when \(S = \varnothing \) and \([n]\), \(\mathop{\mathrm{OBJ}}(f) = 1\), i.e., we want to implement \hyperref[not:dictation]{dictation} test. This can be done via introducing noise. Firstly, consider the following new \hyperref[prb:max-3LIN]{3LIN} instances.

\begin{itemize}
	\item Variables: \(\left\{ \pm 1 \right\} ^n\) (hence, the assignment is a \hyperref[def:boolean-function]{boolean function} \(f\colon \left\{ \pm 1 \right\} ^n \to  \left\{ \pm 1 \right\} \)).
	\item Sample \(x\in \left\{ \pm 1 \right\} ^n\) and \(y\in \left\{ \pm 1 \right\} ^n\) independently, and also \(b\in \left\{ \pm 1 \right\} \).
	\item For all \(i\), let
	      \[
		      z_i = \begin{dcases}
			      x_i y_i b,  & \text{ with probability } 1-\epsilon  ; \\
			      -x_i y_i b, & \text{ with probability} \epsilon  .
		      \end{dcases}
	      \]
	\item Output \(f(x) \cdot f(y)\cdot f(z) = b\).
\end{itemize}

\begin{remark}[Sanity check]
	If \(f = \chi _\varnothing \), \(\mathop{\mathrm{OBJ}}(f) = 1/2\); if \(f = \chi _{[n]}\), \(\mathop{\mathrm{OBJ}}(f) \approx 1 / 2\).
\end{remark}
\begin{explanation}
	We see that
	\begin{itemize}
		\item If \(f = \chi _\varnothing \) (i.e., \(f(x) = 1\) for all \(x\)): \(\mathop{\mathrm{OBJ}}(f) = 1 / 2\).
		\item If \(f = \chi _{[n]}\) (i.e., \(f(x) = x_1 x_2 \ldots  x_n\)): \(\mathop{\mathrm{OBJ}}(f) = \mathbb{E}_{x, y, z, b}\left[1 / 2 + 1 / 2 f(x)f(y)f(z)b \right] \). Since
		      \[
			      \begin{split}
				      \mathbb{E}_{}\left[f(x) f(y) f(z) b \right]
				      &= \frac{1}{2} \mathbb{E}_{}\left[f(x) f(y) f(z) b \mid b = 1 \right] + \frac{1}{2} \mathbb{E}_{}\left[f(x) f(y) f(z) b \mid b = -1 \right] \\
				      &= \frac{1}{2} \prod_{i=1}^{n} \mathbb{E}_{}\left[x_i y_i z_i \mid b = 1 \right] - \frac{1}{2} \prod_{i=1}^{n} \mathbb{E}_{}\left[x_i y_i z_i \mid b = -1 \right] \\
				      &= \frac{1}{2} (1 - 2 \epsilon )^n - \frac{1}{2} (-1 + 2\epsilon )^n\\
				      &= \begin{dcases}
					      0,                & \text{ if \(n\) is even}  ; \\
					      (1-2\epsilon )^n, & \text{ if \(n\) is odd}.
				      \end{dcases}
			      \end{split}
		      \]
		      So, if \(n \gg 1 / \epsilon \), then \((1 - 2 \epsilon )^n \leq e^{-2 \epsilon n} \approx 0\), i.e., \(\mathop{\mathrm{OBJ}}(f) \approx 1 / 2\).
	\end{itemize}
\end{explanation}

The above remark holds for a general \(f\), i.e., when \(f\) is far from \hyperref[not:dictation]{dictation}, the value will be less than \(1\) significantly. To see this, let \(f(x) = \sum_{S \subseteq [n]} \hat{f} (S)\chi _S\), recall that
\[
	\mathop{\mathrm{OBJ}}(f)
	= \mathbb{E}_{x, y, z, b}\left[\frac{1}{2} + \frac{1}{2} f(x) f(y) f(z) b \right]
	= \frac{1}{2} + \frac{1}{2} \sum_{S, T, U \subseteq [n]} \hat{f} (S) \hat{f} (T) \hat{f} (U) \mathbb{E}_{}\left[\chi _S(x) \chi _T(y) \chi _U(z) b \right],
\]
hence we're interested in
\[
	\mathbb{E}_{x, y, z, b}\left[\chi _S(x) \chi _T(y) \chi _U (z) b \right]
	= \frac{1}{2} \mathbb{E}\left[\chi _S(x) \chi _T(y) \chi _U (z) \mid b=1\right] - \frac{1}{2} \mathbb{E}\left[\chi _S(x) \chi _T(y) \chi _U (z) \mid b=-1\right]
\]
for a fixed \(S, T, U \subseteq [n]\). Notice that in this expectation, things are independent among the coordinate, i.e., if \(S\) contains \(i\), then \(x_i\) will appear in the calculation, and same for \(T\) and \(U\). But observe the following.

\begin{claim}
	For all \(i\in [n]\), given \(b = \pm 1\),
	\[
		\mathbb{E}_{}\left[x_i \right]
		= \mathbb{E}_{}\left[y_i \right]
		= \mathbb{E}_{}\left[z_i \right]
		= \mathbb{E}_{}\left[x_i y_i \right]
		= \mathbb{E}_{}\left[x_i z_i \right]
		= \mathbb{E}_{}\left[y_i z_i \right]
		= 0
	\]
\end{claim}
\begin{explanation}
	Consider the case that \(b = 1\), since
	\[
		z_i = \begin{dcases}
			x_i y_i,  & \text{ w.p.\ } 1-\epsilon  ; \\
			-x_i y_i, & \text{ w.p.\ } \epsilon  ;
		\end{dcases},\quad x_i z_i = \begin{dcases}
			y_i,  & \text{ w.p.\ } 1-\epsilon  ; \\
			-y_i, & \text{ w.p.\ } \epsilon  ,
		\end{dcases}
	\]
	we have that \(\mathbb{E}_{}\left[x_i z_i \right] = (1 - \epsilon ) \mathbb{E}_{}\left[y_i \right] - \epsilon \mathbb{E}_{}\left[y_i \right] = 0\). The same holds for \(b = -1\) as well.
\end{explanation}

And hence, we see that only \(\mathbb{E}_{}\left[x_i y_i z_i \right] \) is left, with
\[
	\mathbb{E}_{}\left[x_i y_i z_i \mid b= 1 \right] = 1 - 2\epsilon,
	\quad \mathbb{E}_{}\left[x_i y_i z_i \mid b= -1 \right] = -1 + 2\epsilon.
\]
This suggests that
\[
	\begin{split}
		\mathbb{E}_{}\left[\chi _S(x) \chi _T(y) \chi _U(z) \mid b=1 \right] &= \begin{dcases}
			(1-2\epsilon )^{\vert S \vert }, & \text{ if } S=T=U ;  \\
			0,                               & \text{ otherwise}  .
		\end{dcases}\\
		\mathbb{E}_{}\left[\chi _S(x) \chi _T(y) \chi _U(z) \mid b=-1 \right] &= \begin{dcases}
			(-1+2\epsilon )^{\vert S \vert }, & \text{ if } S=T=U ;  \\
			0,                                & \text{ otherwise}  .
		\end{dcases}
	\end{split}
\]
And hence,
\[
	\mathbb{E}_{x, y, z, b}\left[\chi _S(x) \chi _T(y) \chi _U(z) b \right] = \begin{dcases}
		(1 - 2\epsilon )^{\vert S \vert }, & \text{ if } S=T=U \text{ and \(\vert S \vert \) is odd}; \\
		0,                                 & \text{ otherwise} ,
	\end{dcases}
\]
implying that
\[
	\begin{split}
		\mathop{\mathrm{OBJ}}(f)
		&= \mathbb{E}_{x, y, z, b}\left[\frac{1}{2} + \frac{1}{2} f(x) f(y) f(z) b \right] \\
		&= \frac{1}{2} + \frac{1}{2} \sum_{S, T, U \subseteq [n]} \hat{f} (S) \hat{f} (T) \hat{f} (U) \mathbb{E}_{}\left[\chi _S(x) \chi _T(y) \chi _U(z) b \right] \\
		&= \frac{1}{2} + \frac{1}{2} \sum_{\vert S \vert \text{ odd}} \hat{f} (S)^3 (1 - 2\epsilon )^{\vert S \vert }.
	\end{split}
\]
We then conclude that if \(f = \chi _i\), then \(\mathop{\mathrm{OBJ}}(f) = 1 - \epsilon \); and if \(f = \chi _S\),
\[
	\mathop{\mathrm{OBJ}}(f) = \begin{dcases}
		\frac{1}{2} + \frac{1}{2} (1-2\epsilon )^{\vert S \vert }, & \text{ if \(\vert S \vert \) is odd};  \\
		\frac{1}{2},                                               & \text{ if \(\vert S \vert \) is even}.
	\end{dcases}
\]
So \(f\) has \(1 / \epsilon ^2\) Fourier coefficients of \(\epsilon \), leading to
\[
	\mathop{\mathrm{OBJ}}(f) \leq \frac{1}{2} + \frac{1}{2}\frac{1}{\epsilon ^2} \epsilon ^3 = \frac{1}{2} + \frac{\epsilon}{2}.
\]
Although this is nice and is what we want, but to do the full reduction from \hyperref[prb:label-cover]{label cover} to \hyperref[prb:max-3LIN]{3LIN}, we will need a lot more work.

\section{Unique Games}
Thankfully, to prove \autoref{thm:3LIN}, we can instead show the reduction from \hyperref[prb:unique-game]{unique games} to \hyperref[prb:max-3LIN]{3LIN}.\footnote{Although as we noted, this can be done directly without any conjecture.}

\begin{problem}[Unique game]\label{prb:unique-game}
Given a \(d\)-regular bipartite graph \(\mathcal{\MakeUppercase{g}} =(U\sqcup V , \mathcal{\MakeUppercase{e}} )\) with \(\vert U \vert = \vert V \vert = n\), with two label sets \(R \sqcup R\) (one for each \(U\), \(V \)) such that for all \(e=(u, v)\in \mathcal{\MakeUppercase{e}} \), we have a bijection \(\Pi _e \colon [R]\to [R]\). The \emph{label cover} problem asks for an assignment \(\sigma\colon U \sqcup V \to R\) maximizes the satisfied edges.
\end{problem}

\begin{remark}
	We see that the \hyperref[prb:unique-game]{unique game} problem is a special case of \hyperref[prb:label-cover]{label cover}, where now the label sets are the same on both sides, and the projection becomes a bijection, i.e., we now have uniqueness.
\end{remark}

\begin{center}
	\incfig{unique-game}
\end{center}

The uniqueness plays an important role here: since an assignment to one vertex uniquely determines all its neighbors, which implies the following.
\begin{claim}
	\hyperref[def:c-s-Gap]{\((1, 1)\)-Gap} \hyperref[prb:unique-game]{unique game} has a polynomial time algorithm.
\end{claim}
\begin{explanation}
	Assume that \(\mathcal{\MakeUppercase{g}} \) is connected, then pick an arbitrary vertex \(v\), we just try all labels and propagate. If there is an assignment satisfying every edge, we can find it this way.
\end{explanation}

On the other hand, given a \hyperref[prb:label-cover]{label cover} instance, even if we are told that there is a perfect assignment, we still can't find it. But interestingly, we hypothesize the following.

\begin{conjecture}[Unique games conjecture~\cite{10.1145/509907.510017}]\label{conj:unique-game}
	For every \(\epsilon > 0\), there exists \(R\)\footnote{\(R\) is depending on \(\epsilon \).} such that the \hyperref[def:c-s-Gap]{\((1-\epsilon , \epsilon )\)-Gap} \hyperref[prb:unique-game]{unique game} is \(\NP\)-hard with \(R\).
\end{conjecture}

Compared to \autoref{thm:label-cover}, which states that the \hyperref[def:c-s-Gap]{\((1, \epsilon )\)-Gap} \hyperref[prb:label-cover]{label cover} is \(\NP\)-hard, we see that the \hyperref[conj:unique-game]{unique game conjecture} suggests that the only difference between \hyperref[prb:unique-game]{unique game} and \hyperref[prb:label-cover]{label cover} is at the \hyperref[def:c-s-Gap]{\((1, 1)\)-Gap} version, i.e., we can solve the exact \hyperref[prb:unique-game]{unique game}, but this is the only thing we can do additionally compared to \hyperref[prb:label-cover]{label cover}.

\begin{remark}[Optimal hardness]
	Assuming the \hyperref[conj:unique-game]{unique game conjecture}, for all \(\epsilon > 0\), it is \(\NP\)-hard to
	\begin{enumerate}[(a)]
		\item \((2-\epsilon )\)-approximate \hyperref[prb:vertex-cover]{vertex cover} and \hyperref[prb:feedback-vertex-set]{feedback vertex set};
		\item \(c\)-approximate multicut for all \(c > 1\);
		\item \((\alpha _{\mathrm{GW} } + \epsilon )\)-approximate \hyperref[prb:max-cut]{max cut}.
	\end{enumerate}
\end{remark}

We're going to see how we can deduce the hardness of \hyperref[prb:max-3LIN]{3LIN} from \hyperref[conj:unique-game]{unique game conjecture}.