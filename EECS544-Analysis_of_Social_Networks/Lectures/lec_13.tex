\lecture{13}{20 Oct. 12:30}{Markov Chain}
\begin{eg}
	Some example for stochastic process.
	\begin{enumerate}
		\item Let \(X_i\sim \mathrm{Beronulli}(p)\), where \(X_{i}\in\{0, 1\}\) and
		      \[
			      \probability{}{X_{i} = 1} = p.
		      \]
		      \begin{intuition}
			      Biased coin toss.
		      \end{intuition}
		\item Let \(X_i\sim \mathrm{Uniform}(\{1, 2, 3, 4, 5, 6\})\), where \(X_{i}\in\{1, 2, 3, 4, 5, 6\}\) and
		      \[
			      \probability{}{X_{i} = j} = \frac{1}{6}.
		      \]
		      \begin{intuition}
			      Random tosses of dice.
		      \end{intuition}
		\item Let \(X_i\sim \mathrm{Uniform}(\left[ 0, 1 \right] )\), where \(X_{i}\in\left[ 0, 1 \right] \).
		\item Let \(X_i\sim \mathrm{exp}(\lambda)\), then
		      \[
			      \probability{}{X_{i}>u} = e^{-\lambda u}\qquad \forall u\geq 0.
		      \]
		\item Let \(X_i\sim \mathcal{N}(\mu, \sigma^2)\). This is so-called normal distribution with \emph{Gaussian mean} \(\mu\) and variance \(\sigma^2\).
	\end{enumerate}
\end{eg}

\subsection{i.i.d. Sequence}
\begin{remark}
	Notice that our discussion will focus on \(\expectation{X_i} < +\infty \).
\end{remark}

\begin{note}
	The core property of i.i.d. sequence is \textbf{strong law of large numbers}(SLLN). Namely,
	\[
		\lim_{n \to \infty} \frac{1}{n}\sum\limits_{i=1}^{n} x_{i}\to \expectation{X}.
	\]
\end{note}

In general, we let \(Y_{i} = \mathbbm{1}_{\{X_{i}\in B\}}\)(indicator function), where
\[
	\mathbbm{1}_{\{X_{i}\in B\}}(\omega) = \begin{dcases}
		0, & \text{ if }X_{i}(\omega)\in B \\
		1, & \text{ otherwise},
	\end{dcases}
\]
hence \(y_{i}\sim \mathrm{Bernoulli}(p = \probability{}{X_{i}\in B}\).

\begin{note}
	Markov chains will not be i.i.d. sequence of random variables since there will be dependence. But still, the SLLN will hold under conditions.
\end{note}

\subsection{Conditional Property}
\begin{prev}
	We first note that if \(\probability{}{A\cap B} = \probability{}{A} \probability{}{B} \), then \(A\) and \(B\) are independent.
\end{prev}

\begin{definition}
	Let \(\probability(B)>0\), then we can define so-called \emph{conditional probability} between event \(A\) and \(B\) as
	\[
		\frac{\probability{}{A\cap B}}{\probability{}{B}} \eqqcolon \probability{}{A \mid B}.
	\]
\end{definition}

\begin{definition}
	Similarly, we can have so-called \emph{conditional expectation} defined as
	\[
		\expectation{}{X \mid X\in A} = \expectation{}{X \mid A}.
	\]
\end{definition}

\begin{eg}
	We look at an example for discrete random variable \(X\). Let \(X\in \Omega\), then
	\[
		\begin{split}
			\expectation{}{X \mid X\in A} &= \sum\limits_{\omega\in \Omega} \omega\probability{}{X = \omega \mid X\in A}\\
			&=\sum\limits_{\omega\in \Omega} \omega \frac{\probability{}{\left\{X = \omega\right\}\cap \left\{x\in A\right\}}}{\probability{}{X\in A} }\\
			&=\sum\limits_{\omega\in \Omega} \omega \frac{\probability{}{X = \omega}}{\probability{}{X\in A} }\\
			&=\frac{\sum\limits_{\omega\in \Omega} \omega \probability{}{X = \omega}}{\probability{}{X\in A}}
		\end{split}
	\]
\end{eg}

We also have so-called conditional independence.
\begin{definition}
	Let \(A, B, C\) be events. Then
	\[
		\probability{}{X\in A, X\in B, X\in C} = \probability{}{X\in A\cap B\cap C}.
	\]
	Then if \(A, B,  C\) are independent, then we further have
	\[
		\probability{}{X\in A\cap B\cap C}= \probability{}{X\in A}\probability{}{X\in B}\probability{}{X\in C}
	\]
	as we have seen. If they are dependent, given \(\probability{}{X\in C}>0\), then
	\[
		\probability{}{X\in A, X\in B, X\in C} = \probability{}{X\in A, X\in B \mid X\in C}\probability{}{X\in C}.
	\]

	Now, if
	\[
		\probability{}{X\in A, X\in B \mid X\in C} = \probability{}{X\in A \mid X\in C}\probability{}{X\in B \mid X\in C},
	\]
	then \(A\) and \(B\) are \emph{conditionally independent} given \(C\).
\end{definition}

\section{Markov Chain}
Markov Chains are dependent processes but with a specific form of conditional independence.

Consider random variables sequence
\[
	X_1, X_2, X_3, \ldots
\]
with discrete time index \(1, 2, 3, \ldots \). Noting that they lie on a \underline{discrete space}, like number, labels, etc.

\begin{note}
	\textbf{Conditional Independence properties}: Knowing the present, the past and the future are \emph{conditionally independent}.
\end{note}

Say we are at time \(n>1\), then
\[
	X_n = a_n
\]
where \(a_n\) is taking values in a discrete set. Then
	{\small
		\[
			\begin{split}
				&\probability{}{X_1 = a_1, X_2 = a_2,  \ldots X_{n-1} = a_{n-1}, X_{n+1} = a_{n+1},\ldots , X_{n+m} = a_{n+m}  \mid X_n = a_n }\\
				=&\probability{}{X_1 = a_1, X_2 = a_2,  \ldots X_{n-1} = a_{n-1}\mid X_n = a_n }\\
				&\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad\cdot \probability{}{X_{n+1} = a_{n+1},\ldots , X_{n+m} = a_{n+m}  \mid X_n = a_n}.
			\end{split}
		\]
	}

\begin{intuition}
	Knowing where we are, the progress in the future is conditionally independent of how we got to the present.
\end{intuition}

\section{Time Homogeneous Markov Chain}
We need to know two things:
\begin{itemize}
	\item Initial distribution. \(X_0 \sim \mu\)(or \(X_1\sim \mu\))
	\item One-step transition matrix.
	      \[
		      P_{ij} \coloneqq \probability{}{X_{n+1} = j \mid X_n = i}\qquad \forall i, j.
	      \]


	      \begin{note}
		      \begin{enumerate}
			      \item Notice that this is not a function of \(n\).
			      \item The condition here is just for an example from the lecture.\todo{Review the example in lecture}
		      \end{enumerate}
	      \end{note}

	      Then we see that the total probability of \(\probability{}{X_n = i}\) is
	      \[
		      \begin{split}
			      \probability{}{X_n = i} &= \sum_j \probability{}{X_n = i, X_0 = j}\\
			      &= \sum_j \probability{}{X_0 = j}\probability{}{X_n = i  \mid  X_0 = j}\\
			      &=\sum_j \mu_{j} (P^n)_{ji}\\
			      &= (\mu^{T} P^n)_i
		      \end{split}
	      \]
	      where the first step is from the definition of \emph{total probability}. Recall that
	      \[
		      P_{ij} = \probability{}{X_{n+1} = j \mid X_n = i},
	      \]
	      hence
	      \[
		      \begin{split}
			      \sum_{j\in \Omega} P_{ij} &= \sum_{j\in \Omega} \probability{}{X_{n+1} = j \mid X_n = i}\\
			      &= \probability{}{X_{n+1}\in \Omega \mid X_n = i}\\
			      &= \frac{\probability{}{X_{n+1}\in \Omega, X_n = i} }{\probability{}{X_n = i} }\\
			      &= \frac{\probability{}{X_n = i} }{\probability{}{X_n = i} }\\
			      &= 1.
		      \end{split}
	      \]
	      We see that \(P\) is row stochastic and \(P_{i\cdot}\) is a probability distribution on \(\Omega\). So \(\rho(P) = 1\implies \exists  \pi\)(distribution) such that
	      \[
		      \pi^{T} P = \pi^{T}.
	      \]
	      We see that the left eigenvector is a distribution. We have \(P\) is irreducible and \(\Omega\) is finite, then
	      \[
		      \lim_{n\to \infty } \frac{1}{n}\underline{\sum\limits_{m=1}^{n}  \mathbbm{1}_{\left\{ X_m = i \right\} }} = \pi_i
	      \]
	      almost surely.
\end{itemize}

Refinement with aperiodic \(P\),
\[
	\lim_{n\to \infty }\probability{}{X_n = i} = \pi_i
\]

\section{Connection with Basic Page Rank}
\begin{prev}
	The \(r(n)\) is updated by the equation
	\[
		r(n+1) = N^{T}r(n) \iff r^{T}(n+1) = r^{T}(n)N.
	\]
	Further, recall that \(N\) is a row stochastic matrix, so compare to Markov Chain,
	\[
		\probability{}{X_n = \cdot}^{T} = \mu^{T}P^n = \mu^{T}P^{n-1}P = \probability{}{X_{n-1} = \cdot}^{T}P
	\]
	where \(\mu\sim \probability{}{X_0 = \cdot}\).

	We see that the above two equations are the same! With
	\[
		r(0)\coloneqq \begin{pmatrix}
			1/V    \\
			\vdots \\
			1/V    \\
		\end{pmatrix}.
	\]
\end{prev}

Recall that \(N_{ij}\) is defined as
\[
	N_{ij} = \begin{dcases}
		\frac{1}{d_i^{out}}, & \text{ if }A_{ij}\geq 1 \\
		0,                   & \text{ otherwise}.
	\end{dcases}
\]
We pick every outgoing neighbors uniformly at random, but no one else.

\section{Random Walker}
\begin{intuition}
	At location \(i\), regardless of how the random walker came to \(i\), pick the next destination uniformly at random from outgoing neighbors.
\end{intuition}

\begin{figure}[H]
	\centering
	\incfig{random-walker}
	\caption{Random Walker}
	\label{fig:random-walker}
\end{figure}

\begin{note}
	If \(d_i^{out} = 0\), then the random walker get stuck.
\end{note}