\lecture{2}{23 Aug.\ 9:00}{Sub-Gaussian Random Variables and the MGF Trick}
\section{Bounding Supremum of Empirical Process}
Most of this course will focus on bounding suprema of the \hyperref[def:EP]{empirical process}. Let's define it rigorously.

\begin{problem}[Bounding supremum of empirical process]\label{prb:bounded-sup-of-EP}
Given a domain \(\chi \), a probability measure \(\mathbb{P} \) on \(\chi \), data \(X_1, \dots , X_n \overset{\text{i.i.d.} }{\sim } \mathbb{P} \), and a function class \(\mathscr{F}\ni f \colon \chi \to \mathbb{R}  \). We want to find an (non-asymptotically) bound on
\[
	S_n(\mathscr{F} ) = \sup _{f\in \mathscr{F} } \left\vert \frac{1}{n} \sum_{i=1}^{n} f(X_i) - \mathbb{E}_{}\left[f(X) \right]  \right\vert.
\]
\end{problem}
\begin{answer}
	To do this, broadly speaking, we will go through a route with three basic steps:
	\begin{enumerate}[(a)]
		\item \(S_n(\mathscr{F} )\) ``concentrates'' around its expectation \(\mathbb{E}_{}\left[S_n(\mathscr{F} ) \right] \).
		\item \(\mathbb{E}_{}\left[S_n(\mathscr{F} ) \right] \leq \) the Rademacher complexity of \(\mathscr{F} \) via ``symmetrization''.
		\item Bounding the Radamacher complexity expected supremum of a ``sub-gaussian process'' by a technique called \emph{chaining}.
	\end{enumerate}
\end{answer}

Toward this end, we need some basic and fundamental concentration inequalities which are of wide interest and use.

\chapter{Concentration Inequalities}
As we just saw, to solve \autoref{prb:bounded-sup-of-EP}, we need some basic tools on concentration inequalities. The most celebrate concentration inequality might be the Gaussian tail, which achieve a quadratic exponential decay. Combine this with the classical central limit theorem, we can expect that as \(n \to \infty \), approximately the Gaussian tail bound kicks in.

However, to get a concrete, non-asymptotic bound for \(S_n(\mathscr{F} )\), we would need more sophisticated tools. Let's start with the basics, i.e., the Gaussian distribution.

\section{Gaussian Distribution}
For us, the gold standard for concentration would be the Gaussian distribution. The property of the Gaussian distribution we are interested in now is its rapid tail decay as we mentioned. This is given in \autoref{lma:Gaussian-tail-bound}.

\begin{lemma}\label{lma:Gaussian-tail-bound}\todo{Add proof}
	For \(Z \sim \mathcal{N} (0, 1)\),
	\[
		\left( \frac{1}{t} - \frac{1}{t^3} \right) \frac{1}{\sqrt{2\pi } } e^{- t^2 / 2} \leq \mathbb{P}(Z \geq t) \leq \frac{1}{t} \cdot \frac{1}{\sqrt{2\pi } } e^{- t^2 / 2}.
	\]
\end{lemma}
\begin{proof}
	Omitted as a homework.
\end{proof}
\begin{corollary}\label{col:Gaussian-tail-bound}
	For all \(t \geq 1\), we have
	\[
		\mathbb{P} (\mathcal{N} (0, \sigma ^{2} ) \geq t) \leq e^{-t^2 / 2\sigma ^{2} }.
	\]
\end{corollary}

Now, as is suggested by CLT, the following question arises.

\begin{problem*}
	Does \autoref{col:Gaussian-tail-bound} hold for sums of independent random variables? That is, given i.i.d.\ \(X_1, \dots , X_n\) with mean \(\mu \) and variance \(\sigma ^{2} \), whether
	\[
		\mathbb{P} (\sqrt{n}(\overline{X} - \mu ) \geq t )\leq e^{-t^2 / 2 \sigma ^{2} }
	\]
	for all \(t \geq 0\)?
\end{problem*}
\begin{answer}
	Just invoking CLT is not enough, we need to handle the error term in the normal approximation. We will see that we can show the above directly for a class of distributions with fast tail decay.
\end{answer}

To go beyond Gaussian tail bound, let start with the \hyperref[lma:MGF-trick]{moment generating function (MGF) trick}.

\section{MGF Trick}
The \hyperref[lma:MGF-trick]{MGF trick} is easy to develop, but it gives a foundation of all the concentration inequalities we're going to develop. Hence, although it's short, it's worth to make it a separate section.

\subsection{Markov's Inequality}
To start with, the most basic tool to bound tail probabilities is the \hyperref[lma:Markov-inequality]{Markov's inequality}.

\begin{lemma}[Markov's inequality]\label{lma:Markov-inequality}
	For a non-negative random variable \(X \geq 0\),
	\[
		\mathbb{P} (X \geq t) \leq \frac{\mathbb{E}_{}\left[X \right] }{t}.
	\]
\end{lemma}

\begin{note}
	\hyperref[lma:Markov-inequality]{Markov's inequality} is valid as soon as \(\mathbb{E}_{}\left[X \right] < \infty \). That is, it holds even when the second moment does not exist.
\end{note}

\begin{remark}
	The rate of tail decay is slow; it is \(O(1 / t)\). For the Gaussian, by \autoref{lma:Gaussian-tail-bound}, it's actually \(O(e^{-t^2 / 2})\).
\end{remark}

By the above remark, as might ask the following.

\begin{problem*}
	Can we derive faster tail decay bounds in general?
\end{problem*}
\begin{answer}
	Yes, if we assume more moments exist. If all moments exist and in particular the MGF exists, like for the Gaussian, then we can expect faster tail decay.
\end{answer}

\subsection{Chebyshev Inequality}
Continuing the discussion on the previous problem, for example, if we assume second moment exists, then we can get an \(O(1 / t^2)\) tail decay by \hyperref[lma:Chebyshev-inequality]{Chebyshev inequality}.

\begin{lemma}[Generalized Chebyshev inequality]\label{lma:Chebyshev-inequality}
	Given a random variable \(X\),
	\[
		\mathbb{P} (\vert X - \mu  \vert \geq t ) = \mathbb{P} (\vert X - \mu  \vert^p \geq t ^p) \leq \min _{p \geq 1} \frac{\mathbb{E}_{}\left[\vert X-\mu  \vert^p \right] }{t^p}.
	\]
\end{lemma}
\begin{proof}
	This is directly implied by the \hyperref[lma:Markov-inequality]{Markov's inequality}.
\end{proof}

\begin{remark}[Chebyshev Inequality]
	For \(p = 2\), we have the usual form
	\[
		\mathbb{P} (\vert X - \mu  \vert \geq t) \leq \frac{\Var_{}\left[X \right] }{t^2}
	\]
\end{remark}

\begin{remark}
	All tail bounds are derived using \hyperref[lma:Markov-inequality]{Markov's inequality}; the clever part is to apply it to the right random variable. In this sense, every tail bound is just \hyperref[lma:Markov-inequality]{Markov's inequality}.
\end{remark}

\subsection{Crarmer-Chernoff Method}
In the same vein, developed by Cramer and Chernoff, if we now assume the MGF exists and apply \hyperref[lma:Markov-inequality]{Markov's inequality}, we get the \hyperref[lma:MGF-trick]{MGF trick}.

\begin{lemma}[MGF trick (Crarmer-Chernoff method)]\label{lma:MGF-trick}
	Given a random variable \(X\),
	\[
		\mathbb{P} (X - \mu \geq t) = \mathbb{P} (e^{\lambda (X - \mu ) } \geq e^{\lambda t}) \leq \inf _{\lambda > 0} \frac{\mathbb{E}_{}\left[e^{\lambda (X - \mu )} \right] }{e^{\lambda t}}.
	\]
\end{lemma}

We will use the \hyperref[lma:MGF-trick]{MGF trick} rather than the \hyperref[lma:Chebyshev-inequality]{generalized Chebyshev's inequality} to derive tail bounds because MGF of a sum of independent random variables decomposes as the product of the MGF's. It is messier to work with the \(p^{\text{th} } \) moment of a sum of independent random variables.

\section{Hoeffding's Inequality}
\subsection{Sub-Gaussian Random Variables}
We will now consider a class of distributions whose MGF is dominated by the MGF of a Gaussian. Then, in a very clean way, the \hyperref[lma:MGF-trick]{MGF trick} will give us Gaussian tail bounds for these distributions.

\begin{definition}[Sub-gaussian]\label{def:sub-gaussian}
	Given a random variable \(X\) with \(\mathbb{E}_{}\left[X \right] = 0\), we say \(X\) is \emph{sub-gaussian} with variance factor\footnote{Also called proxy, sub-gaussian norm, etc.} \(\sigma ^2\) if for all \(\lambda \in \mathbb{R} \),
	\[
		\mathbb{E}_{}\left[e^{\lambda X} \right] \leq e^{\frac{\sigma ^2 \lambda ^2}{2}}.
	\]
\end{definition}

\begin{notation}
	We write \(\mathop{\mathrm{Subg}}(\sigma ^{2} ) \) for a compact representation of the class of \hyperref[def:sub-gaussian]{sub-gaussian} random variables with variance factor \(\sigma ^{2} \).
\end{notation}

\begin{remark}
	Observe that if \(X\in \mathop{\mathrm{Subg}}(\sigma ^{2} ) \):
	\begin{itemize}
		\item \(-X\in \mathop{\mathrm{Subg}}(\sigma ^{2} ) \);
		\item \(X \in \mathop{\mathrm{Subg}}(t^2) \) if \(t^2 > \sigma ^{2} \);
		\item \(cX\in \mathop{\mathrm{Subg}}(c \sigma ^{2} ) \).
	\end{itemize}
\end{remark}

\begin{lemma}[Equivalent conditions]\label{lma:sub-gaussian}\todo{Add proof}
	Given a random variable \(X\) with \(\mathbb{E}_{}\left[X \right] =0\), the following are equivalent for absolute constants \(c_1, \dots , c_5 > 0\).
	\begin{enumerate}[(a)]
		\item\label{lma:sub-gaussian-a} \(\mathbb{E}_{}\left[e^{\lambda X} \right] \leq e^{c_1^2 \lambda ^2}\) for all \(\lambda \in \mathbb{R} \).
		\item\label{lma:sub-gaussian-b} \(\mathbb{P} (\vert X \vert \geq t) \leq 2 e^{- t^2 / c_2^2}\).
		\item\label{lma:sub-gaussian-c} \(\left( \mathbb{E}_{}\left[\vert X \vert ^p \right]  \right)^{1 / p} \leq c_3 \sqrt{p}  \).
		\item\label{lma:sub-gaussian-d} For all \(\lambda \) such that \(\vert \lambda  \vert \leq 1 / c_4 \), \(\mathbb{E}_{}\left[e^{\lambda ^2 X^2} \right] \leq e^{c_4^2 \lambda ^2} \).
		\item\label{lma:sub-gaussian-e} For some \(c_5 < \infty \), \(\mathbb{E}_{}\left[e^{X^2 / c_5^2}  \right] \leq 2\).
	\end{enumerate}
\end{lemma}
\begin{proof}
	Let's just see the first implication from \autoref{lma:sub-gaussian-a} to \autoref{lma:sub-gaussian-b}. Given \(X \in \mathop{\mathrm{Subg}}(\sigma ) \),
	\[
		\mathbb{P} (X\geq t) \leq \inf _{\lambda > 0} e^{\lambda ^2 \sigma ^2 / 2 - \lambda t} \leq e^{-\frac{t^2}{2\sigma ^2}}
	\]
	where the last inequality follows from minimizing the quadratic function \(\lambda ^{2} \sigma ^{2} / 2 - \lambda t\) whose minimizer is \(\lambda ^{\ast} = t / \sigma ^{2} \). The same bound holds for the left tail and a union bound gives the two-sided version.
\end{proof}

Let's see some examples of the \hyperref[def:sub-gaussian]{sub-gaussian} random variables.

\begin{eg}[Rademacher random variable]\label{eg:Rademacher-random-varaible}
	\(\epsilon = \pm 1\) with probability \(1 / 2\) is a \(\mathop{\mathrm{Subg}}(1) \) random variable.
\end{eg}
\begin{explanation}
	We see that
	\[
		\mathbb{E}_{}\left[e^{\lambda \epsilon } \right]
		= \frac{1}{2} e^\lambda + \frac{1}{2} e^{-\lambda }
		= \frac{1}{2} \sum_{k=1}^{\infty} \left( \frac{\lambda ^k}{k!} + \frac{(-\lambda )^k}{k!} \right)
		= \sum_{k=1}^{\infty} \frac{\lambda ^{2k}}{(2k)!}
		\leq 1 + \sum_{k=1}^{\infty} \frac{(\lambda ^2)^k}{2^k k!}
		= e^{\lambda ^2 / 2}
	\]
	since \((2k)! \geq 2^k \cdot k!\).
\end{explanation}

In fact, the above can be generalized for any bounded random variable.

\begin{lemma}\label{lma:bounded-rv-is-sub-gaussian}\todo{Add proof}
	Given \(X\in [a, b]\) such that \(\mathbb{E}_{}\left[X \right] = 0\). Then
	\[
		\mathbb{E}_{}\left[e^{\lambda X} \right] \leq \exp (\lambda ^2 \frac{(b-a)^2}{8})
	\]
	for all \(\lambda \in \mathbb{R} \), i.e., \(X\in \mathop{\mathrm{Subg}}((b-a)^2 / 4) \).
\end{lemma}
\begin{proof}
	We will prove this with a worse constant. Let \(X^{\prime} \overset{\text{i.i.d.} }{\sim } X\) be an i.i.d.\ copy, then
	\[
		\mathbb{E}_{}\left[e^{\lambda X} \right]
		= \mathbb{E}_{}\left[e^{\lambda (X - \mathbb{E}_{}\left[X^{\prime}  \right] )} \right]
		= \mathbb{E}_{}\left[e^{\lambda X}\cdot e^{-\lambda (\mathbb{E}_{}\left[X^{\prime}  \right] )} \right]
		\leq \mathbb{E}_{}\left[e^{\lambda X} \right] \cdot \mathbb{E}_{}\left[e^{-\lambda X^{\prime} } \right]
		= \mathbb{E}_{}\left[e^{\lambda (X - X^{\prime} )} \right],
	\]
	where we have used the \href{https://en.wikipedia.org/wiki/Jensen%27s_inequality}{Jensen's inequality} for \(e^{-\lambda \mathbb{E}_{}\left[X^{\prime}  \right] } \leq \mathbb{E}_{}\left[e^{-\lambda X^{\prime} } \right] \).\footnote{This is a trick called symmetrization. A basic example is \(\Var_{}\left[X \right] = \frac{1}{2} \mathbb{E}_{}\left[(X - X^{\prime} )^2 \right] \).} Now we introduce a \hyperref[eg:Rademacher-random-varaible]{Rademacher random variable} \(\epsilon = \pm 1\), to further write
	\[
		\mathbb{E}_{}\left[e^{\lambda X} \right]
		\leq \mathbb{E}_{X, X^{\prime} }\left[e^{\lambda (X - X^{\prime} )} \right]
		= \mathbb{E}_{X, X^{\prime} , \epsilon }\left[ e^{\lambda \cdot \epsilon (X - X^{\prime} )} \right]
		= \mathbb{E}_{X, X^{\prime} }\left[ \mathbb{E}_{\epsilon }\left[e^{\lambda \epsilon (X - X^{\prime} )} \right]  \right] ,
	\]
	and \(\mathbb{E}_{\epsilon }\left[ e^{\lambda \epsilon (X - X^{\prime} )} \right] \leq \mathbb{E}_{}\left[e^{\frac{\lambda ^2(X - X^{\prime} )}{2}} \right] \leq e^{\frac{\lambda ^2(b - a)^2}{2}} \), where we used the known bound on MGF of a \hyperref[eg:Rademacher-random-varaible]{Rademacher random variable}, hence overall, we get
	\[
		\mathbb{E}_{}\left[e^{\lambda X} \right] \leq \mathbb{E}_{X, X^{\prime} }\left[e^{\frac{\lambda ^2(b-a)^2}{2}} \right] = e^{\frac{\lambda ^2(b-a)^2}{2}}.
	\]
\end{proof}

\begin{note}
	If \(a = -1\) and \(b = 1\), we get back to the earlier example.
\end{note}

Just like independent Gaussians, sums of independent \hyperref[def:sub-gaussian]{sub-gaussians} remain \hyperref[def:sub-gaussian]{sub-gaussian}.

\begin{lemma}[Closed under convolution]\label{lma:sub-gaussian-add}
	Let \(X_i\) be independent random variables with \(\mathbb{E}_{}\left[X_i \right] = \mu _i\), and \(X_i - \mu _i \in \mathop{\mathrm{Subg}}\left( \sigma _i^2 \right)  \). Then
	\[
		\sum_{i=1}^n X_i - \sum_{i=1}^n \mu _i \in \mathop{\mathrm{Subg}}\left( \sum_{i=1}^n \sigma _i^2 \right) .
	\]
\end{lemma}
\begin{proof}
	We simply observe that
	\[
		\mathbb{E}_{}\left[e^{\lambda \sum_{i} (X_i - \mu _i)} \right]
		= \prod _{i=1}^n \mathbb{E}_{}\left[e^{\lambda (X_i - \mu _i)} \right]
		\leq e^{\frac{\lambda ^2 (\sum_{i} \sigma _i^2)}{2}}.
	\]
\end{proof}

\subsection{Hoeffding's Inequality}
We can now immediately prove the famous \hyperref[thm:Hoeffding-inequality]{Hoeffding's inequality}, which is the main tool in our interest.

\begin{theorem}[Hoeffding's inequality for sub-gaussian random variables]\label{thm:Hoeffding-inequality}
	Let \(X_i\) be independent random variables with \(\mathbb{E}_{}\left[X_i \right] = \mu _i\), and \(X_i - \mu _i \in \mathop{\mathrm{Subg}}(\sigma _i^2) \). Then for all \(t \geq 0\),\footnote{One-sided version holds without the factor \(2\).}
	\[
		\mathbb{P} \left( \left\vert \sum_{i=1}^n (X_i - \mu _i) \right\vert \geq t \right) \leq 2 \exp (\frac{-t^2}{2 \sum_{i} \sigma _i^2}).
	\]
\end{theorem}
\begin{proof}
	It's immediate from \autoref{lma:sub-gaussian-add} and the equivalent condition \autoref{lma:sub-gaussian-b} in \autoref{lma:sub-gaussian}.
\end{proof}