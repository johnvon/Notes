\lecture{4}{28 Aug.\ 9:00}{McDiarmid's Inequality}
\section{Bounded Difference Concentration Inequality}
\subsection{Applications of Berstein's Inequality to Bounded Random Variables}
Now we see some applications of \hyperref[thm:Bernstein-inequality]{Bernstein's inequality}, addressing weaknesses of \hyperref[thm:Hoeffding-inequality]{Hoeffding's inequality}.

\begin{lemma}\label{lma:Bernstein-inequality-for-bounded-random-variable}
	Let \(\vert X - \mu \vert \leq b\) and \(X - \mu \) is \(\Subg(b^2) \). It's also true that \(X - \mu \in \SubExp(2 \sigma ^{2} , 2b) \) where \(\Var_{}\left[X \right] = \sigma ^{2} \).
\end{lemma}
\begin{proof}
	From \((X - \mu )^k \leq (X - \mu )^2 \vert X - \mu \vert ^{k - 2} \leq (X - \mu )^2 b^{k-2}\), we have
	\[
		\mathbb{E}_{}\left[e^{\lambda (X - \mu )} \right]
		= 1 + \frac{\lambda ^{2} }{2} \sigma ^{2} + \sum_{k=3}^{\infty} \lambda ^k \frac{\mathbb{E}_{}\left[X - \mu  \right] ^k}{k!}
		\leq 1 + \frac{\lambda ^2 \sigma ^2}{2} + \frac{\lambda \sigma ^2}{2} \sum_{k=3}^{\infty} (\vert \lambda  \vert b)^{k - 2}.
	\]
	The last sum is a geometric series, which converges if \(\vert \lambda  \vert < 1 / b\) to
	\[
		1 + \frac{\lambda ^2 \sigma ^2}{2} \left( \frac{1}{1 - b \vert \lambda \vert } \right) .
	\]
	Then from \(1 + x \leq e^x\), we see that for \(\vert \lambda \vert < 1/2b\),
	\[
		\mathbb{E}_{}\left[e^{\lambda (X - \mu )} \right]
		\leq e^{\frac{\lambda ^2 \sigma ^{2} }{2(1 - b \vert \lambda \vert )}}
		\leq e^{\lambda ^{2} \sigma ^{2} }.
	\]
\end{proof}

From this, by directly apply \hyperref[thm:Bernstein-inequality]{Bernstein's inequality}, we have the following.

\begin{corollary}\label{col:Bernstein-inequality-for-bounded-random-variable*}
	Let \(X\) be a random variable such that \(\vert X - \mu \vert \leq b\). For any \(t > 0\),
	\[
		\mathbb{P} (\vert X - \mu  \vert \geq t) \leq 2 \exp \left( \frac{- t^2}{2(2\sigma ^{2} + t\cdot 2b)} \right).
	\]
	Furthermore, let \(X_1, \dots , X_n\) be independent random variables with \(\mathbb{E}_{}\left[X_i \right] = \mu _i\) and \(\Var_{}\left[X_i \right] = \sigma _i^{2}\) such that \(\vert X_i - \mu _i \vert \leq b\) for all \(i\). Then for any \(t > 0\),
	\[
		\mathbb{P} \left( \left\vert \sum_{i=1}^n (X_i - \mu _i) \right\vert \geq t \right) \leq 2 \exp \left( \frac{-t^2}{4\left( \sum_{i} \sigma _i^2 + tb\right) } \right).
	\]
	In particular, if \(\mu _i = \mu \) for all \(i\), then
	\[
		\Pr_{}\left( \left\vert \frac{1}{n} \sum_{i=1}^{n} X_i - \mu \right\vert \geq t \right)  \leq 2 \exp \left( - \frac{nt^2}{4(\sigma ^{2} + tb)} \right) .
	\]
\end{corollary}

\begin{remark}
	Observe that in the last line of the proof of \autoref{lma:Bernstein-inequality-for-bounded-random-variable}, the inequality is quite loose. This means that we can explicitly maximize the quantity in the exponent over \(\vert \lambda \vert \in (0 , 1 / 2b)\) to get a higher bound and hence, a better variance factor. This leads to a tighter version of \autoref{col:Bernstein-inequality-for-bounded-random-variable*}.
\end{remark}

\begin{corollary}\label{col:Bernstein-inequality-for-bounded-random-variable}
	Let \(X_1, \dots , X_n\) be independent random variables with \(\mathbb{E}_{}\left[X_i \right] = \mu \) and \(\Var_{}\left[X_i \right] = \sigma ^{2}\) such that \(\vert X_i - \mu \vert \leq b\) for all \(i\). Then for any \(t > 0\),
	\[
		\mathbb{P} \left( \left\vert \sum_{i=1}^{n} X_i - \mu \right\vert \geq t \right) \leq 2 \exp \left( \frac{- t^2 / 2}{n \sigma ^{2} + bt / 3 } \right) .
	\]
	In particular,
	\[
		\mathbb{P} \left( \left\vert \frac{1}{n}\sum_{i=1}^{n} X_i - \mu \right\vert \geq t \right) \leq 2 \exp \left( \frac{- nt^2 / 2}{\sigma ^{2} + bt / 3} \right).
	\]
\end{corollary}

\begin{note}
	From \autoref{col:Bernstein-inequality-for-bounded-random-variable}:
	\begin{itemize}
		\item if \(t \leq 3 \sigma ^{2} / b\), the tail of the sample mean behaves like a \hyperref[def:sub-Gaussian]{sub-Gaussian} tail;
		\item if \(t > 3\sigma ^{2} / b\), the tail of the sample mean behaves like a \hyperref[def:sub-exponential]{sub-exponential} tail.
	\end{itemize}
\end{note}

\begin{remark}
	In practice, since we know that sample mean is \(\sqrt{n} \)-consistent, we generally look at a sequence of quantiles of the sample mean that is of \(O(n^{-1 / 2})\). Therefore, the tail behavior when \(t\) gets large, is practically irrelevant.
\end{remark}

By choosing the appropriate \(t\) in the above tail bound, we can get the following confidence interval for \(\mu \).

\begin{corollary}\label{col:Bernstein-confidence-interval}
	Under the assumption of \autoref{col:Bernstein-inequality-for-bounded-random-variable},
	\[
		\mathbb{P} \left( \left\vert \frac{1}{n} \sum_{i=1}^{n} X_i - \mu  \right\vert \leq \frac{\sigma}{\sqrt{n} } \sqrt{2 \log \frac{2}{\alpha }} + \frac{3b}{3n} \log \frac{2}{\alpha } \right) \geq 1 - \alpha
	\]
\end{corollary}
\begin{proof}
	Let
	\[
		\alpha = 2 \exp \left( \frac{- t^2}{2(V + bt / 3)} \right),
	\]
	then
	\[
		t^2 - \frac{2tb}{3}\log \frac{2}{\alpha } - 2 V \log \frac{2}{\alpha } = 0.
	\]
\end{proof}

In \autoref{col:Bernstein-confidence-interval}, we have an \(O(1 / \sqrt{n} )\) term, which is similar to the \hyperref[rmk:Hoeffding-confidence-interval]{one} derived from \hyperref[thm:Hoeffding-inequality]{Hoeffding's inequality} for bounded random variables. In contrary to the Hoeffding's bound, we have an additional lower order term here.

\begin{remark}
	Observe that the higher order term in \autoref{col:Bernstein-confidence-interval} involves the variance, whereas in the case of \hyperref[rmk:Hoeffding-confidence-interval]{Hoeffding}, it involves the range. Therefore, for random variables with large range but highly concentrated around its mean, the \hyperref[rmk:Hoeffding-confidence-interval]{Hoeffding confidence interval} would be much wider.
\end{remark}

The above remark is demonstrated by the following example.

\begin{eg}
	Let \(X_1 , \dots , X_n \overset{\text{i.i.d.} }{\sim } \operatorname{Ber}(p) \). Suppose we observe \(X_i = 0\) for all \(i\), then \(\hat{p} = \overline{X} = 0\) and the estimate of \(\Var_{}\left[X_1 \right] \) would be \(\hat{p} (1 - \hat{p} ) = 0\).

	Hence, if we plug this estimate of variance into the \hyperref[col:Bernstein-confidence-interval]{confidence bound from Bernstein}, the length of which would be \(O(1 / n)\). However, in the case of \hyperref[rmk:Hoeffding-confidence-interval]{Hoeffding} (which works with the range, in this case, \(1\)), the length would be \(O(1 / \sqrt{n} )\).
\end{eg}

\subsection{McDiarmid's Inequality}
Now we go back to the discussion about \hyperref[def:EP]{empirical process}. We do the first step, i.e., we want to show
\[
	S_n = \sup _{f \in \mathscr{F} } \left\vert \frac{1}{n}\sum_{i=1}^{n} f(X_i) - \mathbb{E}_{}\left[f(X) \right]  \right\vert
\]
``concentrates'' when \(\mathscr{F} \) is bounded provided that
\[
	\sup _{x\in \chi , f\in \mathscr{F} } \vert f(x) \vert \leq B.
\]

One simple example of bounded function class arises in the task of classification.

\begin{eg}[Classification]
	Consider \(f(x)\) corresponds to the class label of an observation with feature value \(x\), then the class is bounded.
\end{eg}

However, since \(S_n\) falls neither into the category of \hyperref[thm:Hoeffding-inequality]{Hoeffding} nor \hyperref[thm:Bernstein-inequality]{Bernstein}, we would need a more general concentration inequalities: the \hyperref[thm:McDiarmid-inequality]{McDiarmid's inequality}, which considers the \(f\) satisfying the \hyperref[def:bounded-difference-property]{bounded-difference property}.\footnote{Hence it's also known as the \emph{bounded difference inequality}.}

\begin{definition}[Bounded-difference property]\label{def:bounded-difference-property}
	Let \(X_1, \dots , X_n\) be i.i.d.\ random variables on \(\chi \). We say \(f\colon \chi ^n \to \mathbb{R} \) satisfies the \emph{bounded-difference property} with parameters \(c_i\) if for all \(i\),
	\[
		\sup _{x_1, \dots , x_n, x_i^{\prime} }\vert f(x_1, \dots , x_n) - f(x_1, \dots , x_i^{\prime} , \dots , x_n) \vert \leq c_i.
	\]
\end{definition}

\begin{theorem}[McDiarmid's inequality]\label{thm:McDiarmid-inequality}
	Let \(X_1, \dots , X_n\) be i.i.d.\ random variables on \(\chi \), and let \(f\colon \chi ^n \to \mathbb{R} \) satisfying the \hyperref[def:bounded-difference-property]{bounded-difference property} with parameters \(c_1, \dots , c_n\). Then for any \(t > 0\),
	\[
		\mathbb{P} (f(X_1, \dots , X_n) - \mathbb{E}_{}\left[f (X_1, \dots , X_n)\right] \geq t) \leq \exp \left( \frac{-2t^2}{\sum_{i} c_i^2} \right).
	\]
	The same bound holds for the left tail.
\end{theorem}

\begin{remark}
	The qualitative statement for \hyperref[thm:McDiarmid-inequality]{McDiarmid's inequality} is that ``a random variable that depends on the influence of many independent random variables but not too many on any one of them concentrates''.
\end{remark}
\begin{explanation}
	Typically, \(\sum_{i} c_i = O(1)\) concentration will happen if \(\sum_{i} c_i^2 = o(1)\). For example, if each \(c_i = O(1 / n)\), then concentration happens but not when all \(c_i = 0\) except one of them is \(1\).
\end{explanation}

\begin{remark}
	\hyperref[thm:McDiarmid-inequality]{McDiarmid's inequality} is a generalization of \hyperref[thm:Hoeffding-inequality]{Hoffding's inequality}.
\end{remark}
\begin{explanation}
	Let
	\[
		f(x_1, \dots , x_n) = \frac{1}{n}(x_1 + \dots + x_n).
	\]
	When \(X_i\)'s are independent and \(X_i \in [a_i, b_i]\) for all \(i\), it's easy to observe that when we change the \(i^{\text{th} }\) argument of \(f\), the value of \(f\) can change at most by \((b_i - a_i) / n\), i.e., \hyperref[thm:McDiarmid-inequality]{McDiarmid's inequality} is satisfied with \(c_i \coloneqq (b_i - a_i) / n\), plugging in, we get back \hyperref[thm:Hoeffding-inequality]{Hoffding's inequality}.
\end{explanation}

With \hyperref[thm:McDiarmid-inequality]{McDiarmid's inequality}, we can check that the following holds for bounded function classes \(\mathscr{F} \):
\[
	\left\vert S_n(x_1, \dots , x_n) - S_n(x_1, \dots , x_i^{\prime} , \dots , x_n) \right\vert \leq \frac{2B}{n} \eqqcolon c_i.
\]
Then from \hyperref[thm:McDiarmid-inequality]{McDiarmid's inequality}, for any \(t > 0\),
\[
	\mathbb{P} (S_n \geq \mathbb{E}_{}\left[S_n \right] + t) \leq \exp \left( \frac{-nt^2}{2B^2} \right) \eqqcolon \delta ,
\]
or equivalently, \(S_n \leq \mathbb{E}_{}\left[S_n \right] + B \sqrt{\frac{2}{n}\log \frac{1}{\delta }}\) with probability at least \(1 - \delta \).

\begin{note}
	\(B \sqrt{\frac{2}{n} \log \frac{1}{\delta }} \) is a lower order term, i.e., \(\mathbb{E}_{}\left[S_n \right] \) dominates it.
\end{note}
\begin{explanation}
	Since\footnote{This upper bound is pretty weak, and we will eventually work on getting better bounds. }
	\[
		O(B)
		\geq \mathbb{E}_{}\left[S_n \right]
		\geq \mathbb{E}_{}\left[\left\vert \frac{1}{n} \sum_{i=1}^{n} f(x_i) - \mathbb{E}_{}\left[f(X) \right] \right\vert \right]
		= O\left( \sqrt{\frac{\Var_{}\left[f(X_1) \right] }{n}}  \right)
		\approx O\left( \frac{1}{\sqrt{n} } \right) .
	\]
\end{explanation}

All these imply that \emph{it's enough to bound \(\mathbb{E}_{}\left[S_n \right] \)}.