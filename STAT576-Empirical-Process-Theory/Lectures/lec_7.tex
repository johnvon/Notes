\lecture{7}{8 Sep.\ 9:00}{Bracketing and Symmetrization}
Our main \hyperref[def:EP]{empirical process} is so far \(\mathbb{E}_{}\left[\sup _{f\in \mathscr{F} } \mathbb{P} _n f - \mathbb{P} f \right]\). Let's first focus on the \hyperref[eg:1D-classification-thresholds]{1-D thresholds classification}, i.e., we want to bound the supremum
\[
  \mathbb{E}_{}\left[\sup _{\theta \in \mathbb{R} } \left\vert \frac{1}{n} \sum_{i=1}^{n} \mathbbm{1}_{x_i \leq \theta } - \mathbb{P} (X \leq \theta ) \right\vert \right] .
\]
There are \(2\) approaches to bound this supremum: bracketing and symmetrization.

\subsection{Bracketing}
The main idea of bracketing is the following.

\begin{intuition}
  Reduce an infinite number of random variables to finite, which will be more manageable.
\end{intuition}

Assume that \(\mathbb{P} \) is continuous, and consider a finite set \(\{ \theta _i \}_{i = 0}^{N+1} \) with \(\theta _0 = -\infty \), \(\theta _{N+1} = \infty \), such that they correspond to quantile of \(\mathbb{P} \), i.e.,
\[
  \mathbb{P} (\theta _i \leq X \leq \theta _{i+1}) = \frac{1}{N+1}.
\]
Given a \(\theta \), \(X\) will lie in between two adjacent \(\theta _i\)'s in the sequence. Denote the upper-bound as \(u(\theta )\) and the lower-bound as \(\ell (\theta )\) for this \(\theta \), then
\begin{align}
  \mathbb{P} (X \leq \theta ) - \frac{1}{n} \sum_{i=1}^{n} \mathbbm{1}_{x_i \leq \theta }
   & \leq \mathbb{P} (X \leq u(\theta )) - \frac{1}{n}\sum_{i=1}^{n} \mathbbm{1}_{x_i \leq \ell (\theta )}                                                                                           \nonumber            \\
   & \leq \mathbb{E}_{}\left[\mathbbm{1}_{X \leq u(\theta )}  \right] - \frac{1}{n}\sum_{i=1}^{n} \mathbbm{1}_{x_i \leq \ell (\theta )}                                                              \nonumber            \\
   & \leq \mathbb{E}_{}\left[\mathbbm{1}_{X \leq \ell (\theta )}  \right] - \frac{1}{n}\sum_{i=1}^{n} \mathbbm{1}_{x_i \leq \ell (\theta )} + \mathbb{P} (\ell (\theta ) \leq X \leq u(\theta ))     \nonumber            \\
   & \leq \mathbb{E}_{}\left[\mathbbm{1}_{X \leq \ell (\theta )}  \right] - \frac{1}{n}\sum_{i=1}^{n} \mathbbm{1}_{x_i \leq \ell (\theta )} + \frac{1}{N+1}                                          \nonumber            \\
  \shortintertext{if we take the supremum over \(\ell (\theta) \in \mathbb{R} \) instead of \(\theta \),}
   & \leq \frac{1}{N+1} + \mathbb{E}_{}\left[\max _{0 \leq j \leq N} \mathbb{E}_{}\left[\mathbbm{1}_{X \leq \theta _j}\right] - \frac{1}{n} \sum_{i=1}^{n} \mathbbm{1}_{x_i \leq \theta _j} \right].\label{eq:bracketing}
\end{align}

To further bound \autoref{eq:bracketing}, recall the following.

\begin{prev}
  If \(X_i \sim \Subg(\sigma ^2) \) independent, \(\sum_{i} a_i X_i \sim \Subg((\sum_{i} a_i^2) \sigma ^2) \) from \autoref{lma:sub-gaussian-add}.
\end{prev}

\begin{remark}
  Let \(a_i = 1/n\), we see that \(\mathbb{E}_{}\left[\mathbbm{1}_{X \leq \theta _j}\right] - \frac{1}{n} \sum_{i=1}^{n} \mathbbm{1}_{x_i \leq \theta _j} \in \Subg(1 / n) \).\footnote{Since it's bounded between \(0\) and \(1\).}
\end{remark}

Finally, recall what we have proved in the homework.

\begin{lemma}\label{lma:lec7}
  Let \(X_1, \dots , X_n \sim \Subg(\sigma ^{2} ) \),\footnote{Not necessary independent.} then \(\mathbb{E}_{}\left[\max _i X_i \right] \leq \sqrt{2 \sigma ^{2} \log n}\).
\end{lemma}

Then, we can show the final bound.

\begin{proposition}[Bracketing]\label{prop:bracketing}
  Let \(x_1, \dots , x_n \overset{\text{i.i.d.} }{\sim } \mathbb{P} \), and \(\mathscr{F} = \{ \mathbbm{1}_{X \leq \theta} \colon \theta \in \mathbb{R} \} \). Then
  \[
    \mathbb{E}_{X}\left[ \sup _{f\in \mathscr{F} } \left( \mathbb{P} (X \leq \theta ) - \frac{1}{n}\sum_{i=1}^{n} \mathbbm{1}_{x_i \leq \theta } \right) \right] = O\left( \sqrt{\frac{\log n}{n}}  \right).
  \]
\end{proposition}
\begin{proof}
  From \autoref{lma:lec7}, since we have \((N+1)\) random variables with variance factor \(1 / n\), by choosing \(N+1 \coloneqq n\),\footnote{Recall that \(n\) is the sample size, so we can choose the corresponding \(n\) to meet the requirement.} \autoref{eq:bracketing} can be further bounded by
  \[
    \sqrt{\frac{2 \log (N+1)}{n}} + \frac{1}{N+1} = O\left( \sqrt{\frac{\log n}{n}}  \right).
  \]
\end{proof}
\subsection{Symmetrization}
Another technique called symmetrization, which is essentially stated in the following lemma.

\begin{lemma}[Symmetrization]\label{lma:symmetrization}
  Given a function class \(\mathscr{F} = \{ f\colon \chi \to \mathcal{Y} \} \) and \(X_1, \dots , X_n \overset{\text{i.i.d.} }{\sim } \mathbb{P} \), and \(\epsilon _1, \dots , \epsilon _n\) be i.i.d.\ \hyperref[eg:Rademacher-random-varaible]{Rademacher random variables}. Then
  \[
    \max \left( \mathbb{E}_{}\left[\sup _{f\in \mathscr{F} } \mathbb{P} _n f - \mathbb{P} f \right], \mathbb{E}_{}\left[\sup _{f\in \mathscr{F} } \mathbb{P} f - \mathbb{P} _n f \right] \right)
    \leq 2 \mathbb{E}_{\epsilon _i, X_i}\left[\sup _{f\in \mathscr{F} }\frac{1}{n}\sum_{i=1}^{n} \epsilon _i f(X_i) \right].
  \]
  In particular,
  \[
    \mathbb{E}_{}\left[\sup _{f\in \mathscr{F} }\left\vert \mathbb{P} _n f - \mathbb{P} f \right\vert  \right]
    \leq 2 \mathbb{E}_{\epsilon _i, X_i}\left[\sup _{f\in \mathscr{F} }\left\vert \frac{1}{n}\sum_{i=1}^{n} \epsilon _i f(X_i) \right\vert  \right] .
  \]
\end{lemma}
\begin{proof}
  Let \(X_i^{\prime} \)'s be i.i.d.\ copies of \(X_i\)'s for all \(i\). Since adding a sign \(\epsilon _i\) won't change the expectation,\footnote{Since the distributions of \(f(X_i^{\prime} ) - \sum_{i} f(X_i)\) and \(f(X_i) - \sum_{i} f(X_i^{\prime} )\) are the same.}
  \[
    \begin{split}
      \mathbb{E}_{}\left[\sup _{f\in \mathscr{F} } \mathbb{E}_{}\left[f(X) \right] - \frac{1}{n}\sum_{i=1}^{n} f(X_i) \right]
      &=\mathbb{E}_{}\left[\sup _{f\in \mathscr{F} } \mathbb{E}_{X_i^{\prime} }\left[ \frac{1}{n} \sum_{i=1}^{n} f(X_i^{\prime} ) - \frac{1}{n} \sum_{i=1}^{n} f(X_i)\right]  \right]\\
      &\leq \mathbb{E}_{X_i}\left[ \mathbb{E}_{X_i^{\prime} }\left[ \sup _{f\in \mathbb{F} } \frac{1}{n} \sum_{i=1}^{n} (f(X_i^{\prime} ) - f(X_i)) \right]  \right] \\
      &= \mathbb{E}_{X_i, X_i^{\prime} , \epsilon _i}\left[ \sup _{f\in \mathbb{F} } \frac{1}{n} \sum_{i=1}^{n} (f(X_i^{\prime} ) - f(X_i))\epsilon _i \right] \\
      &\leq \mathbb{E}_{X_i^{\prime} , \epsilon _i}\left[ \sup _{f\in \mathscr{F} } \frac{1}{n}\sum_{i=1}^{n} f(X_i^{\prime} ) \epsilon _i \right]
      + \mathbb{E}_{X_i, \epsilon _i}\left[ \sup _{f\in \mathscr{F} } \frac{1}{n}\sum_{i=1}^{n} f(X_i ) \epsilon _i \right]\\
      &= 2 \mathbb{E}_{}\left[\sup _{f\in \mathscr{F} } \frac{1}{n}\sum_{i=1}^{n} \epsilon _i f(X_i) \right] .
    \end{split}
  \]
\end{proof}

\begin{intuition}
  If we condition on \(X_i\)'s, the bound can be seen as linear combination of \hyperref[eg:Rademacher-random-varaible]{Rademacher random variables}. Thus, we can refer to properties of \hyperref[def:sub-gaussian]{sub-gaussian} random variables.
\end{intuition}

The upper-bound deserves a special name.

\begin{definition}[Rademacher complexity]\label{def:Rademacher-complexity}
  Let \(X_i \overset{\text{i.i.d.} }{\sim } \mathbb{P} \) be independent and \(\epsilon _i\) be i.i.d.\ \hyperref[eg:Rademacher-random-varaible]{Rademacher random variables}. The \emph{Rademacher complexity} of a function class \(\mathscr{F} \) w.r.t.\ \(\mathbb{P} \) is
  \[
    R_n(\mathscr{F} ) \coloneqq \mathbb{E}_{\epsilon _i, X_i}\left[\sup _{f\in \mathscr{F} }\left\vert \frac{1}{n}\sum_{i=1}^{n} \epsilon _i f(X_i) \right\vert  \right] .
  \]
\end{definition}

On the other hand, the opposite direction of \hyperref[lma:symmetrization]{symmetrization lemma} also holds.
\begin{lemma}
  Given a function class \(\mathscr{F} = \{ f\colon \chi \to \mathcal{Y} \} \) and \(X_1, \dots , X_n \overset{\text{i.i.d.} }{\sim } \mathbb{P} \), and \(\epsilon _1, \dots , \epsilon _n\) be i.i.d.\ \hyperref[eg:Rademacher-random-varaible]{Rademacher random variables}. Then
  \[
    \mathbb{E}_{X_i, \epsilon _i}\left[\sup _{f\in \mathscr{F} } \left\vert \frac{1}{n}\sum_{i=1}^{n} \epsilon _i f(X_i) \right\vert \right]
    \leq 2 \mathbb{E}_{}\left[\sup _{f\in \mathscr{F} } \left\vert \mathbb{P} _n f - \mathbb{P} f \right\vert  \right] + \frac{1}{\sqrt{n} } \sup _{f\in \mathscr{F} } \vert \mathbb{P} f \vert.
  \]
\end{lemma}
\begin{proof}
  This technique is so-called \emph{desymmetrization}: Consider
  \[
    \begin{split}
      \mathbb{E}_{\epsilon _i, X_i}&\left[\sup _{f\in \mathscr{F} } \left\vert \frac{1}{n}\sum_{i=1}^{n} \epsilon _i f(X_i) \right\vert \right]\\
      &\leq \mathbb{E}_{\epsilon _i, X_i}\left[\sup _{f\in \mathscr{F} } \left\vert \frac{1}{n}\sum_{i=1}^{n} \epsilon _i (f(X_i) - \mathbb{E}_{}\left[f(X) \right] ) \right\vert \right]
      + \mathbb{E}_{\epsilon _i}\left[\sup _{f\in \mathscr{F} } \left\vert \frac{1}{n}\sum_{i=1}^{n} \epsilon _i \mathbb{E}_{}\left[f(X) \right] \right\vert \right]\\
      &= \mathbb{E}_{\epsilon _i, X_i, X_i^{\prime} }\left[\sup _{f\in \mathscr{F} } \left\vert \frac{1}{n}\sum_{i=1}^{n} \epsilon _i (f(X_i) - \mathbb{E}_{}\left[f(X_i^{\prime} ) \right] ) \right\vert \right]
      + \mathbb{E}_{}\left[\sup _{f\in \mathscr{F} } \left\vert \frac{1}{n}\sum_{i=1}^{n} \epsilon _i \mathbb{E}_{\epsilon _i}\left[f(X_i) \right] \right\vert \right].
    \end{split}
  \]
  The first term can be further bounded by
  \[
    \begin{split}
      \mathbb{E}_{\epsilon _i, X_i, X_i^{\prime} }\left[\sup _{f\in \mathscr{F} } \left\vert \frac{1}{n}\sum_{i=1}^{n} \epsilon _i (f(X_i) - \mathbb{E}_{}\left[f(X_i^{\prime} ) \right] ) \right\vert \right]
      &\leq \mathbb{E}_{\epsilon _i, X_i, X_i^{\prime} }\left[ \sup _{f\in \mathscr{F} } \left\vert \frac{1}{n}\sum_{i=1}^{n} \epsilon _i (f(X_i) - f(X_i^{\prime} )) \right\vert  \right]\\
      &= \mathbb{E}_{}\left[\sup _{f\in \mathscr{F} } \frac{1}{n}\sum_{i=1}^{\infty} ( f(X_i) - f(X_i^{\prime} )) \right] \\
      &= \mathbb{E}_{}\left[\sup _{f\in \mathscr{F} } \left\vert \frac{1}{n}\sum_{i=1}^{n} \big( f(X_i) - f(X_i^{\prime} ) + (\mathbb{E}_{}\left[f \right] - \mathbb{E}_{}\left[f \right] ) \big) \right\vert  \right]\\
      &= 2 \mathbb{E}_{}\left[\sup _{f\in \mathscr{F} } \left\vert \mathbb{P} _n f - \mathbb{P} f \right\vert  \right] ,
    \end{split}
  \]
  and the second term ca be bounded by
  \[
    \mathbb{E}_{\epsilon _i}\left[\sup _{f\in \mathscr{F} } \left\vert \frac{1}{n}\sum_{i=1}^{n} \epsilon _i \mathbb{E}_{}\left[f(X) \right] \right\vert \right]
    \leq \sup _{f\in \mathscr{F} } \left\vert \mathbb{E}_{}\left[f(X) \right]  \right\vert \cdot \mathbb{E}_{}\left[\left\vert \frac{1}{n} \sum_{i=1}^{n} \epsilon _i \right\vert \right]
    \leq \frac{1}{\sqrt{n} } \sup _{f\in \mathscr{F} } \vert \mathbb{P} f \vert
  \]
  where \(\mathbb{E}_{}\left[\left\vert \frac{1}{n} \sum_{i} \epsilon _i \right\vert \right] \leq \frac{c}{\sqrt{n} } \) with \(c = 1\). Combine them together, we have the final result.
\end{proof}