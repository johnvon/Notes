\lecture{22}{20 Oct.\ 9:00}{More Examples on Rate of Convergence}
Before we extend \autoref{thm:non-asymptotic-rate-of-convergence} to handle the local \hyperref[def:growth-condition*]{growth condition}, we note the following.

\begin{remark}
	The \hyperref[def:rate-of-convergence]{rate} obtained from \autoref{thm:non-asymptotic-rate-of-convergence} is usually correct; on the other hand, the probability tail bound obtained from \autoref{thm:non-asymptotic-rate-of-convergence} is
	\[
		\mathbb{P} (d(\hat{\theta} , \theta _0) > t \delta _n) \lesssim \frac{1}{t},
	\]
	with \(t = 2^M\) in the argument, which can be weak in the sense that it does not imply \(\mathbb{E}_{}[d(\hat{\theta} , \theta _0) ] = O(\delta _n)\). Potentially, more sophisticated concentration arguments can be used.
\end{remark}

\begin{remark}
	The main step to apply \autoref{thm:non-asymptotic-rate-of-convergence} is to bound the expected supremum of \hyperref[def:localized-EP]{localized empirical process}, which can be hard.
\end{remark}

Finally, as shown in some situations, we cannot expect the \hyperref[def:growth-condition*]{growth condition} to hold for all \(\theta \in \Theta \); instead, typically we only have \(\theta \in B(\theta _0, u^{\ast} )\) for some \(u^{\ast} \in \mathbb{R} \). In this case, a variation of \autoref{thm:non-asymptotic-rate-of-convergence} still holds~\cite[Theorem 3.2.5]{vandervaartWeakConvergenceEmpirical1996}.

\begin{theorem}\label{thm:non-asymptotic-rate-of-convergence-extend}
	For an \hyperref[prb:M-estimation]{\(M\)-estimation problem}, assume the \hyperref[def:growth-condition*]{growth condition} on \(M\) holds for \(\theta \in B(\theta _0, u^{\ast} )\) for some \(u^{\ast} \), and the \hyperref[def:sub-quadratic-assumption]{sub-quadratic assumption} (with parameter \(\alpha < 2\)) and the \hyperref[def:rate-determining-equation]{rate-determining equation} are valid for \(\phi _n\)'s arose from bounding the \hyperref[def:localized-EP]{localized empirical process},
	\[
		\mathbb{P} (d(\hat{\theta} _n, \theta _0) > 2^M \delta _n) \leq 4c \sum_{j > M} 2^{(\alpha -2) j}.
	\]
\end{theorem}
\begin{proof}
	We again start by doing the \hyperref[eq:peeling-step]{peeling step}, but this time consider
	\[
		\mathbb{P} (d(\hat{\theta} , \theta _0) > 2^M \delta _n)
		\leq \sum_{\substack{j > M\colon \\ 2^j \delta _n \leq u^{\ast} }} \mathbb{P} (2^{j-1} \delta _n < d(\hat{\theta} , \theta _0) < 2^j \delta _n) + \mathbb{P} \left( d(\hat{\theta} , \theta _0) > \frac{u^{\ast} }{2} \right).
	\]
	We then handle the first term as in \autoref{thm:non-asymptotic-rate-of-convergence}, and show the second term goes to \(0\).
\end{proof}

\subsection{More Examples}
We see two more examples of using \autoref{thm:non-asymptotic-rate-of-convergence-extend}.

\begin{eg}[Sample quantile]
	Let \(X_1, \dots , X_n \overset{\text{i.i.d.} }{\sim } \mathbb{P} \)  which has density \(f\) w.r.t.\ Lebesgue measure. Moreover, for \(0 < \tau < 1\), let
	\[
		\rho _{\tau } (x) = \begin{dcases}
			(\tau -1 ), & \text{ if } x < 0 ;    \\
			\tau x,     & \text{ if } x \geq 0 ,
		\end{dcases}
	\]
	and \(m_\theta (x) = \rho _\theta (x - \theta )\) for all \(\theta \in \mathbb{R} \), so
	\[
		M(\theta )= \mathbb{E}_{}\left[m_\theta (x) \right],\quad
		M_n(\theta ) = \frac{1}{n} \sum_{i=1}^{n} \rho _{\tau } (x_i - \theta )
	\]
	We see that \(\theta _0 \coloneqq \argmin_{\theta } M(\theta )\) is the \(\tau ^{\text{th} }\) quantile of \(\mathbb{P} \), and let \(\hat{\theta} \coloneqq \argmin_{\theta } M_n(\theta )\) be the corresponding \hyperref[prb:M-estimation]{\(M\)-estimator}. The \hyperref[def:rate-of-convergence]{rate of convergence} is \(\vert \hat{\theta} _n - \theta _0 \vert = O_p(1 / \sqrt{n} )\).
\end{eg}
\begin{explanation}
	To show the \hyperref[def:rate-of-convergence]{rate of convergence}, consider the following.

	\begin{itemize}
		\item \(\vert m_{\theta _1}(x) - m_{\theta _2}(x) \vert \leq \vert \theta _1 - \theta _2 \vert \), i.e., this is a Lipschitz \hyperref[def:parametric]{parametric} class.
		\item To show the \hyperref[def:growth-condition*]{growth condition}, we need the following.
		      \begin{lemma}
			      For all \(w, v\in \mathbb{R} \),
			      \[
				      \rho _\tau (w - u) - \rho _\tau (w) = - v(\tau - \mathbbm{1}_{w \leq 0} ) + \int_{0}^{v} \left[ \mathbbm{1}_{w \leq z} - \mathbbm{1}_{w \leq 0}  \right]  \,\mathrm{d}z .
			      \]
		      \end{lemma}

		      Then, we can show the \hyperref[def:growth-condition*]{growth condition} satisfy in a neighborhood of \(\theta _0\).

		      \begin{claim}
			      For \(d(\theta , \theta _0) = \vert \theta - \theta _0 \vert \), for \(\theta \) in some neighborhood of \(\theta _0\), \(\vert \theta - \theta _0 \vert ^2 \lesssim M(\theta ) - M(\theta _0)\).
		      \end{claim}
		      \begin{explanation}
			      By denoting \(F\) as the CDF of \(f\), we have
			      \begin{align*}
				      M(\theta _0 + \delta ) - M(\theta _0)
				       & = \mathbb{E}_{}\left[\rho _\tau (x - \theta _0 - \delta ) - \rho _\tau (x - \theta _0) \right]                                                                                                                              \\
				       & = \mathbb{E}_{}\left[-\delta (\tau - \mathbbm{1}_{x - \theta _0 \leq 0} ) \right] + \mathbb{E}_{}\left[\int_{0}^{\delta } ( \mathbbm{1}_{x - \theta _0 \leq z} - \mathbbm{1}_{x - \theta _0 \leq 0} ) \,\mathrm{d}z \right] \\
				       & = \int_{0}^{\delta } F(\theta _0 + z) - F(\theta _0) \,\mathrm{d}z                                                                                                                                                          \\
				      \shortintertext{assume there exists a neighborhood of \(\theta _0\) such that \(f \geq L > 0\), then for some \(\xi _z \in (0, \delta )\),}
				       & \geq \int_{0}^{\delta } f(\xi _z) z \,\mathrm{d}z
				      \geq L\cdot \int_{0}^{\delta } z \,\mathrm{d}z
				      = \frac{L \delta ^2}{2},
			      \end{align*}
			      i.e., in this neighborhood, \(M\) grows quadratically in a neighborhood of \(\theta _0\).
		      \end{explanation}
		\item To bound the \hyperref[def:localized-EP]{localized empirical process}, first note that \(\hat{\theta} \) is \hyperref[def:consistent]{consistent},\footnote{This should be proved beforehand, otherwise things doesn't make sense.} and consider \(\mathscr{F} = \{ m_\theta - m_{\theta _0} \colon \vert \theta - \theta _0 \vert \leq t \} \) where the \hyperref[def:localized-EP]{localized empirical process} is
		      \[
			      \mathbb{E}_{}\left[\sup _{\theta \colon \vert \theta - \theta _0 \vert \leq t} \vert (\mathbb{P} _n - \mathbb{P} ) m_\theta - (\mathbb{P} _n - \mathbb{P} ) m_{\theta _0} \vert \right].
		      \]
		      To use the \hyperref[thm:bracketing-bound]{bracketing bound}, we first see that \(F(x) = t\) is an \hyperref[def:envelope]{envelope} with \(\lVert F \rVert _{L_2(\mathbb{P} )} = t\), hence the \hyperref[def:localized-EP]{localized empirical process} can be upper-bounded by
		      \[
			      \frac{t}{\sqrt{n} } \int_{0}^{1} \sqrt{\log N_{[\ ]}(\mathscr{F} , L_2(\mathbb{P} ), \epsilon t)}  \,\mathrm{d}\epsilon
			      \leq \frac{t}{\sqrt{n} } \int_{0}^{1} \sqrt{\log \left( 1 + \frac{4}{\epsilon } \right) } \,\mathrm{d}\epsilon ,
		      \]
		      i.e., the \hyperref[def:localized-EP]{localized empirical process} can be upper-bounded by \(\phi _n(t) \approx t / \sqrt{n} \).
		\item It's evident that \(\phi _n\)'s satisfy the \hyperref[def:sub-quadratic-assumption]{sub-quadratic assumption} with \(\alpha = 1\).
		\item By the \hyperref[def:rate-determining-equation]{rate-determining equation},
		      \[
			      \delta _n / \sqrt{n} \approx \delta _n^2
			      \implies \delta _n = 1 / \sqrt{n},
		      \]
		      implying \(\vert \hat{\theta} _n - \theta _0 \vert = O_p (1 / \sqrt{n}) \).
	\end{itemize}
\end{explanation}

\begin{remark}
	In this sample quantile example, the classical result for \(\tau = 0.5\) shows that
	\[
		\sqrt{n} (\hat{\theta} - \theta _0) \overset{d}{\to } \mathcal{N} \left( 0, \frac{1}{4 ( f(\theta _0) )^2} \right).
	\]
\end{remark}

Another example is the high-dimensional linear regression.

\begin{eg}[High-dimensional linear regression]\label{eg:high-dim-LR}
	Consider \(Z = (Y, X_1, \dots , X_p) \in \mathbb{R} ^{p + 1}\) such that \(Z_1, \dots , Z_n \overset{\text{i.i.d.} }{\sim } \mathbb{P} \), and we want to predict \(Y\) by \(\beta ^{\top} X\). Let \(M(\beta ) = \mathbb{E}_{}\left[(Y - \beta ^{\top} X) ^2 \right] \) with
	\[
		\beta ^{\ast} = \argmin_{\beta \colon \lVert \beta \rVert _1 \leq L}  M(\beta )
	\]
	for some \(L\), and let \(M_n(\beta ) = \frac{1}{n} \sum_{i=1}^{n} (Y^i - \beta ^{\top} X^i)^2\) with
	\[
		\hat{\beta} = \argmin_{\beta \colon \lVert \beta  \rVert _1 \leq L} M_n(\beta ).
	\]

	\begin{intuition}
		We want a \emph{sparse} \(\beta ^{\ast} \), which is achieved by controlling \(L\).
	\end{intuition}

	\begin{note}
		We're not assuming the underlying \(\mathbb{P} \) to be linear.
	\end{note}
\end{eg}

The main question of interest for the \hyperref[eg:high-dim-LR]{high dimensional linear regression} is the following.

\begin{problem*}[Persistency]
	How large can \(L = L(n, p)\) be so \(M(\hat{\beta} ) - M(\beta ^{\ast} ) \to 0\) as \(n, p \to \infty \)?
\end{problem*}
\begin{answer}
	With some assumptions, \autoref{thm:lec22} shows that \(L \lesssim \sqrt[4]{\log p / n} \).
\end{answer}

\begin{theorem}\label{thm:lec22}
	Under the setup of \hyperref[eg:high-dim-LR]{high-dimensional linear regression}, let \(Y = X_0\) and define \(F(Z) = \max _{0 \leq j, k \leq p} \vert X_j X_k - \mathbb{E}_{}\left[X_j X_k \right] \vert \). Assume further that \(\mathbb{E}_{}\left[F^2(Z) \right] < \infty \), then
	\[
		M(\hat{\beta} ) - M(\beta ^{\ast} ) = O_p \left( L , \sqrt[4]{\frac{\log p}{n}} \right).
	\]
\end{theorem}
\begin{proof}
	From the basic inequality, \(M(\hat{\beta} ) - M(\beta ^{\ast} ) \leq 2 \sup _{\beta \colon \lVert \beta  \rVert _1 \leq L} \vert M_n(\beta ) - M(\beta ) \vert \). By letting
	\[
		\gamma = (-1, \beta )^{\top},\quad
		\Sigma = \left( \mathbb{E}_{}\left[X_j^1 X_k^1 \right] \right) _{j, k = 0, \dots , p},\quad
		\Sigma _n = \left( \frac{1}{n} \sum_{i=1}^{n} X_j^i X_n^i \right)_{j, k = 0, \dots , p} ,
	\]
	we may then write \(M(\beta ) = \gamma ^{\top} \Sigma \gamma\) and \(M_n(\beta ) = \gamma ^{\top} \Sigma _n \gamma\). Hence,
	\[
		\sup _{\beta \colon \lVert \beta  \rVert _1 \leq L} \vert M_n(\beta ) - M(\beta ) \vert
		= \vert \gamma ^{\top} \Sigma _n \gamma - \gamma ^{\top} \Sigma \gamma  \vert
		\leq \lVert \gamma  \rVert _1^2 \cdot \lVert \Sigma _n - \Sigma  \rVert _\infty
		\leq (1 + L)^2 \lVert \Sigma _n - \Sigma  \rVert _\infty ,
	\]
	which implies
	\[
		\sup _{\beta \colon \lVert \beta  \rVert _1 \leq L} \mathbb{P} (M(\hat{\beta} ) - M(\beta ^{\ast} ) > \epsilon )
		\leq P((1 + L)^2 \lVert \Sigma _n - \Sigma  \rVert _\infty > \epsilon )
		\leq \frac{(1 + L)^2 \mathbb{E}_{}\left[\lVert \Sigma _n - \Sigma  \rVert _\infty  \right] }{\epsilon }
	\]
	by \hyperref[lma:Markov-inequality]{Markov's inequality}. Finally, we observe that
	\[
		\mathbb{E}_{}\left[\lVert \Sigma _n - \Sigma  \rVert _\infty  \right]
		= \mathbb{E}_{}\left[\sup _{f\in \mathscr{F} } \vert \mathbb{P} _n f - \mathbb{P} f \vert  \right]
	\]
	where \(\mathscr{F} = \{ f_{jk} \colon 0 \leq j, k \leq p\} \) with \(f_{jk} = X_j X_k - \mathbb{E}_{}\left[X_j X_k \right] \). Now, \(F\) is clearly an \hyperref[def:envelope]{envelope} with \(\lVert F \rVert _{L_2(\mathbb{P} )} < \infty \), and by defining \hyperref[def:eps-bracket]{\(\epsilon\)-brackets} to be \([f_{j,k} \pm \epsilon / 2 ]\) for all \(j, k = 0, \dots , p\), we have
	\[
		N_{[\ ]}(\mathscr{F} , L_2(\mathbb{P} ), \epsilon ) \leq (p + 1) ^2.
	\]
	By the \hyperref[thm:bracketing-bound]{bracketing bound}, \(\mathbb{E}_{}\left[\lVert \Sigma _n - \Sigma  \rVert _\infty  \right] \leq \frac{1}{\sqrt{n} } \lVert F \rVert _{L_2(\mathbb{P} )} \sqrt{2 \log (p+1)} \), i.e.,
	\[
		M(\hat{\beta} ) - M(\beta ^{\ast} )
		\leq \frac{(1 + L)^2 }{\sqrt{n} } \lVert F \rVert _{L_2(\mathbb{P} )} \sqrt{2 \log (p + 1)} .
	\]
	In order to let this goes to \(0\), we require \(L \lesssim \sqrt[4]{n / \log p}\), which finally implies
	\[
		M(\hat{\beta} ) - M(\beta ^{\ast} ) = O_p \left( L, \sqrt[4]{\frac{\log p}{n}} \right).
	\]
\end{proof}

\begin{remark}
	Observe that in the above proof, we do not need to \hyperref[def:localized-EP]{localize the empirical process}, i.e., the \hyperref[thm:bracketing-bound]{bracketing bound} can be used for any \hyperref[def:EP]{empirical process} induced by finite class.
\end{remark}