\lecture{29}{10 Nov.\ 9:00}{Convex Penalized Estimators}
\section{Penalized Least Square}
Now, instead of constraining \(\mathscr{F} \) in the \hyperref[prb:constrained-LS]{constrained least square}, we may start by considering \(\mathscr{F} = \mathbb{R} ^n\) (i.e., no constraint) but use a different objective to ``penalize'' some estimators, or equivalently, to favor a particular class of estimators. Specifically, consider the following problem.

\begin{problem}[Penalized least square]\label{prb:penalized-LS}
Let \(\theta ^{\ast} \in \mathbb{R} ^n\) and \(y = \theta ^{\ast} + \epsilon \) where \(\epsilon _1, \dots , \epsilon _n \overset{\text{i.i.d.} }{\sim } \mathcal{N} (0, \sigma ^2)\). Then, given a penalty function \(f\colon \mathbb{R} ^n \to \mathbb{R} \) and a tuning parameter \(\lambda \in \mathbb{R} \), the \emph{penalized least square} aims to estimate \(\theta ^{\ast} \) via
\[
	\hat{\theta}_{\lambda , f} = \argmin_{\theta \in \mathbb{R} ^n} \frac{1}{2} \lVert y - \theta \rVert ^2 + \lambda f(\theta ).
\]
\end{problem}

Here, we're using \(\theta \) instead of \(f\) since we're always considering fixed-design, i.e., the data is just an \(n\)-dimensional vector. With the fact that we don't have any constraint on \(\mathscr{F} \), we might just forget about functions at all. Let's see some examples of penalty function.

\begin{eg}[\(\ell _1\)-norm]
	If  \(\theta^{\ast} \) is sparse, then we can use the \(\ell _1\)-norm penalty \(f(\cdot) = \lVert \cdot \rVert _1\).
\end{eg}

\begin{eg}[Nuclear-norm]
	If \(\theta ^{\ast} \) (a matrix) is low-rank, we can use the nuclear-norm as \(f\).
\end{eg}

\begin{eg}[Fused Lasso]
	If \(f(\theta ) = \sum_{i} \vert \theta _{i+1} - \theta _i \vert = \TV(\theta )\), then \(\theta ^{\ast} \) is piecewise constant.
\end{eg}

\begin{eg}[Trend Filtering]
	If \(f(\theta ) = \sum_{i} \vert \theta _{i+1} - 2 \theta _i + \theta _i\vert \), then \(\theta ^{\ast} \) is piecewise linear.
\end{eg}
The above are all \(\ell _1\)-type penalty. One can also consider \(\ell _2\)-type penalty.

\begin{eg}[Ridge regression]
	\(f(\theta ) = \lVert \theta \rVert _2^2 \).
\end{eg}

There are some non-trivial differences between the \hyperref[prb:penalized-LS]{penalized least square} and \hyperref[prb:constrained-LS]{constrained least square}. Consider the following example.

\begin{eg}[Constrained fused Lasso v.s.\ penalized fused Lasso]
	Let \(f(\theta ) = \TV(\theta ) \), then the \hyperref[prb:constrained-LS]{constrained least square} estimator for \(\mathscr{F}_v = \{ \theta \colon \TV(\theta ) \leq v\} \) is
	\[
		\hat{\theta} _v = \argmin_{\theta \colon \TV(\theta ) \leq v} \lVert y - \theta \rVert ^2,
	\]
	while the \hyperref[prb:penalized-LS]{penalized least square} estimator is
	\[
		\hat{\theta} _{\lambda , \TV} = \argmin_{\theta \in \mathbb{R} ^n} \lVert y - \theta  \rVert ^2 + \lambda \TV(\theta ) .
	\]
	Observe that for any fixed \(y\), \(\hat{\theta} _{\lambda , \TV } = \hat{\theta} _v\) for some \(v\) depends on \(y\). That is to say, in general,
	\[
		\lVert \hat{\theta} _v - \theta ^{\ast}  \rVert ^2 \neq \lVert \hat{\theta} _{\lambda , \TV } - \theta ^{\ast} \rVert ^2 .
	\]
\end{eg}

\begin{remark}
	Despite the fact that we have some well-established theory around \hyperref[prb:constrained-LS]{constrained least square}, the \hyperref[prb:penalized-LS]{penalized} version is more popular because of computational concerns.
\end{remark}

Clearly, \(\hat{\theta} _{\lambda , f}\) is an \hyperref[prb:M-estimation]{\(M\)-estimator}, so we're going to use our established theory, i.e., the \hyperref[thm:non-asymptotic-rate-of-convergence]{non-asymptotic rate of convergence}. First, fix some \(\lambda \) and \(f\), we may write \(\hat{\theta} _{\lambda , f} = \hat{\theta} \). Then, by plugging in \(y = \theta ^{\ast} + \epsilon \),
\[
	\hat{\theta} = \argmax_{\theta \in \mathbb{R} ^n} \left( \langle \epsilon , \theta - \theta ^{\ast}  \rangle - \frac{1}{2} \lVert \theta - \theta ^{\ast}  \rVert ^2 - \lambda f(\theta )  \right)
\]
where we omit \(\lVert \epsilon \rVert / 2\) since it's independent of \(\theta \). Hence, we may define\footnote{Note that previously we're dealing with minimum, so here the signs are flipped.}
\[
	M_n(\theta ) \coloneqq \langle \epsilon , \theta - \theta ^{\ast} \rangle - \frac{1}{2} \lVert \theta - \theta ^{\ast} \rVert ^2 - \lambda f(\theta ).
\]
Correspondingly, let
\[
	M(\theta ) \coloneqq -\frac{1}{2} \lVert \theta - \theta ^{\ast}  \rVert ^2
\]
with
\[
	\theta ^{\ast} = \argmax_{\theta \in \mathbb{R} ^n} M(\theta ).
\]

\begin{intuition}
	Here, \(M_n (\theta ) \neq \mathbb{E}_{}\left[M(\theta ) \right] \), where we have additional terms. However, if the penalty is ``appropriate'', the actual penalization would be small. In other case, even if the penalty is not small, one may want to use it to control the ``complexity'' of the estimator.\footnote{This is the same thing as regularization in, e.g., machine learning.}
\end{intuition}

We first check the following.

\begin{claim}
	\(d(\theta , \theta ^{\prime} ) = \lVert \theta - \theta ^{\prime} \rVert _2 \) satisfies the \hyperref[def:growth-condition*]{growth condition}.
\end{claim}
\begin{explanation}
	Since \(d^2(\theta , \theta ^{\prime} ) = \lVert \theta - \theta ^{\prime} \rVert ^2 \geq -\frac{1}{2} \left( \lVert \theta - \theta ^{\ast} \rVert ^2 - \lVert \theta ^{\prime} - \theta ^{\ast} \rVert ^2 \right) = M(\theta ) - M(\theta ^{\prime} )\).
\end{explanation}

Then, our next goal is to bound the \hyperref[def:localized-EP]{localized empirical process}
\[
	\sup _{\substack{\theta \in \mathbb{R} ^n \\ \lVert \theta - \theta ^{\ast}  \rVert \leq t }} (M_n - M)(\theta ) - (M_n - M)(\theta ^{\ast})
	= \langle \epsilon , \theta - \theta ^{\ast} \rangle - \lambda f(\theta ) + \lambda f(\theta ^{\ast} ).
\]

\subsection{Convex Penalty}
To control the term \(\lambda (f(\theta ^{\ast} ) - f(\theta ))\), we assume that \(f\) is convex. In this case, we can linearize \(f\) by
\[
	f(\theta ) \geq f(\theta ^{\ast} ) + \langle s, \theta - \theta ^{\ast}  \rangle,
\]
i.e., \(s\) is a \hyperref[def:sub-gradient]{sub-gradient}.

\begin{definition}[Sub-gradient]\label{def:sub-gradient}
	The set of \emph{sub-gradients} \(\partial f(x^{\ast} )\) of \(f \colon \mathbb{R} ^n \to \mathbb{R} \) at \(x^{\ast} \) contains \(s \in \mathbb{R} ^n\) such that for every \(x \in \mathbb{R} ^n\),
	\[
		f(x) \geq f(x^{\ast} ) + \langle s, x - x^{\ast}  \rangle.
	\]
\end{definition}

Hence, for convex \(f\), we may take any \(s\in \partial f(\theta ^{\ast} )\) such that
\[
	\sup _{\substack{\theta \in \mathbb{R} ^n                                            \\ \lVert \theta - \theta ^{\ast}  \rVert \leq t }} \langle \epsilon , \theta - \theta ^{\ast} \rangle - \lambda f(\theta ) + \lambda f(\theta ^{\ast} )
	\leq \sup _{\substack{\theta \in \mathbb{R} ^n                                    \\ \lVert \theta - \theta ^{\ast}  \rVert \leq t }} \langle \epsilon , \theta - \theta ^{\ast} \rangle - \lambda \langle s, \theta - \theta ^{\ast}  \rangle
	= \sup _{\substack{\theta \in \mathbb{R} ^n                                       \\ \lVert \theta - \theta ^{\ast}  \rVert \leq t }} \langle \epsilon - \lambda s, \theta - \theta ^{\ast}  \rangle .
\]
By Cauchy-Schwarz, this is further upper-bounded by \(t \lVert \epsilon - \lambda s \rVert\). Since this is true for any \(s\in \partial f(\theta ^{\ast} )\),
\[
	\sup _{\substack{\theta \in \mathbb{R} ^n \\ \lVert \theta - \theta ^{\ast}  \rVert \leq t }} (M_n - M)(\theta ) - (M_n - M)(\theta ^{\ast} )
	\leq t \cdot \inf _{s\in \partial f(\theta ^{\ast} )} \lVert \epsilon - \lambda s \rVert
	= t \cdot \dist(\epsilon , \lambda \cdot \partial f(\theta ^{\ast} )) .
\]
By taking the expectation, we successfully bound the \hyperref[def:localized-EP]{localized empirical process} by \(\phi _n (t)\) where
\[
	\phi _n(t) \coloneqq t \cdot \mathbb{E}_{}\left[\dist(\epsilon , \lambda \cdot \partial f(\theta ^{\ast} ))  \right] .
\]
It's clear that \(\phi _n(t)\) satisfies the \hyperref[def:sub-quadratic-assumption]{sub-quadratic assumption} with \(\alpha = 1\) since \(\phi _n(ct) = c^\alpha \phi _n(t)\) for \(\alpha = 1\). As the last step, we consider the \hyperref[def:rate-determining-equation]{rate-determining equation} \(\phi _n(\delta _n) \approx \delta _n^2\), which gives
\[
	\delta _n \approx \mathbb{E}_{}\left[\dist(\epsilon , \lambda \partial f(\theta ^{\ast} ))  \right] ,
\]
which implies \(\lVert \hat{\theta} - \theta ^{\ast}  \rVert = O_p(\delta _n)\).

\subsection{A Deterministic Approach}
Without going through these machinery, actually one can directly show the following deterministic bound.

\begin{lemma}[Oymak-Hassibi]\label{lma:Oymak-Hassibi}
	For a \hyperref[prb:penalized-LS]{penalized least square} problem, \(\lVert \hat{\theta} - \theta ^{\ast} \rVert \leq \dist(\epsilon , \lambda \partial f(\theta ^{\ast} )) \).
\end{lemma}
\begin{proof}
	First, \(\hat{\theta} \) minimizes \(g\) if and only if \(0\in \partial g(\hat{\theta} )\), where \(g\) is defined as
	\[
		g(\theta ) = \frac{1}{2} \lVert y - \theta  \rVert ^2 + \lambda f(\theta ).
	\]
	Equivalently, \(0 \in \hat{\theta} - y + \lambda \cdot \partial f(\hat{\theta} )\), which implies \(y - \hat{\theta} \in \lambda \cdot \partial f(\hat{\theta} )\).

	\begin{claim}
		If \(f\colon \mathbb{R} ^n \to \mathbb{R} \) is convex, then for any \(\theta _1, \theta _2 \in \mathbb{R} ^n\), \(s_1 \in \partial f(\theta _1)\), \(s_2 \in \partial f(\theta _2)\),
		\[
			\langle \theta _1 - \theta _2, s_1 - s_2 \rangle \geq 0.
		\]
	\end{claim}
	\begin{explanation}
		Since \(f(\theta _1) \geq f(\theta _2) + \langle s_2, \theta _1 - \theta _2 \rangle\) and \(f(\theta _2) \geq f(\theta _1) + \langle s_1, \theta _2 - \theta _1 \rangle\), adding them together gives the result.
	\end{explanation}

	Hence, for any \(s\in \partial f(\theta ^{\ast} )\), \(\langle y - \hat{\theta} - \lambda s , \hat{\theta} - \theta ^{\ast} \rangle \geq 0\). By writing \(y = \theta ^{\ast} + \epsilon \), we finally get
	\[
		\lVert \hat{\theta} - \theta ^{\ast} \rVert ^2
		\leq \langle \epsilon - \lambda s, \hat{\theta} - \theta ^{\ast}  \rangle
		\leq \lVert \epsilon - \lambda s \rVert \lVert \hat{\theta} - \theta ^{\ast} \rVert.
	\]
	Since this holds for any \(s\in \partial f(\theta ^{\ast} )\), taking the infimum over \(s\in \partial f(\theta ^{\ast} )\) gives the result.
\end{proof}

\begin{remark}
	This bound is good if \(\partial f(\theta ^{\ast} )\) is large, i.e., this is useful when \(f\) is not differentiable.
\end{remark}

\subsection{Sparse Means}
Let's see an application. Let \(y = \theta ^{\ast} + \epsilon \) where \(\theta ^{\ast} \) is \(k\)-sparse, so it's natural to consider \(f(\theta ) = \lVert \theta \rVert _1\), i.e.,
\[
	\hat{\theta} = \argmin_{\theta \in \mathbb{R} ^n} \frac{1}{2} \lVert y - \theta \rVert ^2 + \lambda \lVert \theta  \rVert _1.
\]
This problem has a closed form solution, but let's see how can our technique helps us. First, we see that
\[
	\partial f(\theta ^{\ast} ) = \left\{ v \in \mathbb{R} ^n \colon v_i = \begin{dcases}
		1,       & \text{ if } \theta ^{\ast} _i > 0 ; \\
		-1,      & \text{ if } \theta ^{\ast} _i < 0 ; \\
		[-1, 1], & \text{ if } \theta ^{\ast} _i = 0.
	\end{dcases} \right\}.
\]

\begin{intuition}
	We see that \(\partial f(\theta ^{\ast} )\) can be potentially large.
\end{intuition}

Then, from \autoref{lma:Oymak-Hassibi} (actually, the expectation version),
\[
	\lVert \hat{\theta} - \theta ^{\ast}  \rVert ^2
	\leq \mathbb{E}_{}\left[ \dist^2(\epsilon , \lambda \partial f(\theta ^{\ast} )) \right]
	= \mathbb{E}_{}\left[ \sum_{i \colon \theta ^{\ast} _i \neq 0} \left( \epsilon _i - \lambda \sgn (\theta ^{\ast} _i) \right) ^2 \right] + \mathbb{E}_{}\left[\sum_{i \colon \theta ^{\ast} _i = 0} \left( \epsilon _i - \Proj_{[-\lambda, \lambda ]} (\epsilon _i) \right) ^2 \right],
\]
where \(\Proj_{[-\lambda, \lambda ]} (\epsilon _i)\) is the projection of \(\epsilon _i\) to the interval \([-\lambda , \lambda ]\). Thus, for all \(i\), let
\[
	\operatorname{Soft}_\lambda (\epsilon _i)
	\coloneqq  \epsilon _i - \Proj_{[-\lambda, \lambda ]} (\epsilon _i)
	= \begin{dcases}
		\epsilon _i - \lambda , & \text{ if } \epsilon _i > \lambda  ;                \\
		0 ,                     & \text{ if } \epsilon _i \in [-\lambda , \lambda ] ; \\
		\epsilon _i + \lambda , & \text{ if } \epsilon _i < \lambda .
	\end{dcases}.
\]

\begin{note}
	One can show that \(\hat{\theta} _i = \operatorname{Soft}_\lambda (y_i)\).
\end{note}

With \(k = \lVert \theta ^{\ast} \rVert _0\), we therefore have
\[
	\lVert \hat{\theta} - \theta ^{\ast}  \rVert ^2
	\leq \mathbb{E}_{}\left[\dist^2(\epsilon , \partial f(\theta ^{\ast} )) \right]
	\leq k (1 + \lambda ^2) + (n - k) \cdot \mathbb{E}_{}\left[\left( \operatorname{Soft}_\lambda (Z)  \right) ^2 \right]
\]
for \(Z \sim \mathcal{N} (0, 1)\).

\begin{claim}
	For \(Z \sim \mathcal{N} (0, 1)\) and \(\operatorname{Soft}_\lambda \) defined above,
	\[
		\mathbb{E}_{}\left[ \left( \operatorname{Soft}_\lambda (Z) \right) ^2 \right]
		\leq \frac{2 e^{-\lambda ^2 / 2}}{ \lambda \sqrt{2\pi } }.
	\]
\end{claim}
\begin{explanation}
	By a direct calculation, we have
	\[
		\begin{split}
			\mathbb{E}_{}\left[ (\operatorname{Soft}_\lambda (Z) ) ^2 \right]
			 & = \int_{-\infty}^{\infty} ( \operatorname{Soft}_\lambda (z) ) ^2 \phi (z) \,\mathrm{d}z                                                                                                          \\
			 & = 2 \int_{0}^{\infty} ( \operatorname{Soft}_\lambda (z) ) ^2 \phi (z) \,\mathrm{d}z                                                                                                              \\
			 & = 2 \int_{\lambda}^{\infty} (z - \lambda )^2 \phi (z) \,\mathrm{d}z                                                                                                                              \\
			 & = 2 \left[ \int_{\lambda }^{\infty} z^2 \phi (z) \,\mathrm{d}z - 2\lambda \int_{\lambda }^{\infty} z \phi (z) \,\mathrm{d}z + \lambda ^2 \int_{\lambda }^{\infty} \phi (z) \,\mathrm{d}z \right] \\
			 & = 2 (1 + \lambda ^2) (1 - \Phi (\lambda )) - 2 \lambda \phi (\lambda ).
		\end{split}
	\]
	From the \hyperref[lma:Gaussian-tail-bound]{Gaussian tail bound}, \(1 - \Phi (\lambda ) \leq \phi (\lambda ) / \lambda \) for any \(\lambda > 0\), hence
	\[
		\mathbb{E}_{}\left[(\operatorname{Soft}_\lambda (Z) ) ^2 \right]
		\leq 2 (1 + \lambda ^2) \frac{\phi (\lambda )}{\lambda } - 2 \lambda \phi (\lambda )
		= 2 \frac{\phi (\lambda )}{\lambda }
		= \frac{2 \exp (-\lambda ^2 / 2)}{\lambda \sqrt{2\pi } }.
	\]
\end{explanation}

Hence, our final bound becomes
\begin{align*}
	\lVert \hat{\theta} - \theta ^{\ast} \rVert ^2
	 & \leq k(1 + \lambda ^2) + (n-k) \sqrt{\frac{2}{\pi }} \frac{e^{-\lambda ^2 / 2}}{\lambda }                         \\
	\shortintertext{with \(\lambda = \sqrt{2 \log n / k} \),}
	 & = k \left( 1 + 2 \log \frac{n}{k} \right) + (n-k) \sqrt{\frac{2}{\pi }} \frac{k}{n} \sqrt{\frac{1}{2 \log n / k}}
	= 2 k \log \frac{n}{k} (1 + o(1))
\end{align*}
w.r.t.\ \(k / n \to 0\). Finally, we note that
\[
	\mathbb{E}_{}\left[\frac{1}{n} \lVert \hat{\theta} - \theta ^{\ast} \rVert ^2 \right]
	\leq 2 \frac{k}{n} \log \frac{n}{k} (1 + o(1)).
\]

\begin{note}
	In general, if \(\theta ^{\ast} \) is \(k\)-sparse, \(\ell _1\) penalization with the right penalty can obtain a \(\frac{k}{n} \log \frac{n}{k}\) \hyperref[def:rate-of-convergence]{rate}.
\end{note}

\begin{remark}
	\(\lambda = \sqrt{2 \log n / k} \) is optimal, even regarding the \(2\) factor in the final bound \(2k \log n / k\).
\end{remark}

But setting \(\lambda \) requires some knowledge of \(k\), hence the structure of \(\theta ^{\ast} \). In this is not the case, we just set \(\lambda = \sqrt{2 \log n} \), yielding a bound of \(2 k \log n (1 + o(1))\). If \(k\) is large, then this bound is not tight.

\begin{note}
	This technique of bounding distant to the \hyperref[def:sub-gradient]{sub-gradient set} has been applied to show fast \hyperref[def:rate-of-convergence]{rates} in other settings, such as nuclear-norm penalization (Oymak-Hassibi), or TV penalty (\(\theta ^{\ast} \) is \(k\)-sparse), and filtering.
\end{note}