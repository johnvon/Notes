\lecture{27}{1 Nov.\ 9:00}{Another Approach of Localization}
\subsection{Offset Gaussian Rademacher Complexity}
We now see another approach of bounding the \hyperref[eq:basic-inequality]{basic inequality} \(\lVert \hat{f} - f^{\ast}  \rVert ^2 \leq 2 \langle \epsilon , \hat{f} - f^{\ast} \rangle \) given a \hyperref[prb:constrained-LS]{constrained least square} problem over \(\mathscr{F} \). First, instead of dividing both sides by \(\lVert \hat{f} - f^{\ast} \rVert \), we first double both sides and rearrange, we have
\[
	\lVert \hat{f} - f^{\ast}  \rVert ^2
	\leq 4 \langle \epsilon , \hat{f} - f^{\ast}  \rangle - \lVert \hat{f} - f^{\ast} \rVert ^2
	\leq \sup _{f\in \mathscr{F} } 4 \langle \epsilon , f - f^{\ast}  \rangle - \lVert f - f^{\ast}  \rVert ^2.
\]
Hence, our goal now is to bound this so-called \hyperref[def:offset-Gaussian-Rademacher-complexity]{offset Gaussian Rademacher complexity} of \(\mathscr{F} \) around \(f^{\ast} \).

\begin{definition}[Offset Gaussian Rademacher complexity]\label{def:offset-Gaussian-Rademacher-complexity}
	Consider the \hyperref[prb:constrained-LS]{constrained least square} problem over \(\mathscr{F} \) with \(f^{\ast} \in \mathscr{F} \). Then the \emph{offset Gaussian Rademacher complexity} of \(\mathscr{F} \) around \(f^{\ast} \) is defined as
	\[
		\sup _{f\in \mathscr{F} } 4 \langle \epsilon , f - f^{\ast}  \rangle - \lVert f - f^{\ast}  \rVert ^2 .
	\]
\end{definition}

\begin{eg}
	Consider \(\mathscr{F} = \{ X \beta \colon \beta \in \mathbb{R} ^d \} \), where \(X\) is a fixed \(n \times d\) design matrix. Then one can show that the corresponding \hyperref[def:offset-Gaussian-Rademacher-complexity]{offset Gaussian Rademacher complexity} is
	\[
		\sup _{\beta \in \mathbb{R} ^d} \langle \epsilon , X \beta  \rangle - \lVert X \beta  \rVert ^2 \approx \rank (X ^{\top} X).
	\]
\end{eg}

Note that \(\epsilon \) is random, so a natural goal now is to consider the expected \hyperref[def:offset-Gaussian-Rademacher-complexity]{offset Gaussian Rademacher complexity} and show that it concentrates around its expectation. Indeed, we can show this as follows.

\begin{theorem}\label{thm:non-asymptotic-bound-on-constrained-LS-offset}
	Consider the \hyperref[prb:constrained-LS]{constrained least square} over \(\mathscr{F} \) such that \(\mathscr{F} ^{\ast} \) is \hyperref[def:star-shaped]{star-shaped}, and let \(t^{\ast} \) be the \hyperref[def:critical-radius]{critical radius}. Then for all \(s > 0\), the \hyperref[def:offset-Gaussian-Rademacher-complexity]{offset Gaussian Rademacher complexity} satisfies
	\[
		\mathbb{P} \left( \sup _{f\in \mathscr{F} ^{\ast} } 4 \langle \epsilon , f \rangle - \lVert f \rVert ^2 \leq 4 (t^{\ast} + s)^2 \right) \geq 1 - e^{-\frac{s^2}{2\sigma ^2}}.
	\]
	Consequently, \(\mathbb{P} ( \lVert \hat{f} - f^{\ast} \rVert ^2 \leq 4 (t^{\ast} + s)^2 ) \geq 1 - e^{-\frac{s^2}{2\sigma ^2}}\).
\end{theorem}
\begin{proof}
	From \autoref{col:lec26-Lipschitz-probability}, let \(u = s t^{\ast} \), we have
	\[
		\mathbb{P} \left( Z(t^{\ast} ) \leq G(t^{\ast} ) + s  t^{\ast}  \right) \geq 1 - e^{- \frac{s^2}{2\sigma ^2}}.
	\]
	Denote the event of \(Z(t^{\ast} ) \leq G(t^{\ast} ) + s t^{\ast} \) by \(E\), take any \(f \in \mathscr{F} ^{\ast} \).
	\begin{itemize}
		\item If \(\lVert f \rVert \leq t^{\ast} \): then \(4 \langle \epsilon , f \rangle - \lVert f \rVert ^2 \leq 4 \langle \epsilon , f \rangle \leq 4 Z(t^{\ast} )\). On the event of \(E\), we further have
		      \[
			      4 \langle \epsilon , f \rangle - \lVert f \rVert ^2
			      \leq 4 (G(t^{\ast} ) + s t^{\ast} )
			      \leq 4 \left( \frac{{t^{\ast} }^2}{2} + s  t^{\ast} \right)
			      \leq 4 \left( \frac{{t^{\ast} }^2}{2} + s  t^{\ast} + \frac{s^2}{2}\right)
			      = 2(t^{\ast} + s)^2,
		      \]
		      where the second inequality follows from \autoref{col:lec26-non-increasing}.
		\item If \(\lVert f \rVert > t^{\ast} \): let \(\gamma = t^{\ast} / \lVert f \rVert < 1\), then \(\lVert \gamma f \rVert = t^{\ast} \), hence we have
		      \[
			      4 \langle \epsilon , f \rangle - \lVert f \rVert ^2
			      = \frac{4}{\gamma } \langle \epsilon , \gamma f \rangle - \frac{{t^{\ast} }^2}{\gamma ^2}
			      \leq \frac{4}{\gamma } Z(t^{\ast} ) - \frac{{t^{\ast} }^2}{\gamma ^2}
			      = \frac{4 t^{\ast} }{\gamma }  \frac{Z(t^{\ast} )}{t^{\ast} } - \frac{{t^{\ast} }^2}{\gamma ^2}
			      \leq \left(2 \frac{Z(t^{\ast} )}{t^{\ast} } \right) ^2
		      \]
		      from \(2ab - b^2 \leq a^2\) with \(a = 2 Z(t^{\ast} ) / t^{\ast} \) and \(b = t^{\ast} / \gamma \). On the event \(E\), we further have
		      \[
			      4 \langle \epsilon , f \rangle - \lVert f \rVert ^2
			      \leq 4 \left( \frac{Z(t^{\ast} )}{t^{\ast} } \right) ^2
			      \leq 4 \frac{( G(t^{\ast} ) + s t^{\ast} )^2}{{t^{\ast} }^2}
			      \leq 4 \left( \frac{{t^{\ast} }^2 / 2 + s t^{\ast} }{t^{\ast} } \right) ^2
			      \leq 4 ( t^{\ast} + s)^2
		      \]
		      where the third inequality again follows from \autoref{col:lec26-non-increasing}.
	\end{itemize}
	Combining two bounds, we have the desired result.
\end{proof}

\begin{remark}
	Comparing \autoref{thm:non-asymptotic-bound-on-constrained-LS-offset} and \autoref{thm:non-asymptotic-bound-on-constrained-LS}, we see that
	\begin{itemize}
		\item \autoref{thm:non-asymptotic-bound-on-constrained-LS-offset}: \(\lVert \hat{f} - f^{\ast}  \rVert ^2\) after \({t^{\ast} }^2\) has a tail with exponentially decay.
		\item \autoref{thm:non-asymptotic-bound-on-constrained-LS}: \(\lVert \hat{f} - f^{\ast}  \rVert \) after \(t^{\ast} \) has a tail with Gaussian decay.
	\end{itemize}
\end{remark}

We note that in order to apply either \autoref{thm:non-asymptotic-bound-on-constrained-LS-offset} or \autoref{thm:non-asymptotic-bound-on-constrained-LS}, the critical step is to first find \(t^{\ast} \), which involves bounding \(G(t)\). First, recall the following.

\begin{prev}
	Bounding \(G(t)\) is equivalent to bounding the \hyperref[def:localized-Gaussian-width]{localized Gaussian width} since
	\[
		G(t)
		= \mathbb{E}_{}\left[\sup _{f\in \mathscr{F} ^{\ast} \colon \lVert f \rVert \leq t} \langle \epsilon , f \rangle \right]
		= n \cdot \GW(\mathscr{F} - f^{\ast} , t).
	\]
	Essentially, it is an expectation of the supremum of the \hyperref[def:localized-EP]{localized empirical process}, so to bound this, we can use either the \hyperref[thm:bracketing-bound]{bracketing bound} or the \hyperref[thm:Dudley-entropy-bound]{Dudley's entropy bound}.
\end{prev}

Let's now see some examples of \(\mathscr{F} \) of the \hyperref[prb:constrained-LS]{constrained least square} problem and their corresponding \hyperref[def:localized-Gaussian-width]{localized Gaussian width}. Consider the fixed grid setup as in the \hyperref[prb:smooth-LS]{smooth constrained least square}: given a function class \(\mathscr{F} \) from \(\mathbb{R} \) to \(\mathbb{R} \), define the set \(\mathscr{F} \) (abusing the notation) to be
\[
	\mathscr{F} = \left\{ \big( f(1 / n), f(2 / n), \dots , f(1) \big) \in \mathbb{R} ^n \colon f \in \mathscr{F} \right\} .
\]
We note the following.

\begin{remark}
	One can often replace \(\GW(\mathscr{F} - f^{\ast} , t) \) by \(\GW(\mathscr{F} - \mathscr{F} , t) \) without getting a worse \hyperref[def:rate-of-convergence]{rate}.
\end{remark}
\begin{explanation}
	We see that these two \hyperref[def:localized-Gaussian-width]{localized Gaussian widths} correspond to the following supremum
	\[
		\GW(\mathscr{F} - f^{\ast} , t)   \iff \sup _{f\in \mathscr{F} } f - f^{\ast}, \quad \text{ and } \quad
		\GW(\mathscr{F} - \mathscr{F} , t) \iff \sup _{f, g \in \mathscr{F} } f - g
	\]
	The latter gives the worst case rate since the supremum is taken ``uniformly'' over all \(g\in \mathscr{F} \). However, the local geometry of \(\mathscr{F} \) is usually quite ``uniform'', so the \hyperref[def:rate-of-convergence]{rate} doesn't blow up.
\end{explanation}

We can then look at some examples of \(\mathscr{F} \) with their corresponding \(\log \)-\hyperref[def:covering-number]{covering number}.

\begin{eg}[HÃ¶lder smooth functions]
	Consider \(\mathscr{F} = \mathcal{S} _\alpha \),\footnote{Note that actually \(\mathscr{F} \) is a subset in \(\mathbb{R} ^n\) evaluated on the grid.} with \(\alpha > 1 / 2\), for some \(c > 0\),
	\[
		\log N(\mathscr{F} - \mathscr{F} , \lVert \cdot \rVert _2 / \sqrt{n} , \epsilon ) \leq c \left( \frac{1}{\epsilon } \right) ^{1 / \alpha }.
	\]
\end{eg}
\begin{explanation}
	We note that if \(\mathscr{F} \) is \(L\)-Lipschitz, then \(\mathscr{F} - \mathscr{F} \) is \(2L\)-Lipschitz. Similar things hold for \(\mathcal{S} _\alpha \) for \(\alpha > 1 / 2\), hence \autoref{thm:metric-entropy} applies.
\end{explanation}

\begin{eg}[Bounded isotonic functions]
	Consider the class of \emph{bounded isotonic functions} defined as \(\mathscr{F} = \{(f_1, \dots , f_n) \in \mathbb{R} ^n \colon 0 \leq f_1 \leq f_2 \leq \dots \leq f_n \leq 1 \}\). Then for some \(c > 0\), we have
	\[
		\log N(\mathscr{F} , \lVert \cdot \rVert _2 / \sqrt{n} , \epsilon ) \leq \frac{c}{\epsilon }.
	\]
\end{eg}

\begin{eg}[Unbounded isotonic functions]
	If we instead consider the class of \emph{unbounded isotonic functions} defined as \(\mathscr{F} = \{(f_1, \dots , f_n) \colon f_1 \leq f_2 \leq \dots \leq f_n \}\), i.e., \(\hat{f} \) will have no tuning parameters. Then it's known that for \(v^{\ast} = (f^{\ast} _n - f^{\ast} _1)\),
	\[
		\GW(\mathscr{F} - f^{\ast} , t) \leq C n^{1 / 4} {v^{\ast} }^{1 / 2} t^{1 / 2}
	\]
	with \(\mathbb{E}_{}[\lVert \hat{f} - f^{\ast}  \rVert ^2 ] \leq (v^{\ast} / n)^{2 / 3}\).
\end{eg}
\begin{explanation}
	This can be bound since we're always looking at some bounded ball, which is compact. Moreover, if we accept the bound on \(\GW\), then equating this with \(t^2\) we have
	\[
		t^{\ast} \approx {v^{\ast} _n}^{1 / 3} n^{1 / 6}
		\implies \mathbb{E}_{}\left[\lVert \hat{f} - f^{\ast} \rVert ^2 \right] \lesssim {t^{\ast} }^2 / n \approx (v^{\ast} / n)^{2 / 3}
	\]
	where we need to divide by \(n\) due to \(G(t) = n \GW(\mathscr{F} ^{\ast} , t)\).
\end{explanation}

We now see another example where we consider \(\mathscr{F} - \mathscr{F} \) instead.

\begin{eg}[Unimodal functions]
	Consider the class of \emph{unimodal functions} defined as
	\[
		\mathscr{F} = \{ (f_1, \dots , f_n) \in \mathbb{R} ^n \colon \exists i \colon f_1 \leq f_2 \leq \dots \leq f_i \geq f_{i+1} \geq \dots \geq f_n \}.
	\]
	Then
	\[
		\log N(\mathscr{F} , \lVert \cdot \rVert _2 / \sqrt{n} , \epsilon ) \leq \frac{c}{\epsilon },
	\]
	and we get an \(n^{-2 / 3}\) \hyperref[def:rate-of-convergence]{rate} for mean square error.
\end{eg}
\begin{explanation}
	Observe that \(\mathscr{F} \) is non-convex, but it's \hyperref[def:star-shaped]{star-shaped}, hence \(\mathscr{F} - \mathscr{F} \) is also \hyperref[def:star-shaped]{star-shaped} because \(\mathscr{F} \) is \hyperref[def:star-shaped]{star-shaped} around \(0\).
\end{explanation}

\begin{remark}[Isotonic, unimodal, convex]
	We actually have \(\GW(\mathscr{F} - f^{\ast} , t) \approx f^{\ast} \). If \(f^{\ast} \) is linear, then the rate is \(n^{-2 / 3}\); if \(f^{\ast} \) is \(K\) pieces-wise constant, we get \(k \log n / n\). This means the geometry of \(\mathscr{F} \) is different around different \(f^{\ast} \).
\end{remark}

The geometric and local Gaussian complexity around \(f^{\ast} \) as a function of \(t\)