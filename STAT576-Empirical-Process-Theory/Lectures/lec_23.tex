\lecture{23}{23 Oct.\ 9:00}{Fixed Design Non-Parametric Regression}
\section{Non-Parametric Regression}
Now, we can see more advanced applications based on \hyperref[prb:M-estimation]{\(M\)-estimations}.

\subsection{Fixed Design}
First, consider the following problem.

\begin{problem}[Fixed design non-parametric least square]\label{prb:fixed-design-non-parametric-LS}
Let \(\mathscr{F} = \{ f\colon [0, 1] \to [-1, 1] \text{ \(1\)-Lipschitz} \}\), and consider \(\epsilon _1, \dots , \epsilon _n \overset{\text{i.i.d.} }{\sim } \mathcal{N} (0, \sigma ^2)\) such that for a fixed \(f^{\ast} \in \mathscr{F} \), for all \(i = 1, \dots , n\),
\[
	y_i = f^{\ast} (i / n) + \epsilon _i
\]
Then, the problem of \emph{fixed design non-parametric least square} is to find the least square estimate
\[
	\hat{f} _n = \argmin_{f\in \mathscr{F} } \frac{1}{n} \sum_{i=1}^{n} \left( y_i - f (i / n) \right) ^2.
\]
\end{problem}

\begin{notation}
	\hyperref[prb:fixed-design-non-parametric-LS]{Fixed design non-parametric least square} problem is fixed in the sense of the data \((x, y = f(x) + \epsilon )\) is generated at the fixed grid \(x\in \{ 1 / n, \dots , 1 \} \).
\end{notation}

The main idea to solve the \hyperref[prb:fixed-design-non-parametric-LS]{fixed design non-parametric least square} is because it appeals to solve an \hyperref[prb:M-estimation]{\(M\)-estimation} problem.

\begin{remark}
	\(\hat{f} _n \) is an \hyperref[prb:M-estimation]{\(M\)-estimator}.
\end{remark}
\begin{explanation}
	We first observe that from the definition of \(y_i\),\footnote{Note that the term \(-\frac{1}{n} \sum_{i=1}^{n} \epsilon _i ^2\) is omitted since it doesn't depend on \(f \in \mathscr{F} \).}
	\[
		\hat{f} _n = \argmax_{f\in \mathscr{F} } \frac{2}{n} \sum_{i=1}^{n} \epsilon _i \left( f (i / n) - f^{\ast} (i / n)  \right) - \frac{1}{n} \sum_{i=1}^{n} \left( f (i / n) - f^{\ast} (i / n) \right) ^2 ,
	\]
	which motivates us to define \(M_n(f) , M(f) \colon \mathscr{F} \to \mathbb{R} \) such that
	\[
		M_n(f) = \frac{2}{n} \sum_{i=1}^{n} \epsilon _i \left( f (i / n) - f^{\ast} (i / n)  \right) - \frac{1}{n} \sum_{i=1}^{n} \left( f (i / n) - f^{\ast} (i / n) \right) ^2
	\]
	and
	\[
		M(f)
		= \mathbb{E}_{}\left[M_n (f) \right]
		= - \frac{1}{n} \sum_{i=1}^{n} \left( f (i / n) - f^{\ast} (i / n) \right) ^2.
	\]
	We see that \(f^{\ast} = \argmax_{f\in \mathscr{F} } M(f) \).
\end{explanation}

Since \(\hat{f} _n \) is an \hyperref[prb:M-estimation]{\(M\)-estimator}, we want to get the \hyperref[def:rate-of-convergence]{rate}. We need to verify the following.

\begin{claim}
	The \hyperref[def:growth-condition*]{growth condition} is satisfied for some \(d\).
\end{claim}
\begin{explanation}
	By choosing the canonical \(d\), i.e.,
	\[
		d(f, f^{\ast} )
		\coloneqq \sqrt{M(f^{\ast} ) - M(f)}
		= \sqrt{\frac{1}{n} \sum_{i=1}^{n} \left( f (i / n) - f^{\ast} (i / n)  \right) ^2}
		= L_2(\mathbb{P} _n) (f, f^{\ast} ),
	\]
	where \(\mathbb{P} _n\) is the empirical measure on the grid \(\{ 1 / n, 2 / n, \dots , 1 \} \), we see that it satisfies the \hyperref[def:growth-condition*]{growth condition} automatically.
\end{explanation}

To bound the \hyperref[def:localized-EP]{localized empirical process} for the \hyperref[prb:fixed-design-non-parametric-LS]{fixed design non-parametric least square}
\[
	\mathbb{E}_{}\left[\sup _{f\in \mathscr{F} \colon d(f, f^{\ast} ) \leq \delta } (M_n - M)(f) - (M_n - M)(f^{\ast} ) \right].
\]

\begin{claim}
	This \hyperref[def:localized-EP]{localized empirical process} is bounded by \(\phi _n (\delta ) = c \sqrt{\delta / n} \) for some \(c >0\).
\end{claim}
\begin{explanation}
	We first observe that
	\[
		\mathbb{E}_{}\left[\sup _{\substack{f\in \mathscr{F} \colon \\ d(f, f^{\ast} ) \leq \delta } } (M_n - M)(f) - (M_n - M)(f^{\ast} ) \right]
		= 2 \cdot \mathbb{E}_{}\left[\sup _{\substack{f\in \mathscr{F} \colon \\ d(f, f^{\ast} ) \leq \delta } } \frac{1}{n} \sum_{i=1}^{n} \epsilon _i \left( f (i / n) - f^{\ast} (i / n) \right) \right] .
	\]
	For \(f\in \mathscr{F} \), define \(X_f = \frac{1}{\sqrt{n} } \sum_{i=1}^{n} \epsilon _i \left( f (i / n) - f^{\ast} (i / n)  \right) \), so for \(f, g\in \mathscr{F} \),
	\[
		X_f - X_g = \frac{1}{\sqrt{n} } \sum_{i=1}^{n} \epsilon _i \left( f (i / n) - g(i / n)  \right) \sim \mathcal{N} ( 0, \sigma ^2 d^2(f, g) ) ,
	\]
	which implies that \(X_f\) is a \hyperref[def:sub-Gaussian-process]{sub-Gaussian process} with
	\[
		\mathbb{P} ( \vert X_f - X_g \vert \geq u) \leq \exp \left( - \frac{u^2}{2 d^2(f, g)} \right)
	\]
	when \(\sigma ^2 = 1\).\footnote{Without this assumption, we just have \(\sigma ^2\) in the final bound, which is still a \hyperref[def:sub-Gaussian-process]{sub-Gaussian process}.} So, by \hyperref[col:Dudley-integral-entropy-bound]{Dudley's entropy integral bound}, for some \(c> 0\),
	\[
		\begin{split}
			\mathbb{E}_{}\left[\sup _{\substack{f\in \mathscr{F} \colon \\ d(f, f^{\ast} ) \leq \delta } } \frac{1}{n} \sum_{i=1}^{n} \epsilon _i \left( f (i / n) - f^{\ast} (i / n) \right) \right]
			&\leq \frac{12}{\sqrt{n} } \int_{0}^{\delta } \sqrt{\log N (\mathscr{F} , d, \epsilon )}  \,\mathrm{d}\epsilon \\
			&\leq \frac{1}{\sqrt{n} } \int_{0}^{\delta } \sqrt{\frac{c}{\epsilon } }  \,\mathrm{d}\epsilon = c \sqrt{\frac{\delta}{n}} \eqqcolon \phi _n (\delta )
		\end{split}
	\]
	since \(N(\mathscr{F} , d, \epsilon ) \leq N(\mathscr{F} , \lVert \cdot \rVert _\infty , \epsilon ) \leq \exp (c_1 / \epsilon )\) from \autoref{thm:metric-entropy}.
\end{explanation}

Then, we verify the \hyperref[def:sub-quadratic-assumption]{sub-quadratic assumption} for such \(\phi _n\)'s.

\begin{claim}
	\(\phi _n\)'s satisfy the \hyperref[def:sub-quadratic-assumption]{sub-quadratic assumption} for \(\alpha = 1 / 2\).
\end{claim}
\begin{explanation}
	Since \(\phi _n (c \delta ) = \sqrt{c} \cdot c \sqrt{\delta / n} = \sqrt{c} \cdot \phi _n(\delta )\).
\end{explanation}

With all these, we can finally solve the \hyperref[def:rate-determining-equation]{rate-determining equation}, which is
\[
	\phi _n (\delta ) \approx \delta ^2
	\implies \sqrt{\frac{\delta}{n}} \approx \delta ^2
	\implies \delta \approx n^{-1 / 3}.
\]
In all, we have the \hyperref[def:rate-of-convergence]{rate of convergence}
\[
	d(\hat{f} _n, f^{\ast} )
	= \sqrt{\frac{1}{n} \sum_{i=1}^{n} \left( \hat{f} _n (i / n) - f^{\ast} (i / n)   \right) ^2 }
	= O_p(n^{-1 / 3}) ,
\]
or more specifically, from \autoref{thm:non-asymptotic-rate-of-convergence-extend},
\[
	\mathbb{P} \left( \frac{1}{n} \sum_{i=1}^{n} \left( \hat{f} _n (i / n) - f^{\ast} (i / n) \right) ^2 > 2^{2M} n^{-2 / 3} \right) \leq c\cdot 2^{-M}.
\]

\begin{remark}
	The \hyperref[def:rate-of-convergence]{rate of convergence} in mean-square error (i.e., w.r.t.\ \(d^2 = L_2^2(\mathbb{P} )\)) for \(1\)-Lipschitz regression is \(n^{-2 / 3}\). Moreover, the ``min-max rate'' worst case over all \(f^{\ast} \in \mathscr{F} \) is \(n^{-2 / 3}\), hence the \hyperref[prb:fixed-design-non-parametric-LS]{fixed design non-parametric least square estimator} is ``min-max rate optimal''. These terms will make sense in the next \hyperref[rmk:min-max-rate-optimal]{remark}.
\end{remark}

More generally, if we assume that \(f^{\ast} \in \mathcal{S} _{\alpha }\), we can also solve the \hyperref[prb:fixed-design-non-parametric-LS]{fixed design non-parametric least square} over \(\mathcal{S} _\alpha \) as follows.

\begin{theorem}\label{thm:fixed-design-non-parametric-LS}
	Consider the \hyperref[prb:fixed-design-non-parametric-LS]{fixed design non-parametric least square} problem over \(\mathcal{S} _\alpha \), the \hyperref[def:rate-of-convergence]{rate of convergence} of \(d(\hat{f} _n, f^{\ast} )\) for the canonical \(d\) is \(O_p(n^{- \frac{\alpha }{2\alpha + 1}})\) for \(\alpha > 1 / 2\).
\end{theorem}
\begin{proof}
	From the same calculation, the corresponding \hyperref[def:localized-EP]{localized empirical process} is bounded by
	\[
		\begin{split}
			\mathbb{E}_{}\left[\sup _{\substack{f\in \mathcal{S} _\alpha \colon \\ d(f, f^{\ast} ) \leq \delta }} \frac{1}{n} \sum_{i=1}^{n} \epsilon _i \left( f (i / n) - f^{\ast} (i / n) \right) \right]
			&\leq \frac{c}{\sqrt{n} } \int_{0}^{\delta } \sqrt{\log N(\mathcal{S} _\alpha , d, \epsilon )}  \,\mathrm{d}\epsilon \\
			&\leq \frac{c}{\sqrt{n} } \int_{0}^{\delta } \sqrt{\log N(\mathcal{S} _\alpha , \lVert \cdot \rVert _\infty , \epsilon )}  \,\mathrm{d}\epsilon
			\leq \frac{c}{\sqrt{n} } \int_{0}^{\delta } \left( \frac{1}{\epsilon } \right) ^{\frac{1}{2\alpha } } \,\mathrm{d}\epsilon
		\end{split}
	\]
	from \autoref{thm:metric-entropy}. Since \(\alpha > 1 / 2\), this bound gives \(\lesssim \frac{1}{\sqrt{n} } \delta ^{1 - 1 / 2\alpha }\), so we can take
	\[
		\phi _n(\delta ) \coloneqq \frac{\delta ^{1 - \frac{1}{2\alpha } }}{\sqrt{n} }.
	\]
	We see that the \hyperref[def:sub-quadratic-assumption]{sub-quadratic assumption} is satisfied since \(\phi _n(c \delta) = c^{1 - \frac{1}{2\alpha }} \phi _n (\delta )\) with \(1 - 1 / 2\alpha < 2\). Solving the \hyperref[def:rate-determining-equation]{rate-determining equation}, we have
	\[
		\phi _n(\delta _n) \approx \delta _n^2
		\implies \frac{\delta _n^{-\frac{1}{2\alpha }}}{\sqrt{n} } \approx \delta_n
		\implies \delta _n\approx n^{- \frac{\alpha}{2\alpha +1}} .
	\]
\end{proof}

It's usually more natural to consider the rate for \(d^2(\hat{f} _n, f^{\ast} ) = L_2^2(\mathbb{P} _n) (\hat{f} _n, f^{\ast} ) = O_p(n^{-\frac{2\alpha }{2\alpha +1}})\).

\begin{note}
	When \(\alpha < 1 / 2\), we need to use the \hyperref[col:Dudley-integral-entropy-bound-finite-resolution]{modified version of Dudley's entropy integral bound}. In this case, we will get a slower rate than \(n^{-2\alpha / 2\alpha + 1}\) w.r.t.\ \(d^2\).
\end{note}

\begin{remark}[Min-max rate optimal]\label{rmk:min-max-rate-optimal}
	\(n^{-2\alpha / 2\alpha +1}\) is \emph{min-max rate optimal}, i.e., it's the ``right'' rate.
\end{remark}
\begin{explanation}
	We just showed that \(\mathbb{E}_{}[ L_2^2 (\mathbb{P} ) ( \hat{f} _n, f^{\ast} )] \leq c n^{-2\alpha / (2\alpha + 1)}\), and it's known that the \emph{min-max rate} is lower-bounded by
	\[
		\inf _{\hat{f} _n} \sup _{f \in \mathscr{F} } \mathbb{E}_{}\left[ L_2^2(\mathbb{P} ) (\hat{f} _n, f^{\ast} )\right] \geq c n^{-\frac{2\alpha}{2\alpha + 1}}.
	\]
	Hence, the rate for the \hyperref[prb:fixed-design-non-parametric-LS]{fixed design non-parametric least square estimator} is \emph{min-max rate optimal} over \(\mathcal{S} _\alpha \) for \(\alpha > 1 / 2\). However, it's not known whether the same conclusion holds for \(\alpha < 1 / 2\).
\end{explanation}

We do not know \(\alpha \). A natural question is the following.

\begin{problem*}
	Can we adaptively get \(O(n^{-2\alpha / 2\alpha + 1})\) rate without knowing \(\alpha \)?
\end{problem*}
\begin{answer}
	Yes. There are several ways to do this~\cite{tsybakov2008introduction}.
\end{answer}

\subsection{Contraction Principle}
Before we consider the more general setting, i.e., the random design case (data does not come from a fixed grid), we first recall the \hyperref[sec:statistical-learning]{statistical learning set-up} to compare Prediction and estimation.

\begin{prev}
	Given \((X_1, Y_1), \dots , (X_n, Y_n) \overset{\text{i.i.d.} }{\sim } \mathbb{P} \), with \(f^{\ast} (x) = \mathbb{E}_{}\left[Y \mid X = x \right] \), i.e.,
	\[
		f^{\ast} = \argmin_{f \text{ measurable} } \mathbb{E}_{(X, Y)}\left[ \left( Y - f(X)  \right) ^2 \right] .
	\]
\end{prev}

Given a class of function \(\mathscr{F} \), we can define
\[
	f_{\mathscr{F} } = \argmin_{f\in \mathscr{F} } \mathbb{E}_{}\left[ ( Y - f(X) ) ^2 \right] .
\]
For any prediction function \(f\colon \chi \to \mathbb{R} \), its \hyperref[not:excess-risk]{excess risk} is
\[
	\mathbb{E}_{}\left[ ( Y - f(X) ) ^2 \right] - \inf _{h \in \mathscr{F} } \mathbb{E}_{}\left[( Y - h(X) ) ^2 \right] .
\]
Observe that we can write
\begin{align*}
	 & \mathbb{E}_{}\left[ ( Y - f(X) ) ^2 \right] - \inf _{h \in \mathscr{F} } \mathbb{E}_{}\left[( Y - h(X) ) ^2 \right]                                                      \\
	 & = \mathbb{E}_{}\left[ ( Y - f(X) \pm f^{\ast} (X) ) ^2 \right]  - \inf _{h \in \mathscr{F} } \mathbb{E}_{}\left[( Y - h(X) \pm f^{\ast} (X)) ^2 \right]                  \\
	 & = \mathbb{E}_{}\left[ ( f(X) - f^{\ast} (X) ) ^2 \right] - \inf _{h \in \mathscr{F} } \mathbb{E}_{}\left[( h(X) - f^{\ast} (X) ) ^2 \right] \tag*{cross terms are \(0\)} \\
	 & = \lVert f - f^{\ast}  \rVert _{L_2(\mathbb{P} )}^2 - \inf _{h\in \mathscr{F} } \lVert h - f^{\ast}  \rVert _{L_2(\mathbb{P} )} ^2,
\end{align*}
where the last equation is the estimation error in the traditional statistics terminology.

\begin{remark}
	Prediction and estimation are the same in this content.
\end{remark}

Now, if we change \(f\) to a predictor \(\hat{f} _n \) (depending on the training data), we have
\[
	\mathbb{E}_{}\left[ ( Y - \hat{f} _n(X) ) ^2 \right] - \inf _{h \in \mathscr{F} } \mathbb{E}_{}\left[ ( Y - h(X) ) ^2 \right]
	= \lVert \hat{f} _n - f^{\ast}  \rVert _{L_2(\mathbb{P} )}^2 - \inf _{h\in \mathscr{F} } \lVert h - f^{\ast}  \rVert _{L_2(\mathbb{P} )} ^2
\]
There are two standard scenarios:
\begin{enumerate}[(a)]
	\item Well specified case: Given \(\mathscr{F} \), assume \(f^{\ast} \in \mathscr{F} \). In particular, \(\mathbb{P} \) is such that \(f(x) = \mathbb{E}_{}\left[Y \mid X = x \right] \in \mathscr{F} \). In this case, the \hyperref[not:excess-risk]{excess risk} of \(\hat{f} _n \) is just \(\lVert \hat{f} _n - f^{\ast} \rVert _{L_2(\mathbb{P} )}^2 \) since \(\inf _{h\in \mathscr{F} } \lVert h - f^{\ast} \rVert ^2 _{L_2(\mathbb{P} )} = 0\), i.e., the estimation error in \(L_2(\mathbb{P} )\). (this is just the traditional statistics).
	\item Mis-specified case: do not assume \(f^{\ast} \in \mathscr{F} \), we need to bound the \emph{oracle inequality} (which bounds both terms) entirely.
\end{enumerate}