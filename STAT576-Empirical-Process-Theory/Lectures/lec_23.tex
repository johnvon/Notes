
\chapter{Fixed Design Non-Parametric Regression}
\lecture{23}{23 Oct.\ 9:00}{Smooth Constrained Least Square}
In this chapter, we're going to study the \emph{fixed design non-parametric regression}, a more advanced application based on \hyperref[prb:M-estimation]{\(M\)-estimations}. Here, fixed design refers to the case that when an \hyperref[prb:ERM]{empirical risk minimization} problem has data \(X_i\)'s given are ``fixed'', i.e., the only randomness comes from \(Y_i\)'s.\footnote{In contrast, there is also \emph{random design}, where \(X_i\)'s are also sampled from a distribution.}

\section{Smooth Constrained Least Square}
In this section, we will see an example of \hyperref[prb:constrained-LS]{constrained least square} called \hyperref[prb:smooth-LS]{smooth constrained least square}. This helps us prepared for developing  a more unified theory towards the former problem.

\subsection{Prediction and Estimation: Statistical Learning v.s.\ \(M\)-Estimation}
Since we're going to consider regression problems, i.e., \hyperref[prb:ERM]{empirical risk minimizations}, with the tools developed for \hyperref[prb:M-estimation]{\(M\)-estimations}, it's natural to compare ``prediction'' and ``estimation''. Recall the following.

\begin{prev}[Prediction]
	Given \((X_1, Y_1), \dots , (X_n, Y_n) \overset{\text{i.i.d.} }{\sim } \mathbb{P} \), let \(f^{\ast} (x) = \mathbb{E}_{}\left[Y \mid X = x \right] \), i.e.,
	\[
		f^{\ast} = \argmin_{f \text{ measurable} } \mathbb{E}_{(X, Y)}\left[ ( Y - f(X) ) ^2 \right] .
	\]
	More generally, given a class of function \(\mathscr{F} \), we can define the predictor
	\[
		f_{\mathscr{F} } = \argmin_{f\in \mathscr{F} } \mathbb{E}_{}\left[ ( Y - f(X) ) ^2 \right] .
	\]
	For any prediction function \(f\colon \chi \to \mathbb{R} \), its \hyperref[def:excess-risk]{excess risk} is
	\[
		\mathbb{E}_{}\left[ ( Y - f(X) ) ^2 \right] - \inf _{h \in \mathscr{F} } \mathbb{E}_{}\left[( Y - h(X) ) ^2 \right] .
	\]
\end{prev}

We now ask, how does this ``prediction'' approach relates to the problem of ``estimation'', e.g., \hyperref[prb:M-estimation]{\(M\)-estimation}? Observe that we can write
\begin{align*}
	 & \mathbb{E}_{}\left[ ( Y - f(X) ) ^2 \right] - \inf _{h \in \mathscr{F} } \mathbb{E}_{}\left[( Y - h(X) ) ^2 \right]                                                      \\
	 & = \mathbb{E}_{}\left[ ( Y - f(X) \pm f^{\ast} (X) ) ^2 \right]  - \inf _{h \in \mathscr{F} } \mathbb{E}_{}\left[( Y - h(X) \pm f^{\ast} (X)) ^2 \right]                  \\
	 & = \mathbb{E}_{}\left[ ( f(X) - f^{\ast} (X) ) ^2 \right] - \inf _{h \in \mathscr{F} } \mathbb{E}_{}\left[( h(X) - f^{\ast} (X) ) ^2 \right] \tag*{cross terms are \(0\)} \\
	 & = \lVert f - f^{\ast}  \rVert _{L_2(\mathbb{P} )}^2 - \inf _{h\in \mathscr{F} } \lVert h - f^{\ast}  \rVert _{L_2(\mathbb{P} )} ^2,
\end{align*}
where the last equation is the estimation error in the traditional statistics terminology. Looking at the last line, we see that ``prediction'' and ``estimation'' are the same in this content.

\begin{intuition}
	Predicting \(\mathbb{E}_{}\left[Y \mid X = x \right] \) given \(x\) is equivalent to estimating \(f^{\ast} \) in the view of minimizing \hyperref[def:excess-risk]{excess risk} and \(L_2(\mathbb{P} )\) difference, respectively.
\end{intuition}

\subsection{Oracle Inequality}
If we change \(f\) in the above bound to a predictor \(\hat{f} _n \) (depending on the training data), we have the so-called \emph{oracle inequality}
\begin{equation}\label{eq:oracle-inequality}
	\mathbb{E}_{}\left[ ( Y - \hat{f} _n(X) ) ^2 \right] - \inf _{h \in \mathscr{F} } \mathbb{E}_{}\left[ ( Y - h(X) ) ^2 \right]
	= \lVert \hat{f} _n - f^{\ast}  \rVert _{L_2(\mathbb{P} )}^2 - \inf _{h\in \mathscr{F} } \lVert h - f^{\ast}  \rVert _{L_2(\mathbb{P} )} ^2
\end{equation}

There are two standard scenarios regarding bounding the \hyperref[eq:oracle-inequality]{oracle inequality}.

\begin{definition}[Well specified]\label{def:well-specified}
	Given a function class \(\mathscr{F} \), an \hyperref[prb:ERM]{empirical risk minimization} is \emph{well specified} if \(f^{\ast} \in \mathscr{F} \), i.e., \(\mathbb{P} \) is such that \(f(x) = \mathbb{E}_{}\left[Y \mid X = x \right] \in \mathscr{F} \).
\end{definition}

In the \hyperref[def:well-specified]{well specified} case, the \hyperref[def:excess-risk]{excess risk} of \(\hat{f} _n \) is just \(\lVert \hat{f} _n - f^{\ast} \rVert _{L_2(\mathbb{P} )}^2 \) since \(\inf _{h\in \mathscr{F} } \lVert h - f^{\ast} \rVert ^2 _{L_2(\mathbb{P} )} = 0\), i.e., the estimation error in \(L_2(\mathbb{P} )\). This is what we usually assume in the traditional statistics. On the other hand, we can also consider the contrary.

\begin{definition}[Misspecified]\label{def:misspecified}
	Given a function class \(\mathscr{F} \), an \hyperref[prb:ERM]{empirical risk minimization} is \emph{misspecified} if \(f^{\ast} \notin \mathscr{F} \), i.e., \(\mathbb{P} \) is such that \(f(x) = \mathbb{E}_{}\left[Y \mid X = x \right] \notin \mathscr{F} \).
\end{definition}

In the \hyperref[def:misspecified]{misspecified} case, we need to bound the \hyperref[eq:oracle-inequality]{oracle inequality} entirely.

\subsection{Smooth Constrained Least Square}
First, consider the following fixed-design problem with data generated at the fixed grid \(x\in \{ 1 / n, \dots , 1 \} \).

\begin{problem}[Smooth constrained least square]\label{prb:smooth-LS}
Let \(\mathscr{F} = \{ f\colon [0, 1] \to [-1, 1] \text{ \(1\)-Lipschitz} \}\), and consider \(\epsilon _1, \dots , \epsilon _n \overset{\text{i.i.d.} }{\sim } \mathcal{N} (0, \sigma ^2)\) such that for a fixed \(f^{\ast} \in \mathscr{F} \), for all \(i = 1, \dots , n\),
\[
	y_i = f^{\ast} (i / n) + \epsilon _i
\]
Then, the problem of \emph{smooth constrained least square} is to find the least square estimate
\[
	\hat{f} _n = \argmin_{f\in \mathscr{F} } \frac{1}{n} \sum_{i=1}^{n} \left( y_i - f (i / n) \right) ^2.
\]
\end{problem}

\begin{notation}[Sequence model]\label{not:sequence-model}
	The \emph{sequence model} refers to the noise \(\epsilon _i\) being \(\epsilon _i \overset{\text{i.i.d.} }{\sim } \mathcal{N} (0, \sigma ^2)\).
\end{notation}

We may use our techniques in \hyperref[prb:M-estimation]{\(M\)-estimations} due to the following.

\begin{remark}
	\(\hat{f} _n \) is an \hyperref[prb:M-estimation]{\(M\)-estimator}.
\end{remark}
\begin{explanation}
	We first observe that from the definition of \(y_i\),\footnote{Note that the term \(-\frac{1}{n} \sum_{i=1}^{n} \epsilon _i ^2\) is omitted since it doesn't depend on \(f \in \mathscr{F} \).}
	\[
		\hat{f} _n = \argmax_{f\in \mathscr{F} } \frac{2}{n} \sum_{i=1}^{n} \epsilon _i \left( f (i / n) - f^{\ast} (i / n)  \right) - \frac{1}{n} \sum_{i=1}^{n} \left( f (i / n) - f^{\ast} (i / n) \right) ^2 ,
	\]
	which motivates us to define \(M_n(f) , M(f) \colon \mathscr{F} \to \mathbb{R} \) such that
	\[
		M_n(f) = \frac{2}{n} \sum_{i=1}^{n} \epsilon _i \left( f (i / n) - f^{\ast} (i / n)  \right) - \frac{1}{n} \sum_{i=1}^{n} \left( f (i / n) - f^{\ast} (i / n) \right) ^2
	\]
	and
	\[
		M(f)
		= \mathbb{E}_{}\left[M_n (f) \right]
		= - \frac{1}{n} \sum_{i=1}^{n} \left( f (i / n) - f^{\ast} (i / n) \right) ^2.
	\]
	We see that \(f^{\ast} = \argmax_{f\in \mathscr{F} } M(f) \).
\end{explanation}

Since \(\hat{f} _n \) is an \hyperref[prb:M-estimation]{\(M\)-estimator}, we want to get the \hyperref[def:rate-of-convergence]{rate}. We need to verify the following.

\begin{claim}
	The \hyperref[def:growth-condition*]{growth condition} is satisfied for some \(d\).
\end{claim}
\begin{explanation}
	By choosing the canonical \(d\), i.e.,
	\[
		d(f, f^{\ast} )
		\coloneqq \sqrt{M(f^{\ast} ) - M(f)}
		= \sqrt{\frac{1}{n} \sum_{i=1}^{n} \left( f (i / n) - f^{\ast} (i / n)  \right) ^2}
		= L_2(\mathbb{P} _n) (f, f^{\ast} ),
	\]
	where \(\mathbb{P} _n\) is the empirical measure on the grid \(\{ 1 / n, 2 / n, \dots , 1 \} \), we see that it satisfies the \hyperref[def:growth-condition*]{growth condition} automatically.
\end{explanation}

Next, we bound the \hyperref[def:localized-EP]{localized empirical process} for the \hyperref[prb:smooth-LS]{smooth constrained least square}, i.e.,
\[
	\mathbb{E}_{}\left[\sup _{f\in \mathscr{F} \colon d(f, f^{\ast} ) \leq \delta } (M_n - M)(f) - (M_n - M)(f^{\ast} ) \right].
\]

\begin{claim}
	This \hyperref[def:localized-EP]{localized empirical process} is bounded by \(\phi _n (\delta ) = c \sqrt{\delta / n} \) for some \(c >0\).
\end{claim}
\begin{explanation}
	We first observe that
	\[
		\mathbb{E}_{}\left[\sup _{\substack{f\in \mathscr{F} \colon \\ d(f, f^{\ast} ) \leq \delta } } (M_n - M)(f) - (M_n - M)(f^{\ast} ) \right]
		= 2 \cdot \mathbb{E}_{}\left[\sup _{\substack{f\in \mathscr{F} \colon \\ d(f, f^{\ast} ) \leq \delta } } \frac{1}{n} \sum_{i=1}^{n} \epsilon _i \left( f (i / n) - f^{\ast} (i / n) \right) \right] .
	\]
	For \(f\in \mathscr{F} \), define \(X_f = \frac{1}{\sqrt{n} } \sum_{i=1}^{n} \epsilon _i \left( f (i / n) - f^{\ast} (i / n)  \right) \), so for \(f, g\in \mathscr{F} \),
	\[
		X_f - X_g = \frac{1}{\sqrt{n} } \sum_{i=1}^{n} \epsilon _i \left( f (i / n) - g(i / n)  \right) \sim \mathcal{N} ( 0, \sigma ^2 d^2(f, g) ) ,
	\]
	which implies that \(X_f\) is a \hyperref[def:sub-Gaussian-process]{sub-Gaussian process} with
	\[
		\mathbb{P} ( \vert X_f - X_g \vert \geq u) \leq \exp \left( - \frac{u^2}{2 d^2(f, g)} \right)
	\]
	when \(\sigma ^2 = 1\).\footnote{Without this assumption, we just have \(\sigma ^2\) in the final bound, which is still a \hyperref[def:sub-Gaussian-process]{sub-Gaussian process}.} So, by \hyperref[col:Dudley-integral-entropy-bound]{Dudley's entropy integral bound}, for some \(c> 0\), from \autoref{thm:metric-entropy},
	\[
		\mathbb{E}_{}\left[\sup _{\substack{f\in \mathscr{F} \colon                                                                                              \\ d(f, f^{\ast} ) \leq \delta } } \frac{1}{n} \sum_{i=1}^{n} \epsilon _i \left( f (i / n) - f^{\ast} (i / n) \right) \right]
		\leq \frac{12}{\sqrt{n} } \int_{0}^{\delta } \sqrt{\log N (\mathscr{F} , d, \epsilon )}  \,\mathrm{d}\epsilon
		\leq \frac{1}{\sqrt{n} } \int_{0}^{\delta } \sqrt{\frac{c}{\epsilon } }  \,\mathrm{d}\epsilon = c \sqrt{\frac{\delta}{n}}
	\]
	since \(N(\mathscr{F} , d, \epsilon ) \leq N(\mathscr{F} , \lVert \cdot \rVert _\infty , \epsilon ) \leq \exp (c_1 / \epsilon )\). Let \(\phi _n (\delta ) \coloneqq c \sqrt{\delta / n} \) concludes the proof.
\end{explanation}

Then, we verify the \hyperref[def:sub-quadratic-assumption]{sub-quadratic assumption} for such \(\phi _n\)'s.

\begin{claim}
	\(\phi _n\)'s satisfy the \hyperref[def:sub-quadratic-assumption]{sub-quadratic assumption} for \(\alpha = 1 / 2\).
\end{claim}
\begin{explanation}
	Since \(\phi _n (c \delta ) = \sqrt{c} \cdot c \sqrt{\delta / n} = \sqrt{c} \cdot \phi _n(\delta )\).
\end{explanation}

With all these, we can finally solve the \hyperref[def:rate-determining-equation]{rate-determining equation}, which is
\[
	\phi _n (\delta ) \approx \delta ^2
	\implies \sqrt{\frac{\delta}{n}} \approx \delta ^2
	\implies \delta \approx n^{-1 / 3}.
\]
In all, we have the \hyperref[def:rate-of-convergence]{rate of convergence}
\[
	d(\hat{f} _n, f^{\ast} )
	= \sqrt{\frac{1}{n} \sum_{i=1}^{n} \left( \hat{f} _n (i / n) - f^{\ast} (i / n)   \right) ^2 }
	= O_p(n^{-1 / 3}) ,
\]
or more specifically, from \autoref{thm:non-asymptotic-rate-of-convergence-extend},
\[
	\mathbb{P} \left( \frac{1}{n} \sum_{i=1}^{n} \left( \hat{f} _n (i / n) - f^{\ast} (i / n) \right) ^2 > 2^{2M} n^{-2 / 3} \right) \leq c\cdot 2^{-M}.
\]

\begin{remark}
	The \hyperref[def:rate-of-convergence]{rate of convergence} in mean-square error (i.e., w.r.t.\ \(d^2 = L_2^2(\mathbb{P} )\)) for \(1\)-Lipschitz regression is \(n^{-2 / 3}\). Moreover, the ``min-max rate'' worst case over all \(f^{\ast} \in \mathscr{F} \) is \(n^{-2 / 3}\), hence the \hyperref[prb:smooth-LS]{smooth constrained least square estimator} is ``min-max rate optimal''. These terms will make sense in the next \hyperref[rmk:min-max-rate-optimal]{remark}.
\end{remark}

More generally, if we assume that \(f^{\ast} \in \mathcal{S} _{\alpha }\), we can also solve the \hyperref[prb:smooth-LS]{smooth constrained least square} over \(\mathcal{S} _\alpha \) as follows.

\begin{theorem}\label{thm:fixed-design-non-parametric-LS}
	Consider the \hyperref[prb:smooth-LS]{smooth constrained least square} problem over \(\mathcal{S} _\alpha \), the \hyperref[def:rate-of-convergence]{rate of convergence} of \(d(\hat{f} _n, f^{\ast} )\) for the canonical \(d\) is \(O_p(n^{- \frac{\alpha }{2\alpha + 1}})\) for \(\alpha > 1 / 2\).
\end{theorem}
\begin{proof}
	From the same calculation, the corresponding \hyperref[def:localized-EP]{localized empirical process} is bounded by
	\[
		\begin{split}
			\mathbb{E}_{}\left[\sup _{\substack{f\in \mathcal{S} _\alpha \colon                                                                               \\ d(f, f^{\ast} ) \leq \delta }} \frac{1}{n} \sum_{i=1}^{n} \epsilon _i \left( f (i / n) - f^{\ast} (i / n) \right) \right]
			 & \leq \frac{c}{\sqrt{n} } \int_{0}^{\delta } \sqrt{\log N(\mathcal{S} _\alpha , d, \epsilon )}  \,\mathrm{d}\epsilon                            \\
			 & \leq \frac{c}{\sqrt{n} } \int_{0}^{\delta } \sqrt{\log N(\mathcal{S} _\alpha , \lVert \cdot \rVert _\infty , \epsilon )}  \,\mathrm{d}\epsilon
			\leq \frac{c}{\sqrt{n} } \int_{0}^{\delta } \left( \frac{1}{\epsilon } \right) ^{\frac{1}{2\alpha } } \,\mathrm{d}\epsilon
		\end{split}
	\]
	from \autoref{thm:metric-entropy}. Since \(\alpha > 1 / 2\), this bound gives \(\lesssim \frac{1}{\sqrt{n} } \delta ^{1 - 1 / 2\alpha }\), so we can take
	\[
		\phi _n(\delta ) \coloneqq \frac{\delta ^{1 - \frac{1}{2\alpha } }}{\sqrt{n} }.
	\]
	We see that the \hyperref[def:sub-quadratic-assumption]{sub-quadratic assumption} is satisfied since \(\phi _n(c \delta) = c^{1 - \frac{1}{2\alpha }} \phi _n (\delta )\) with \(1 - 1 / 2\alpha < 2\). Solving the \hyperref[def:rate-determining-equation]{rate-determining equation}, we have
	\[
		\phi _n(\delta _n) \approx \delta _n^2
		\implies \frac{\delta _n^{-\frac{1}{2\alpha }}}{\sqrt{n} } \approx \delta_n
		\implies \delta _n\approx n^{- \frac{\alpha}{2\alpha +1}} .
	\]
\end{proof}

It's usually more natural to consider the rate for \(d^2(\hat{f} _n, f^{\ast} ) = L_2^2(\mathbb{P} _n) (\hat{f} _n, f^{\ast} ) = O_p(n^{-\frac{2\alpha }{2\alpha +1}})\).

\begin{note}
	When \(\alpha < 1 / 2\), we need to use the \hyperref[col:Dudley-integral-entropy-bound-finite-resolution]{modified version of Dudley's entropy integral bound}. In this case, we will get a slower rate than \(n^{-2\alpha / 2\alpha + 1}\) w.r.t.\ \(d^2\).
\end{note}

\begin{remark}[Min-max rate optimal]\label{rmk:min-max-rate-optimal}
	\(n^{-2\alpha / 2\alpha +1}\) is \emph{min-max rate optimal}, i.e., it's the ``right'' rate.
\end{remark}
\begin{explanation}
	We just showed that \(\mathbb{E}_{}[ L_2^2 (\mathbb{P} ) ( \hat{f} _n, f^{\ast} )] \leq c n^{-2\alpha / (2\alpha + 1)}\), and it's known that the \emph{min-max rate} is lower-bounded by
	\[
		\inf _{\hat{f} _n} \sup _{f \in \mathscr{F} } \mathbb{E}_{}\left[ L_2^2(\mathbb{P} ) (\hat{f} _n, f^{\ast} )\right] \geq c n^{-\frac{2\alpha}{2\alpha + 1}}.
	\]
	Hence, the rate for the \hyperref[prb:smooth-LS]{smooth constrained least square estimator} is \emph{min-max rate optimal} over \(\mathcal{S} _\alpha \) for \(\alpha > 1 / 2\). However, it's not known whether the same conclusion holds for \(\alpha < 1 / 2\).
\end{explanation}

We do not know \(\alpha \). A natural question is the following.

\begin{problem*}
	Can we adaptively get \(O(n^{-2\alpha / 2\alpha + 1})\) rate without knowing \(\alpha \)?
\end{problem*}
\begin{answer}
	Yes. There are several ways to do this~\cite{tsybakov2008introduction}.
\end{answer}