\lecture{16}{1 Nov. 08:00}{Large-Scale Linear Optimization}
\section{Large-Scale Linear Optimization}

Let's first look at an example.
\begin{eg}
	Consider a constraint matrix
	\[
		\left(\begin{array}{cccccc}
				[\quad\quad\quad\quad                  &                                        &                                        &  &        & \quad\ \quad]                          \\
				\left[\begin{array}{cc} & \\ &\end{array}\right] &                                        &                                        &  &        &                                        \\
				                                       & \left[\begin{array}{cc} & \end{array}\right] &                                        &  & 0      &                                        \\
				                                       &                                        & \left[\begin{array}{cc} & \\ & \\ & \end{array}\right] &  &        &                                        \\
				                                       & 0                                      &                                        &  & \ddots                                          \\
				                                       &                                        &                                        &  &        & \left[\begin{array}{cc} & \\ & \end{array}\right]
			\end{array}\right)
	\]

	We see that if the first constraint(the first row) doesn't exist, then the problem decomposes to
	those small block matrix corresponds to some smaller, easier linear optimization problems, and we can solve it very quickly.

	\begin{note}
		We called the above matrix as \textbf{Nearly Separates}.
	\end{note}
\end{eg}

There is something we need in order to solve the above problem.

\subsection{Decomposition Algorithm}
In this section we describe what is usually known as \textbf{Dantzig-Wolfe Decomposition}. We need
\begin{enumerate}
	\item Simplex Algorithm.
	\item Geometry of Basic feasible solutions and directions.
	\item Duality.
\end{enumerate}

We first see a useful theorem.
\subsubsection{Representation Theorem}
Let \(P\) be
\begin{align*}
	\min~ & c^Tx     \\
	      & Ax = b   \\
	      & x\geq 0.
\end{align*}

\begin{theorem}
	Representation Theorem. Suppose that \(P\) is feasible. Then let \(\mathcal{X}\) be
	\[
		\mathcal{X} \coloneqq \{\hat{x}^j \colon j\in \mathcal{J}\}
	\]
	be the set of basic feasible solutions of \(P\). Also, let \(\mathcal{Z}\) be
	\[
		\mathcal{Z}\coloneqq \{\hat{z}^k\colon k\in \mathcal{K}\}
	\]
	be the set of basic feasible rays of \(P\).

	Then the feasible region of \(P\) is equal to
	\[
		S^\prime\coloneqq \left\{\sum\limits_{j\in\mathcal{J}} \lambda_{j} \hat{x}^j + \sum\limits_{k\in\mathcal{K}}\mu_k \hat{z}^k\colon \sum\limits_{j\in\mathcal{J}}\lambda_{j} = 1;\ \lambda_{j}\geq 0,\ j\in \mathcal{J};\ \mu^k\geq 0,\ k\in \mathcal{K}\right\}.
	\]
\end{theorem}

\begin{proof}
	Let \(S\) be the feasible region of \(P\). We show that \(S = S^\prime\) by showing \(S^\prime\subseteq S\) and \(S^\prime\supseteq S\).
	\begin{enumerate}
		\item \(S^\prime\subseteq S\). Since
		      \[
			      A\left(\sum\limits_{j}\lambda_{j}\hat{x}^j + \sum\limits_{K}\mu_K \hat{z}^K \right)= \sum\limits_j \lambda_{j}\underbrace{\left(A \hat{x}^j\right)}_{=b}+\sum\limits_{K} \mu^K \underbrace{\left(A \hat{z}^K\right)}_{=0} = b.
		      \]
		      Moreover, since everything in the sum is non-negative, we see that \(S^\prime\subseteq S\).
		\item \(S\subseteq S^\prime\). Assume \(\hat{x}\in S\). Then consider the following system
		      \[
			      \begin{split}
				      \substack{n+1\\ \text{ equations}}&\begin{dcases}
					      \sum\limits_{j\in\mathcal{J}} \lambda_{j}\hat{x}^j + \sum\limits_{k\in\mathcal{K} } \mu_k \hat{z}^k & = \hat{x} \\
					      \sum\limits_{j}\lambda_{j}                                                                          & = 1       \\
				      \end{dcases}\\
				      (I)\quad&\lambda_{j}\geq 0 \text{ for }j\in\mathcal{J};\ \mu^k\geq 0 \text{ for }k\in\mathcal{K}.
			      \end{split}
		      \]
		      \begin{note}
			      Keep in mind that in the above system, \(\hat{x}\) and \(\hat{z}\) are fixed, the variables are the \(\lambda_{j}\) and \(\mu_k\).
		      \end{note}

		      Now, instead of directly constructing a solution, we use Farkas' Lemma. Namely, we write down a system such that
		      if this system is infeasible, by Farkas' Lemma, our original system is feasible. Firstly, in Farkas' Lemma, we have
		      \[
			      A = \begin{pmatrix}
				      \hat{x}^1 & \hat{x}^2 & \ldots & \hat{z}^1 & \hat{z}^2 & \hat{z}^3 & \ldots \\
				      1         & 1         & \ldots & 0         & 0         & \ldots    & 0      \\
			      \end{pmatrix},\qquad b = \begin{pmatrix}
				      \hat{x} \\
				      1       \\
			      \end{pmatrix}
		      \]
		      in \((I)\). Now, denote the dual variables with \(w,\ t\), then we have
		      \[
			      \begin{split}
				      \begin{pmatrix}
					      w^{T} & t \\
				      \end{pmatrix}\begin{pmatrix}
					      \hat{x} \\
					      1       \\
				      \end{pmatrix}&>0\\
				      \begin{pmatrix}
					      w^{T} & t \\
				      \end{pmatrix}\begin{pmatrix}
					      \hat{x}^j \\
					      1         \\
				      \end{pmatrix}&\leq 0 \text{ for }j\in\mathcal{J}\\
				      \begin{pmatrix}
					      w^{T} & t \\
				      \end{pmatrix}\begin{pmatrix}
					      \hat{z}^k \\
					      0         \\
				      \end{pmatrix}&\leq 0 \text{ for }k\in\mathcal{K}\\
			      \end{split}
		      \]
		      for \((II)\). We only need to show that \(II\) cannot have a solution. This is easy to show. Firstly,
		      we see that the above inequalities are equivalent to
		      \[
			      \begin{alignedat}{3}
				      &w^{T}\hat{x}+t&&>0\\
				      &w^{T}\hat{x}^j + t&&\leq 0\\
				      &w^{T}\hat{z}^k &&\leq \vec{0}
			      \end{alignedat}\iff\begin{alignedat}{3}
				      &-w^{T}\hat{x}&&<t\\
				      &-w^{T}\hat{x}^j &&\geq \hat{t} &&\text{ for }j\in\mathcal{J}\\
				      &-w^{T}\hat{z}^K &&\geq 0 &&\text{ for }k\in\mathcal{K}.
			      \end{alignedat}
		      \]
		      Now, suppose this does have a solution \(\hat{w}, \hat{t}\). Then, consider
		      \begin{align*}
			      \min~ & -\hat{w}^{T}x(<\hat{t}) \\
			            & Ax = b                  \\
			            & x\geq 0.
		      \end{align*}
		      Notice that the objective value of \(\hat{x}\) here is less than \(\hat{t}\) by \(II\). Since we know that
		      \(Ax = b\), hence this linear programming is feasible. Moreover, from \(-\hat{w}^{T}\hat{x}\leq \hat{t}\) and
		      \(-\hat{w}^{T}\hat{x}^j\geq \hat{t}\), we see that we have a better solution with respect to the objective function
		      among the linear combination of \emph{extreme points} \(\hat{x}^j\). But this is only possible for unbounded linear
		      programming problem, which needs the positive dot product between rays and the objective vector. But from
		      \(-\hat{w}\hat{z}^k\geq 0\), we see that this will never happen, hence the theorem is proved.
		      \begin{figure}[H]
			      \centering
			      \incfig{representation-theorem}
			      \caption{bounded and unbounded case in Simplex Algorithm}
			      \label{fig:representation-theorem}
		      \end{figure}
	\end{enumerate}
\end{proof}

With this representation theorem, consider
\begin{align*}
	\min~         & c^Tx                       \\
	              & Ex\geq h                   \\
	\text{"easy"} & \begin{dcases}
		Ax = b \\
		x\geq 0.
	\end{dcases}
\end{align*}

Then by
\[
	\left\{\underbrace{\sum\limits_{j\in\mathcal{J}} \lambda_{j} \hat{x}^j + \sum\limits_{k\in\mathcal{K}}\mu_k \hat{z}^k}_{=\left\{x\in\mathbb{\MakeUppercase{R}}^n\colon Ax = b,\ x\geq \vec{0}\right\}}
	\colon \sum\limits_{j\in\mathcal{J}}\lambda_{j} = 1, \lambda_{j}\geq 0 \text{ for }j\in \mathcal{J}, \mu^k\geq 0\text{ for } k\in \mathcal{K}\right\},
\]
we turn the linear problem into
\begin{align*}
	\min~ & c^T\left(\sum\limits_{j\in\mathcal{J}}\lambda_{j}\hat{x}^j + \sum\limits_{k\in\mathcal{K}}\mu_k \hat{z}^k  \right)      \\
	      & E\left(\sum\limits_{j\in\mathcal{J}}\lambda_{j}\hat{x}^j + \sum\limits_{k\in\mathcal{K}}\mu_k \hat{z}^k  \right) \geq h \\
	      & \sum\limits_{j\in\mathcal{J}}\lambda_{j} = 1                                                                            \\
	      & \lambda_{j}\geq 0 \text{ for }j\in\mathcal{J},\ \mu_K\geq 0 \text{ for }k\in\mathcal{K}.
\end{align*}

Furthermore, this is equivalent to
\begin{align*}
	\min~    & \sum\limits_{j\in\mathcal{J}}\left(c^T\hat{x}^j\right)\lambda_{j} + \sum\limits_{k\in\mathcal{K}}\left(c^{T} \hat{z}^k  \right)\mu_k \\
	         & \sum\limits_{j\in\mathcal{J}}\left(E\hat{x}^j\right)\lambda_{j} + \sum\limits_{k\in\mathcal{K}}\left(E \hat{z}^k \right)\mu_k \geq h \\
	         & \sum\limits_{j\in\mathcal{J}}\lambda_{j} = 1                                                                                         \\
	(M)\quad & \lambda_{j}\geq 0 \text{ for }j\in\mathcal{J},\ \mu_k\geq 0 \text{ for }k\in\mathcal{K}.
\end{align*}
The system is now extremely reduced, but the cost is that we now have huge amount of variables. We call this as the \emph{Master Problem}.

We formalize the above result as so-called Decomposition Theorem.
\begin{theorem}
	\label{Decomposition Theorem}
	The Decomposition Theorem. Let
	\begin{align*}
		\min~    & c^Tx      \\
		         & Ex \geq h \\
		         & Ax = b    \\
		(Q)\quad & x\geq 0
	\end{align*}
	Let \(S\coloneqq \left\{ x\in\mathbb{\MakeUppercase{R}}^n\colon Ax = b, x\geq 0 \right\}\), \(\mathcal{X} \coloneqq \left\{\hat{x}^j\colon j\in\mathcal{J} \right\}\) be
	the set of basic feasible solution \(S\) and \(\mathcal{Z} \coloneqq \left\{\hat{z}^k\colon k\in\mathcal{K} \right\}\) be the set of basic feasible
	rays of \(S\). Then \(Q\) is equivalent to the \emph{Master Problem }
	\begin{align*}
		\min~    & \sum\limits_{j\in\mathcal{J}}\left(c^T\hat{x}^j\right)\lambda_{j} + \sum\limits_{k\in\mathcal{K}}\left(c^{T} \hat{z}^k  \right)\mu_k \\
		         & \sum\limits_{j\in\mathcal{J}}\left(E\hat{x}^j\right)\lambda_{j} + \sum\limits_{k\in\mathcal{K}}\left(E \hat{z}^k \right)\mu_k \geq h \\
		         & \sum\limits_{j\in\mathcal{J}}\lambda_{j} = 1                                                                                         \\
		(M)\quad & \lambda_{j}\geq 0 \text{ for }j\in\mathcal{J},\ \mu_k\geq 0 \text{ for }k\in\mathcal{K}.
	\end{align*}
\end{theorem}

\begin{remark}
	We think of \(E\) being a \emph{complicated} constraints matrix, while \(A\) is much easier. Further, the reason why we choose \(\leq \) for \(E\) and \(=\) for \(A\) is not because
	this makes them complicated or easy, but only for our convenience. In deed, we will soon see that we can turn \(M\) into a standard form problem without increasing complexity.
\end{remark}

\subsection{Solution of the Master Problem via the Simplex Algorithm}
We now want to solve \(M\). And since we can't write out \(M\) explicitly since there are too many variables. But instead, we can reasonably
\emph{maintain} a basic solution of \(\overline{M}\), the standard form of \(M\). Furthermore, the only part of the Simplex Algorithm that is
sensitive to the total number of variables is when we check for variables with negative reduced cost. So we now try to find an indirect way to
check this rather than find it one by one.

\hr

Denotes the dual variable of \(M\) as \(y\) and \(\sigma\) with \(y\geq \vec{0}\) and \(\sigma\) unrestricted. We further turn \(M\) into
the standard form problem, which is just
\begin{align*}
	\min~               & \sum\limits_{j\in\mathcal{J}}\left(c^T\hat{x}^j\right)\lambda_{j} + \sum\limits_{k\in\mathcal{K}}\left(c^{T} \hat{z}^k  \right)\mu_k   \\
	                    & \sum\limits_{j\in\mathcal{J}}\left(E\hat{x}^j\right)\lambda_{j} + \sum\limits_{k\in\mathcal{K}}\left(E \hat{z}^k \right)\mu_k - Is = h \\
	                    & \sum\limits_{j\in\mathcal{J}}\lambda_{j} = 1                                                                                           \\
	(\overline{M})\quad & \lambda_{j}\geq 0 \text{ for }j\in\mathcal{J},\ \mu_K\geq 0 \text{ for }k\in\mathcal{K}, s\geq 0.
\end{align*}

Suppose that \(\overline{y}\), \(\overline{\sigma}\) forms a basic dual solution. The reduced cost of \(\lambda_{j}\) associated
with \(\hat{x}^j\) is
\[
	(c^{T}\hat{x}^j) - \begin{pmatrix}
		\overline{y}^{T} & \overline{\sigma} \\
	\end{pmatrix}\begin{pmatrix}
		E\hat{x}^j \\
		1          \\
	\end{pmatrix} = c^{T}\hat{x}^j - \hat{y}^{T}E\hat{x}^j - \overline{\sigma} = (c^{T} - \overline{y}^{T}E)\hat{x}^j - \overline{\sigma}
\]
since \(\overline{c}_{\eta_{j}} = c_{\eta_{j}} - \overline{y}^{T}A_{\eta_{j}}\).

\begin{problem}
Is there a \(\lambda_{j}\) with this reduced cost negative?
\end{problem}

\begin{answer}
	Consider
	\begin{align*}
		-\sigma + \min~ & (c^{T} - \overline{y}^{T}E)x \\
		                & Ax = b                       \\
		                & x\geq 0.
	\end{align*}
\end{answer}

