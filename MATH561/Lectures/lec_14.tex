\lecture{14}{25 Oct. 08:00}{Sensitivity Analysis}

\section{Sensitivity Analysis}
As usual, we start with the primal and the dual
\[
	\begin{alignedat}{5}
		\min~&c^{T}x\qquad\qquad&&\max ~&&y^{T}b\\
		&Ax = b 				&&		&&y^{T}A\leq c^{T}\\
		(P)\quad	&x\geq  0 	&&(D)\quad&&
	\end{alignedat}
\]
with an optimal basic partition \(\beta, \eta\) such that
\[
	\overline{x} \coloneqq \begin{dcases}
		\overline{x}_{\beta} \coloneqq A_\beta^{-1} b \geq  \vec{0} \\
		\overline{x}_{\eta} \coloneqq \vec{0}
	\end{dcases}, \qquad \overline{y}^{T} \coloneqq c_{\beta}^{T}A^{-1}_{\beta}.
\]

\begin{prev}
	The dual feasibility is
	\[
		\overline{c}_{\eta} \coloneqq c_{\eta} - c_{\beta}^{T}A_{\beta}^{-1}A_{\eta} = c_{\eta} - \overline{y}^{T}A_{\eta}\geq \vec{0}.
	\]
\end{prev}

\subsection{Local Analysis}
\subsubsection{Change \(b\) on the right-hand side}
We let
\[
	b\to b+\Delta_i e_i = \begin{pmatrix}
		b_1            \\
		b_2            \\
		\vdots         \\
		b_{i}+\Delta_i \\
		\vdots         \\
		b_m            \\
	\end{pmatrix},
\]
then
\[
	A_{\beta}^{-1}(b+\Delta_{i}e_{i}) = A_{\beta}^{-1}b +\Delta_{i}\underbrace{A_{\beta}^{-1}e_{i}}_{h^{i}},
\]
where \(h_{i}\) is the \(i^{th}\) column of \(A^{-1}_{\beta}\). So now we have
\[
	\overline{x}_{\beta} + \Delta_{i}h^i \geq \vec{0},
\]
where we need \(\beta, \eta\) to still be an optimal partition.

\subsubsection{Objective Value}
Now, the objective value is
\[
	c_{\beta}^{T}(\overline{x}_{\beta}+\Delta_{i}A_{\beta}^{-1}e_{i})+ c_{\eta}^{T}\vec{0} = \underbrace{c_{\beta}^{T}\overline{x}_{\beta}}_{\substack{\text{old obj.}\\\text{value}}}+\Delta_{i}\underbrace{c_{\beta}^{T}A_{\beta}^{-1}}_{\overline{y}^{T}}e_{i} = c_{\beta}^{T}\overline{x}_{\beta}+\Delta_{i}\overline{y}^{T}_i.
\]

\subsubsection{Analysis}
Let \(f\) be
\begin{align*}
	f(b)\coloneqq \min~ & c^Tx    \\
	                    & Ax = b  \\
	(P_b)\quad          & x\geq 0
\end{align*}
where
\[
	f: \mathbb{\MakeUppercase{R}}^{m}\to \mathbb{\MakeUppercase{R}}.
\]

We see that since the optimal objective value is equal for the dual of \(P_b\), then \(f(b) = y^{T}b\). Then
\[
	\frac{\partial f}{\partial b_{i}} = \overline{y}_i
\]
if \(\overline{x}_{\beta}>\vec{0}\).

\begin{problem}
For what values of \(\Delta_{i}\) is
\[
	\overline{x}_{\beta} + \Delta_{i}h^i \geq \vec{0}?
\]
\end{problem}
\begin{answer}
	Firstly, we see that we need
	\[
		\overline{x}_{\beta_K}+\Delta_{i}h^i_K \geq 0 \text{ for }K = 1, \ldots , m.
	\]

	Equivalently,
	\[
		\Delta_{i}h^i_K \geq -\overline{x}_{\beta_K},
	\]
	hence
	\[
		\begin{dcases}
			\Delta_{i}\geq \frac{-\overline{x}_{\beta_K}}{h^i_K}, & \text{ if }h^i_K>0, \\
			\Delta_{i}\leq \frac{-\overline{x}_{\beta_K}}{h^i_K}, & \text{ if }h^i_K<0.
		\end{dcases}
	\]

	We define \(L_{i}, U_{i}\) such that
	\[
		L_{i}\leq \Delta_{i}\leq U_{i}
	\]
	where
	\[
		L_{i} \coloneqq \max_{K\colon h^i_K > 0}\{-\overline{x}_{\beta_K}/h^i_K\},\qquad U_{i} \coloneqq \min_{K\colon h^i_K < 0}\{-\overline{x}_{\beta_K}/h^i_K\}.
	\]

	\textbf{Reality Check}. We see that
	\[
		L_{i}\leq 0\leq U_{i}.
	\]

	\begin{remark}
		Noting that if \(h^i_K\leq 0\) for all \(K\), then we define \(L_{i} \coloneqq -\infty \). Similarly, if \(h^i_K\geq 0\) for all \(K\), we define \(U_i \coloneqq \infty \).
	\end{remark}
\end{answer}

\subsection{Global Analysis}
We start with a theorem.
\begin{theorem}
	The domain of \(f\) is a convex set.
\end{theorem}
Assume that the dual of \(P_b\) is feasible, where we denote the dual as \(D_b\):
\begin{align*}
	\max~      & y^{T}b            \\
	(D_b)\quad & y^{T}A\leq c^{T}.
\end{align*}

\begin{proof}
	Now, the domain is the set of \(b\) such that \(P_b\) is feasible. Mathematically,
	\[
		S\coloneqq \left\{ b\colon Ax = b, x\geq 0 \text{ are feasible.}\right\}\subseteq \mathbb{\MakeUppercase{R}}^m.
	\]
	Suppose \(b^1, b^2\in S\). We want to check
	\[
		\lambda b^{1}+(1-\lambda) b^2 \in S\text{ for }0<\lambda<1.
	\]
	Notice that there is an \(x^1\) such that
	\[
		Ax^1 = b^1, x^1\geq \vec{0}
	\]
	and there is an \(x^2\) such that
	\[
		Ax^2 = b^2, x^2\geq \vec{0}.
	\]

	Firstly, we check that \(\lambda x^{1}+(1 - \lambda)x^2\) is non-negative. This is clear since all components are non-negative. Then we check
	\[
		A(\lambda x^{1}+(1 - \lambda)x^2) = \lambda b^1 + (1 - \lambda)b^2.
	\]
	This is clear since
	\[
		A(\lambda x^{1}+(1 - \lambda)x^2) = \lambda Ax^1 + (1 - \lambda)Ax^2 = \lambda b^1 + (1-\lambda)b^2.
	\]
\end{proof}

We now introduce the convexity of a function.
\begin{definition}
	We say that \(f\) is a \emph{convex function} on a convex domain \(S\) if
	\[
		x^1, x^2\in S\text{ and }0<\lambda<1,
	\]
	then
	\[
		f(\lambda x^1+(1 - \lambda)x^2)\leq \lambda f(x^1)+(1-\lambda)f(x^2).
	\]
\end{definition}

\begin{figure}[H]
	\centering
	\incfig{convex-function}
	\caption{Convex Function}
	\label{fig:convex-function}
\end{figure}

\subsubsection{Affine Function}
Before we go further, we need to have several definitions.
\begin{definition}
	A function \(f\colon \mathbb{\MakeUppercase{R}}^{m}\to \mathbb{\MakeUppercase{R}}\) is \emph{affine} if
	\[
		f(u_1, u_2, \ldots , u_m) = a_0 + \sum\limits_{i=1}^{m} a_{i}u_{i}
	\]
	for \(a_i \in\mathbb{\MakeUppercase{R}},\ i = 0, \ldots , m\).
\end{definition}

\begin{remark}
	If \(a_0 = 0\), then \(f\) is a linear function.
\end{remark}

\begin{definition}
	A function \(f\colon \mathbb{\MakeUppercase{R}}^{m}\to \mathbb{\MakeUppercase{R}}\) is a \emph{convex piece-wise linear function} if
	\textbf{\(f\) is the point-wise maximum of affine functions.}
\end{definition}

\begin{figure}[H]
	\centering
	\incfig{convex-piecewise-linear-function}
	\caption{Convex Piece-wise Linear Function}
	\label{fig:convex-piecewise-linear-function}
\end{figure}

Now, suppose \(f_{i}\colon \mathbb{\MakeUppercase{R}}^m \to \mathbb{\MakeUppercase{R}}\) for \(i = 1, \ldots , K\) and assume that each is affine. Then define
\[
	f(x)\coloneqq \max_{1\leq i\leq K} \left\{ f_i(x) \right\}.
\]

\begin{theorem}
	The point-wise maximum of affine functions is a convex function.
\end{theorem}

\begin{proof}
	We see that
	\[
		\begin{split}
			f(\lambda x^1 + (1 - \lambda)x^2) &=\max_{1\leq i\leq K}\left\{ f_i(\lambda x^1 + (1 - \lambda)x^2) \right\}\\
			&=\max_{1\leq i\leq K}\left\{ \lambda f_{i}(x^1) + (1 - \lambda)f_{i}(x^2) \right\}\\
			&\geq \max_{1\leq i\leq K}\left\{ \lambda f_{i}(x^1)\right\} + \max_{1\leq i\leq K}\left\{(1 - \lambda)f_{i}(x^2) \right\}\\
			&=\lambda\max_{1\leq i\leq K}\left\{f_{i}(x^1)\right\} + (1 - \lambda)\max_{1\leq i\leq K}\left\{f_{i}(x^2) \right\}\\
			&= \lambda f(x^1)+(1 - \lambda)f(x^2).
		\end{split}
	\]
\end{proof}

\begin{remark}
	The second equality holds since
	\[
		\begin{split}
			&\max_{1\leq i\leq K}\left\{ a_{i0}+\sum\limits_{l=1}^{m} a_{il}(\lambda u^1_l + (1 - \lambda)u^2_l) \right\}\\
			= &\max_{1\leq i\leq K}\left\{ \lambda a_{i0}+(1 - \lambda)a_{i0} + \sum\limits_{l=1}^{m} a_{il}(\lambda u^1_l + (1 - \lambda)u^2_l) \right\}
		\end{split}
	\]
\end{remark}