\chapter{\(M\)-Estimation}
\lecture{24}{16 Apr.\ 9:30}{The Maximum Likelihood Estimator}
Consider \(X, X_1, \dots , X_n \overset{\text{i.i.d.} }{\sim } F\), and we would like to say something about \(F\) or \(T(F)\). To do this, one can we first obtain the empirical cdf \(\hat{F} _n\), and plug in \(T(\hat{F} _n)\). This is what we did previously.

\begin{eg}[Quantile]
	The \(p^{\text{th} }\)-quantile is defined as \(T(F) = \theta _p = F^{-1} (p)\), which is estimated by the sample \(p^{\text{th} }\)-quantile \(T(\hat{F} _n) = \hat{\theta} _p = \hat{F} _n ^{-1} (p)\).
\end{eg}

\begin{eg}[Moment]
	The \(k^{\text{th} }\)-moment is defined as \(T(F) = \mu _k = \int (x - \mu )^k F(\mathrm{d} x)\) where \(\mu = \int x F(\mathrm{d} x)\), which is estimated by the sample \(k^{\text{th} }\)-central moment \(T(\hat{F} _n) = M_k = \frac{1}{n} \sum_{i=1}^{n} (X_i - \overline{X} _n)^k\).\footnote{Note that we're essentially interpreting \(M_k = \frac{1}{n}\sum_{i=1}^{n} (X_i - \overline{X} _n)^k\) as \(\int (x - \overline{X} _n) \hat{F} _n(\mathrm{d} x)\) with \(\overline{X} _n = \int x \hat{F} _n(\mathrm{d} x)\).}
\end{eg}

On the other hand, instead of directly estimating \(F\), we can start by postulating a family of cdfs \(\{ G_\theta \colon \theta  \in \Theta \} \) where \(\Theta \) is a metric space, with the goal being to choose \(\hat{\theta} _n\) among \(\Theta \) such that \(G_{\hat{\theta} _n}\) approximates \(F\). Our choice, \(\hat{\theta} _n\), should be a function of the data \(X_1, \dots , X_n\).

\section{Maximum Likelihood Estimator}
If we assume \(G_\theta \) has a corresponding density \(g_\theta \) for all \(\theta \in \Theta \), then a natural choice is to select \(\hat{\theta} _n \in \Theta \) by maximizing the \emph{likelihood function}, i.e., the well-known \hyperref[def:MLE]{maximum likelihood estimator} (MLE).

\begin{definition}[Maximum likelihood eestimator]\label{def:MLE}
	Given \(X_1, \dots , X_n \overset{\text{i.i.d.} }{\sim } F\) and a family of cdfs \(\{ G_\theta \colon \theta \in \Theta \} \) for some metric space \(\Theta \). The \emph{maximum likelihood estimator} \(\hat{\theta} _n\) is the maximizer of the likelihood function, i.e., \(\hat{\theta} _n = \argmax_{\theta \in \Theta } \prod_{i=1}^{n} g_\theta (X_i)\).
\end{definition}

\subsection{Divergence Minimizing}
This is all good, since we're just proposing a method. The true motivating question is the following.

\begin{problem*}
	What does the \hyperref[def:MLE]{maximum likelihood estimator} \(\hat{\theta} _n\) estimating?
\end{problem*}

Observe that \(\hat{\theta} _n\) is also a maximizer of \(\frac{1}{n} \sum_{i=1}^{n} \log (g_\theta (X_i))\).\footnote{We adapt the convention that \(\log (0) \coloneqq -\infty \) since it happens when \(\{ G_\theta  \} \) is misspecified.} As \(n\to \infty \), from the \hyperref[thm:SLLN]{strong law of large number}, we know that for all \(\theta \in \Theta \),
\[
	\frac{1}{n} \sum_{i=1}^{n} \log (g_\theta (X_i))
	\overset{\text{a.s.} }{\to} \mathbb{E}_{}[\log (g_\theta (X))]
	\eqqcolon M(\theta ) \in [-\infty , \infty ].
\]

\begin{note}
	We will need to assume that \(M(\theta ) > -\infty \) for some \(\theta \), otherwise there's no hope.
\end{note}

Since \(\hat{\theta} _n\) is maximizing the left-hand side, it's natural to conjecture the following.

\begin{conjecture}\label{conj:MLE-consistency}
	\(\hat{\theta} _n\) should ``converge'' to the maximizer of \(M(\theta )\).
\end{conjecture}

To see why this might be the case, assuming \(F\) has a density \(f\), then we can write
\[
	M(\theta )
	= \int_{\mathbb{R} } \log g_\theta (x) f(x) \,\mathrm{d} x
	= \int_{\mathbb{R} } \log \frac{g_\theta (x)}{f(x)} f(x) \,\mathrm{d} x + \int_{\mathbb{R} } \log f(x) f(x) \,\mathrm{d} x .
\]
The second term is independent of \(\theta \), and the first term is just the \href{https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence}{Kullback-Leibler divergence} since
\[
	\int_{\mathbb{R} } \log \frac{g_\theta (x)}{f(x)} f(x) \,\mathrm{d} x
	= - \int_{\mathbb{R} } \log \frac{f(x)}{g_\theta (x)} f(x) \,\mathrm{d} x
	= - \KL(f \Vert g_\theta ) .
\]

\begin{remark}
	The maximizer of \(M(\theta )\) is minimizing the \href{https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence}{KL divergence} \(\KL(f \Vert g_\theta ) \).
\end{remark}

Let \(\theta ^{\ast} = \argmax_{\theta \in \Theta } M(\theta )\). Then if we can actually show \autoref{conj:MLE-consistency}, then we can conclude that \(\hat{\theta} _n\) is estimating \(\theta ^{\ast} \), i.e., \(\hat{\theta} _n\) also tries to minimize the \href{https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence}{KL divergence} between \(f\) and \(g_{\hat{\theta} _n}\).

\begin{eg}[Supremum estimation]\label{eg:supremum-estimation-MLE}
	Suppose \(X_1, \dots , X_n\) follows \(\mathbb{P} (0 \leq X \leq u) = 1\) for some \(u > 0\) such that for all \(\epsilon >0\), \(F(u - \epsilon ) < 1 = F(u)\). Consider the family of uniform distribution \(\{ G_\theta = \mathcal{U} (0, \theta ) \colon \theta > 0\} \). The likelihood function is given by
	\[
		\Theta \ni \theta
		\mapsto \prod_{i=1}^{n} \frac{1}{\theta ^n} \mathbbm{1}_{\max _{1 \leq i \leq n} X_i \leq \theta },
	\]
	hence the maximizer of the likelihood function is given by \(\hat{\theta} _n = \max _{1 \leq i \leq n} X_i\).

	On the other hand, \(M(\theta ) = \mathbb{E}_{}[\log g_\theta (X)] \) for all \(\theta  > 0\), and we want to find \(\sup _{\theta > 0} M(\theta )\). But if \(\theta < u\), \(\mathbb{P} (g_\theta (x) = 0) > 0\), then \(M(\theta ) = -\infty \), hence
	\[
		\sup _{\theta > 0} M(\theta )
		= \sup _{\theta \geq u} M(\theta )
		= \sup _{\theta \geq u} \log \frac{1}{\theta },
	\]
	which is attained by \(u\). From the very \hyperref[eg:supremum-estimation]{first example}, indeed, \(\hat{\theta} _n \overset{\text{a.s.} }{\to} u\).
\end{eg}

Next, for the following examples let's consider the following setup: consider a family \(\{ G_\theta (x) = G(x - \theta ) \}\) for some cdf \(G\) with a density \(g\). In this case, \(\hat{\theta} _n\) maximizes \(\prod_{i=1}^{n} g(X_i - \theta )\), or equivalently,
\[
	\hat{\theta} _n
	= \argmax_{\theta \in \Theta } \sum_{i=1}^{n} \log (g(X_i - \theta )).
\]
The following two examples also confirm \autoref{conj:MLE-consistency}.

\begin{eg}[Normal]
	If \(G \sim \mathcal{N} (0, 1)\), i.e., \(g(x) = \frac{1}{\sqrt{2\pi } } e^{- x^2 / 2}\), then
	\begin{itemize}
		\item \(\hat{\theta} _n = \argmax_{\theta \in \Theta } \sum_{i=1}^{n} - (X_i - \theta )^2 = \overline{X} _n\);
		\item \(\theta ^{\ast} = \argmax_{\theta \in \Theta } \mathbb{E}_{}[-(X - \theta )^2] = \mathbb{E}_{}[X] \).
	\end{itemize}
	From \hyperref[thm:SLLN]{strong law of large number}, \(\overline{X} _n \overset{\text{a.s.} }{\to} \mathbb{E}_{}[X] \) assuming it converges.
\end{eg}

\begin{eg}[Laplace]
	If \(G \sim \operatorname{Laplace} (\mu , b)\) where \(\sigma ^2 = 2b^2\), i.e., \(g(x) = \frac{1}{2b} e^{- \frac{\lvert x \rvert}{b}} = \frac{1}{\sigma \sqrt{2} } e^{- \frac{\lvert x - \mu \rvert }{\sigma / \sqrt{2} } }\), then
	\begin{itemize}
		\item \(\hat{\theta} _n = \argmax_{\theta \in \Theta } \sum_{i=1}^{n} - \lvert X_i - \theta \rvert\), which is the sample median;
		\item \(\theta ^{\ast} = \argmax_{\theta \in \Theta } \mathbb{E}_{}[- \lvert X - \theta  \rvert ] \), which is the population median.
	\end{itemize}
	Hence, from \autoref{col:Bahadur-representation}, \(\hat{\theta} _n \overset{p}{\to} \theta ^{\ast} \).
\end{eg}

\begin{remark}
	We see that the \hyperref[def:MLE]{MLE} will do the right thing: when estimating \(\mu \) (hence \(\theta _{1 / 2}\)), as we have shown, for \hyperref[eg:ARE-normal]{normal}, \(\overline{X} _n\) is better than \(\hat{\theta} _{1 / 2}\), while for \hyperref[eg:ARE-Laplace]{Laplace} \(\hat{\theta} _{1 / 2}\) is better.
\end{remark}

However, sometimes we don't know \(\hat{\theta} _n\) and \(\theta ^{\ast} \), hence we don't know whether \autoref{conj:MLE-consistency} is true.

\begin{eg}[Cauchy]\label{eg:MLE-Cauchy}
	If \(G \sim \operatorname{Cauchy} \) with location \(0\) and scale \(1\), i.e., \(g(x) = 1 / \pi (1 + x^2)\), then
	\begin{itemize}
		\item \(\hat{\theta} _n = \argmax_{\theta \in \Theta } \sum_{i=1}^{n} - \log (1 + (X_i - \theta )^2)\);
		\item \(\theta ^{\ast} = \argmax_{\theta \in \Theta } \mathbb{E}_{}[- \log (1 + (X - \theta )^2)] \).
	\end{itemize}
	However, we don't know what are \(\hat{\theta} _n\) and \(\theta ^{\ast} \) this time, hence we need a different technique.
\end{eg}

\subsection{\(M\)-Estimator}
The upshot is that in the above examples, we're dealing with \(\hat{\theta} _n = \argmax_{\theta \in \Theta } \sum_{i=1}^{n} m(X_i - \theta )\) where \(m(x) = - \log (g(x))\). But we're not limited to choosing this specific \(m\).

\begin{eg}
	Consider \(m(x) = x^2 \mathbbm{1}_{\lvert x \rvert \leq k} + (2k\lvert x \rvert - k^2) \mathbbm{1}_{\lvert x \rvert > k} \) for some \(k \in \mathbb{R} \).
\end{eg}

This kind of general estimator in the form of maximizing some \(m\) is called the \hyperref[def:M-estimator]{\(M\)-estimator}.

\begin{definition}[\(M\)-estimator]\label{def:M-estimator}
	Given data \(X_1, \dots , X_n \), not necessarily i.i.d., and a metric space \(\Theta \), let \(m_\theta (x)\) be a function for every \(\theta \in \Theta \), and define a function \(M_n(\theta )\) of \(X_i\)'s as \(M_n (\theta ) \coloneqq \frac{1}{n} \sum_{i=1}^{n} m_\theta (X_i)\). An \emph{\(M\)-estimator} \(\hat{\theta} _n\) is a maximizer of \(M_n\), i.e., \(\hat{\theta} _n = \argmax_{\theta \in \Theta } M_n(\theta )\).
\end{definition}

We see that if \(X_i\)'s are i.i.d., then \(M_n(\theta ) \to M(\theta ) \coloneqq \mathbb{E}_{}[m_\theta (X)] \in [-\infty , \infty )\)\footnote{Again, assume that \(M(\theta ) > -\infty \) for some \(\theta  \in \Theta \), otherwise such a optimization problem is meaningless.} as \(n \to \infty \) for any fixed \(\theta \in \Theta \), where the convergence can be either \hyperref[def:converge-almost-surely]{almost surely} or just \hyperref[def:converge-in-probability]{in probability}.

\begin{notation}
	We will use \(\overunderset{\text{a.s.}}{p}{\to } \) from now on to indicate either \(\overset{\text{a.s.} }{\to} \) or \(\overset{p}{\to} \), depending on the assumption.
\end{notation}

Hence, the goal of an \hyperref[def:M-estimator]{\(M\)-estimator} is to use \((\hat{\theta} _n)\) to approximate \(\theta ^{\ast} = \argmax_{\theta \in \Theta } M(\theta )\), what we really care about. With \(M_n(\theta ) \overunderset{\text{a.s.}}{p}{\to } M(\theta )\), we can also approximate \(\max_{\theta \in \Theta } M(\theta )\).

\begin{intuition}
	We don't need to specify the family \(\{ G_\theta \colon \theta \in \Theta  \} \) explicitly, allowing more flexibility.
\end{intuition}

\section{Consistency}
Our first goal is to establish \(\hat{\theta} _n \overunderset{\text{a.s.}}{p}{\to } \theta ^{\ast} \), i.e., the \hyperref[def:consistent]{consistency} of the \hyperref[def:M-estimator]{\(M\)-estimator}. However, we will see that the \emph{approximate maximizer} \(\hat{\theta} _n\) of \(\theta \mapsto M_n(\theta )\) suffices for our purpose. In particular, we define \(\hat{\theta} _n\) in the way that for a (positive) vanishing sequence \(z_n \overunderset{\text{a.s.}}{p}{\to } 0\) as \(n \to 0\), \(\hat{\theta} _n\) satisfies
\begin{equation}\label{eq:approximate-M-estimator}
	M_n(\hat{\theta} _n)
	\geq \sup _{\theta \in \Theta } M_n(\theta ) - z_n.
\end{equation}

\begin{intuition}
	We don't need to solve the optimization problem \(\argmax_{\theta \in \Theta } M_n(\theta )\) exactly.
\end{intuition}

\subsection{Assumptions for Consistency}
We first consider the most fundamental question:

\begin{problem*}
	Where does \(\hat{\theta} _n\) converge, in what sense, if anywhere?
\end{problem*}

We will need some assumptions around \(\theta ^{\ast} \) to answer this. In particular, consider the following.

\begin{enumerate}[label=(A\arabic*)]
	\item\label{asp:MLE-1} \(M\) has a unique maximizer \(\theta ^{\ast} \), and is well-separated, i.e., for all \(\epsilon > 0\), \(M(\theta ^{\ast} ) > \sup _{\theta \notin B(\theta ^{\ast} , \epsilon )} M(\theta )\).
	\item\label{asp:MLE-2} For all \(\delta > 0\) and \(\epsilon > 0\), \(\mathbb{P} ( \sup _{\theta \notin B(\theta ^{\ast} , \epsilon )} M_n(\theta ) - \sup _{\theta \notin B(\theta ^{\ast} , \epsilon )} M(\theta ) \geq \delta ) \to 0\).
\end{enumerate}
We can also consider a stronger version of \autoref{asp:MLE-2}:
\begin{enumerate}[resume, label=(A\arabic*)]
	\item\label{asp:MLE-3} For all \(\delta > 0\) and \(\epsilon > 0\), \(\mathbb{P} ( \sup _{\theta \notin B(\theta ^{\ast} , \epsilon )} M_n(\theta ) \geq \sup _{\theta \notin B(\theta ^{\ast} , \epsilon )} M(\theta ) + \delta \text{ i.o.} ) = 0\).\footnote{Here, i.o.\ means \emph{infinitely often} (in terms of \(n\)).}
\end{enumerate}

\begin{notation}[Open ball]
	Let \(B(\theta ^{\ast} , \epsilon ) = \{ \theta \in \Theta \colon d(\theta , \theta ^{\ast} ) < \epsilon \} \) be an open ball around \(\theta ^{\ast} \in \Theta \) with radii \(\epsilon > 0\), where \(d\) is the metric on \(\Theta \).
\end{notation}

Let's understand them one-by-one. Firstly, we see that if \(\widetilde{\theta} \neq \theta ^{\ast} \), then \(\widetilde{\theta} \notin B(\theta ^{\ast} , \epsilon )\) for some \(\epsilon > 0\). Hence, \autoref{asp:MLE-1} implies \(M(\theta ^{\ast} ) > \sup _{\theta \notin B(\theta ^{\ast} , \epsilon )} M(\theta ) \geq M(\widetilde{\theta} )\). To satisfy \autoref{asp:MLE-1}, consider the following.

\begin{definition}[Upper semi-continuous]\label{def:upper-semi-continuous}
	A function \(M\colon \Theta \to \mathbb{R} \) is \emph{upper semi-continuous} if for all \(\theta \in \Theta \) and \(\theta _n \to \theta \), \(M(\theta ) \geq \limsup_{n \to \infty} M(\theta _n)\).
\end{definition}

\hyperref[def:upper-semi-continuous]{Upper semi-continuity} is useful since we have the following result from analysis.

\begin{theorem}\label{thm:unique-maximizer-usc}
	If \(\Theta \) is compact and \(M(\theta )\) is \hyperref[def:upper-semi-continuous]{upper semi-continuous}, then \(\sup _{\theta \in \Theta } M(\theta )\) is obtained, i.e., there exists a maximizer \(\theta ^{\ast} \).
\end{theorem}

A simple application of \autoref{thm:unique-maximizer-usc} allows us to prove \autoref{asp:MLE-1}.

\begin{corollary}\label{col:asp:MLE-1}
	If \(\Theta \) is compact, \(M\) is \hyperref[def:upper-semi-continuous]{upper semi-continuous}, and the maximizer \(\theta ^{\ast} \) is unique, then \autoref{asp:MLE-1} is satisfied.
\end{corollary}
\begin{proof}
	Firstly, we can write \(\sup _{\theta \notin B(\theta ^{\ast} , \epsilon )} M(\theta ) = \sup _{\theta \in (B(\theta ^{\ast} , \epsilon ))^{c} } M(\theta ) \). Since \(B(\theta ^{\ast} , \epsilon )\) is open, \((B(\theta ^{\ast} , \epsilon ))^{c} \) is closed, hence compact since \(\Theta \) is compact. Applying \autoref{thm:unique-maximizer-usc}, we see that \(\sup _{\theta \notin B(\theta ^{\ast} , \epsilon )} M(\theta )\) is obtained by a maximizer \(\theta ^{\ast} \), which is unique as assumed.
\end{proof}

Hence, as long as \(\Theta \) is compact and \(M(\theta )\) is reasonable enough, \autoref{asp:MLE-1} can be satisfied. On the other hand, \autoref{asp:MLE-2} is a bit more involved. To understand it, observe the following.

\begin{claim}
	\(\sup _{\theta \in \Theta } \lvert M_n(\theta ) - M(\theta ) \rvert \overset{p}{\to} 0\) implies \autoref{asp:MLE-2}.
\end{claim}
\begin{explanation}
	By the triangle inequality, \(M_n(\theta ) \leq \lvert M_n(\theta ) - M(\theta ) \rvert + M(\theta )\), hence
	\[
		\sup _{\theta \in \Theta } M_n(\theta )
		\leq \sup _{\theta \in \Theta } \lvert M_n(\theta ) - M(\theta ) \rvert  + \sup _{\theta \in \Theta }M(\theta ) \\
		\implies \sup _{\theta \in \Theta } M_n(\theta ) - \sup _{\theta \in \Theta }M(\theta )
		\leq \sup _{\theta \in \Theta }\lvert M_n(\theta ) - M(\theta ) \rvert.
	\]
	Moreover, taking the supremum over a smaller subset, we have
	\[
		\sup _{\theta \notin B(\theta^{\ast}, \epsilon) } M_n(\theta ) - \sup _{\theta \notin B(\theta^{\ast}, \epsilon) }M(\theta )
		\leq \sup _{\theta \in \Theta }\lvert M_n(\theta ) - M(\theta ) \rvert,
	\]
	hence, \(\sup _{\theta \in \Theta } \lvert M_n(\theta ) - M(\theta ) \rvert \overset{p}{\to} 0\) implies \(\mathbb{P} ( \sup _{\theta \notin B(\theta ^{\ast} , \epsilon )} M_n(\theta ) - \sup _{\theta \notin B(\theta ^{\ast} , \epsilon )} M(\theta ) \geq \delta ) \to 0\) for any \(\delta > 0\), which is \autoref{asp:MLE-2}.
\end{explanation}

\begin{intuition}
	\autoref{asp:MLE-2} uniformly controls the convergence.
\end{intuition}

In this regard, by reformulating \autoref{asp:MLE-3} as
\[
	\mathbb{P} \left( \sup _{\theta \notin B(\theta ^{\ast} , \epsilon )} M_n(\theta ) \geq \sup _{\theta \notin B(\theta ^{\ast} , \epsilon )} M(\theta ) + \delta \text{ i.o.} \right) = 0
	\iff \mathbb{P} \left( \limsup_{n \to \infty} \sup _{\theta \notin B(\theta ^{\ast} , \epsilon )} M_n(\theta ) \leq \sup _{\theta \notin B(\theta ^{\ast} , \epsilon )} M(\theta ) \right) = 1,
\]
since the i.o.\ can be expressed as \(\mathbb{P} ( \limsup_{n \to \infty} \sup _{\theta \notin B(\theta ^{\ast} , \epsilon )} M_n(\theta ) \geq \sup _{\theta \notin B(\theta ^{\ast} , \epsilon )} M(\theta ) + \delta  ) = 0\). Then, the same argument shows that \(\sup _{\theta \in \Theta } \lvert M_n(\theta ) - M(\theta ) \rvert \overset{\text{a.s.} }{\to} 0\) implies \autoref{asp:MLE-3}.

\begin{theorem}\label{thm:M-estimator-consistency}
	If \autoref{asp:MLE-1} and \autoref{asp:MLE-2} holds, then \(\hat{\theta} _n \overset{p}{\to} \theta ^{\ast} \). On the other hand, if we replace \autoref{asp:MLE-2} by \autoref{asp:MLE-3}, then we have \(\hat{\theta} _n \overset{\text{a.s.} }{\to} \theta ^{\ast} \).
\end{theorem}