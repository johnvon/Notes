\lecture{21}{4 Apr.\ 9:30}{Slope of a Statistics and Pitman (Local) Alternatives}
\subsection{Deriving the Slope}
Formally, let \(\xi \geq 0\) such that \(\sqrt{n} (\theta _n - \theta _0) \to \xi \), and suppose there exists \(\mu (\theta )\) and \(\sigma (\theta )\) such that
\[
	\sqrt{n} \frac{T_n - \mu (\theta _n)}{\sigma (\theta _n)}
	\overset{D}{\to} \mathcal{N} (0, 1)
	\iff
	\mathbb{P} _{\theta _n} \left( \frac{T_n - \mu (\theta _n)}{\sigma (\theta _n) / \sqrt{n} } \leq x \right)
	\to \Phi (x)
\]
for all \(x \in \mathbb{R} \). Firstly, when \(\xi = 0\), then we know that
\[
	\sqrt{n} \frac{T_n - \mu (\theta _0)}{\sigma (\theta _0)}
	\overset{D}{\to} \mathcal{N} (0, 1).
\]
Hence, we reject \(H_0\) if \(T_n > \mu (\theta _0) + \sigma (\theta _0) Z_\alpha / \sqrt{n} \). We see that this happens with probability
\[
	\mathbb{P} _{\theta _n}(\text{reject} )
	= \mathbb{P} _{\theta _n} \left( T_n > \mu (\theta _0) + \frac{\sigma (\theta _0)}{\sqrt{n} } Z_\alpha \right)
	= \mathbb{P} _{\theta _n} \left( \frac{T_n - \mu (\theta _n)}{\sigma (\theta _n) / \sqrt{n} } > \frac{\mu (\theta _0) - \mu (\theta _n)}{\sigma (\theta _n) / \sqrt{n} } + Z_\alpha \frac{\sigma (\theta _0)}{\sigma (\theta _n)} \right).
\]
If \(\mu \) is differentiable at \(\theta _0\) and \(\sigma \) is continuous at \(\theta _0\), then as \(\sqrt{n} (\theta _n - \theta _0) \to \xi\), the above converges to
\[
	\Phi \left( - \left( - \frac{\mu ^{\prime} (\theta _0)}{\sigma (\theta _0)}\xi + Z_\alpha \right) \right)
	= \Phi \left( \frac{\mu ^{\prime} (\theta _0)}{\sigma (\theta _0)}\xi - Z_\alpha \right).
\]
Let \(\theta ^{\ast} \) to be defined as \(\mathbb{P} _{\theta ^{\ast} }(\text{reject} ) = 1 - \beta \) for some \(\beta > 0\). Then, denote \(n^{\ast} \) such that \(\theta _{n^{\ast} } = \theta ^{\ast} \), and define \(\xi > 0\) such that \(\sqrt{n^{\ast} } (\theta ^{\ast} - \theta _0) = \xi\), i.e., \(\theta ^{\ast} = \theta _0 + \xi / \sqrt{n^{\ast} } \). Then \(\mathbb{P} _{\theta ^{\ast} }(\text{reject} )\) will converge to
\[
	\Phi \left( \frac{\mu ^{\prime} (\theta _0)}{\sigma (\theta _0)}\xi - Z_\alpha \right)
	= 1 - \beta
	\implies \frac{\mu ^{\prime} (\theta _0)}{\sigma (\theta _0)}\xi - Z_\alpha
	= Z_\beta
	\implies \sqrt{n^{\ast} }(\theta ^{\ast} - \theta _0)
	= \xi = \frac{Z_\alpha + Z_\beta }{\mu ^{\prime} (\theta _0) / \sigma (\theta _0)},
\]
solving w.r.t.\ \(\sqrt{n^{\ast} } \) gives
\[
	\sqrt{n^{\ast} }
	= \frac{Z_\alpha + Z_\beta }{\frac{\mu ^{\prime} (\theta _0)}{\sigma (\theta _0)} (\theta ^{\ast} - \theta _0)},
\]
which confirms our heuristic argument.

\begin{remark}
	\(n^{\ast} \) still only depends on \(\mu ^{\prime} (\theta _0) / \sigma (\theta _0)\) from the same reason.
\end{remark}

\subsection{Asymptotic Relative Efficiency for Statistics}
If we have another statistic \(\widetilde{T} \) associates with \(\widetilde{\mu} \), \(\widetilde{\sigma} \), and \(\widetilde{n} ^{\ast} \), such that it also satisfies all the assumptions, i.e., asymptotic normality, differentiability for \(\widetilde{\mu} \), and continuity for \(\widetilde{\sigma} \), then from the same analysis, we can then compare how many samples we need to reach \(\alpha \), \(\beta \), given \(\theta _0\).

\begin{definition}[Asymptotic relative efficiency for statistic]\label{def:asymptotic-relative-efficiency-statistic}
	Given \(\theta _0\), \(\alpha \), and \(\beta \), the \emph{asymptotic relative efficiency} between two statistics \(T_n\) and \(\widetilde{T} _n\) is defined as
	\[
		\ARE(T, \widetilde{T} )
		= \frac{n^{\ast} }{\widetilde{n} ^{\ast} }
		= \left( \frac{\widetilde{\mu} ^{\prime} (\theta _0) / \widetilde{\sigma} (\theta _0)}{\mu ^{\prime} (\theta _0) / \sigma (\theta _0)} \right) ^2.
	\]
\end{definition}

\begin{note}
	Same as \autoref{def:asymptotic-relative-efficiency-estimator}, \autoref{def:asymptotic-relative-efficiency-statistic} is different from the convention, where we usually define the \emph{asymptotic relative efficiency of \(T\) w.r.t.\ \(\widetilde{T} \)} as \(\ARE_\theta (T, \widetilde{T} ) = \widetilde{n} ^{\ast} / n^{\ast} \).
\end{note}

Let compare the \(t\)-test, sign test, and Wilcoxon signed-rank test on the \hyperref[prb:testing-symmetry]{problem of testing symmetry}. In particular, consider the following variation of the problem.

\begin{problem*}
	Let \(\epsilon , \epsilon _1, \dots, \epsilon _n \overset{\text{i.i.d.} }{\sim } F\) where \(F\) is continuous and symmetric around \(0\), i.e., \(\epsilon \overset{D}{=} -\epsilon \). Furthermore, assuming \(X_{n1}, \dots , X_{nn} \overset{\text{i.i.d.} }{\sim } \theta _n + \epsilon _i\) such that \(\sqrt{n} (\theta _n - \theta _0) = \xi \) for some fixed \(\xi \geq 0\). We're interested in testing whether \(H_0 \colon X_{n1} \overset{D}{=} -X_{n1}\). In other words, \(H_0 \colon \theta _0 = 0\).
\end{problem*}

Let's try the simplest sign test and compute its \hyperref[def:slope]{slope}.

\begin{eg}[Sign test]
	The \hyperref[def:slope]{slope} of the averaged \hyperref[def:sign-statistic]{sign statistic} \(\overline{\mathrm{sign}} _n \coloneqq \frac{1}{n} \sum_{i=1}^{n} \mathbbm{1}_{X_{ni} > 0} \) is \(2 f(0)\).
\end{eg}
\begin{explanation}
	We first see that
	\[
		\overline{\mathrm{sign}} _n
		\coloneqq \frac{1}{n} \sum_{i=1}^{n} \mathbbm{1}_{X_{ni} > 0}
		= \frac{1}{n} \sum_{i=1}^{n} \mathbbm{1}_{\epsilon _i > - \theta _n}.
	\]
	The mean of \(\mathbbm{1}_{\epsilon _i > -\theta _n} \) is
	\[
		\mathbb{P} (\epsilon > -\theta _n)
		= \mathbb{P} (\epsilon \leq \theta _n)
		= F(\theta _n)
	\]
	since \(\epsilon \) is symmetric and continuous. Hence, the \hyperref[thm:Lindeberg-CLT]{Lindeberg central limit theorem} gives
	\[
		\frac{\sum_{i=1}^{n} \left( \mathbbm{1}_{\epsilon _i  > -\theta _n} - F(\theta _n) \right) }{\sqrt{n} \sqrt{F(\theta _n) (1 - F(\theta _n))} }
		= \sqrt{n} \frac{\frac{1}{n} \sum_{i=1}^{n} \mathbbm{1}_{\epsilon _i > -\theta _n} - F(\theta _n) }{\sqrt{F(\theta _n) (1 - F(\theta _n)) } }
		= \sqrt{n} \frac{\overline{\mathrm{sign}} _n - F(\theta _n) }{\sqrt{F(\theta _n) (1 - F(\theta _n)) } }
		\overset{D}{\to} \mathcal{N} (0, 1)
	\]
	by checking the \hyperref[col:Lyapunov-CLT]{Lyapunov condition}, indeed, since we have
	\[
		\Var_{}\left[\sum_{i=1}^{n} \mathbbm{1}_{\epsilon _i > -\theta _n} \right]
		= n F(\theta _n) (1 - F(\theta _n))
		\to n F(\theta _0) (1 - F(\theta _0))
		\to \infty.
	\]
	We see that
	\begin{itemize}
		\item \(\mu (\theta ) = F(\theta ) \), hence if \(F\) is differentiable at \(0\) with \(F^{\prime} (0) \eqqcolon f(0)\), then \(\mu ^{\prime} (0) = f(0)\);
		\item \(\sigma (\theta ) = \sqrt{F(\theta )(1 - F(\theta ))} \), so \(\sigma (0) = 1 / 2\) since \(F(0) = 1 / 2\) by the symmetry of \(F\).
	\end{itemize}
	We conclude that the \hyperref[def:slope]{slope} of \(\overline{\mathrm{sign}} _n\) is \(\mu ^{\prime} (0) / \sigma (0) = 2f(0)\).
\end{explanation}

\begin{note}
	For the sign test, we don't need any moment assumption. Additionally, it's expected to be a weak test since it seems only care about the density around \(0\), which is intuitive.
\end{note}

Now, let's try the \(t\)-test. This time, we will need the second moment to exist.

\begin{eg}[\(t\)-test]
	Suppose \(\Var_{}[\epsilon ] = \sigma ^2 < \infty \), then the \hyperref[def:slope]{slope} of the ``normalized'' \hyperref[def:t-statistic]{\(t\)-statistic} \(\widetilde{T} _n \coloneqq T_n / \sqrt{n}\) is \(1 / \sigma \).
\end{eg}
\begin{explanation}
	Since \(\widetilde{T} _n \coloneqq T_n / \sqrt{n} = \overline{X} _n / \hat{\sigma} _n\), with \(\hat{\sigma} _n^2 = \frac{1}{n} \sum_{i=1}^{n} (X_i - \overline{X} _n)^2 = \frac{1}{n} \sum_{i=1}^{n} (\epsilon _i - \overline{\epsilon} _n)^2\), we have
	\[
		\widetilde{T} _n
		= \frac{\overline{X} _n}{\hat{\sigma} _n}
		= \frac{\theta _n + \overline{\epsilon} _n}{\hat{\sigma} _n}
		= \left( \frac{\theta _n}{\hat{\sigma} _n} - \frac{\theta _n}{\sigma } \right) + \left( \frac{\overline{\epsilon} _n}{\hat{\sigma} _n} + \frac{\theta _n}{\sigma } \right)
		\implies
		\sqrt{n} \left( \widetilde{T} _n - \frac{\theta _n}{\sigma } \right)
		= \sqrt{n} \theta _n \left( \frac{1}{\hat{\sigma} _n} - \frac{1}{\sigma } \right) + \sqrt{n} \frac{\overline{\epsilon} _n}{\hat{\sigma} _n}.
	\]
	Since \(\theta _0 = 0\) and \(\sqrt{n} (\theta _n - \theta _0) = \xi \), we have \(\sqrt{n} \theta _n \to \xi \geq 0\), \(1 / \hat{\sigma} _n - 1 / \sigma \overset{p}{\to} 0\), so the first term goes to \(0\). On the other hand, by the usual \hyperref[thm:CLT]{central limit theorem}, \(\sqrt{n} \overline{\epsilon} _n \overset{D}{\to} \mathcal{N} (0, \sigma ^2)\), hence
	\[
		\sqrt{n} \left( \widetilde{T} _n - \frac{\theta _n}{\sigma } \right)
		= \sqrt{n} \frac{\widetilde{T} _n - \frac{\theta _n}{\sigma }}{1}
		\overset{D}{\to} \mathcal{N} (0, 1).
	\]
	We see that
	\begin{itemize}
		\item \(\mu (\theta ) = \theta / \sigma \), which is clearly differentiable with \(\mu ^{\prime} (\theta ) = 1 / \sigma \);
		\item \(\sigma (\theta ) = 1\), so \(\sigma (0) = 1\).
	\end{itemize}
	We conclude that the \hyperref[def:slope]{slope} of \(\widetilde{T} _n\) is \(\mu ^{\prime} (0) / \sigma (0) = 1 / \sigma \).
\end{explanation}

This gives us a way to compare \(\overline{\mathrm{sign} }_n \) and \(\widetilde{T} _n\).

\begin{proposition}
	Consider the problem of \hyperref[prb:testing-symmetry]{testing for symmetry}, the \hyperref[def:asymptotic-relative-efficiency-statistic]{asymptotic relative efficiency} between the \hyperref[def:sign-statistic]{sign statistic} and the \hyperref[def:t-statistic]{\(t\)-statistic} is
	\[
		\ARE(\widetilde{T} _n, \overline{\mathrm{sign}} _n)
		= \left( 2 f(0) \sigma \right) ^2.
	\]
\end{proposition}

\begin{remark}
	From \autoref{prop:sample-quantile-ARE}, \(\ARE(\overline{X} _n , \hat{\theta} _{1 / 2}) = (2 f(0) \sigma )^2\), exactly the same!
\end{remark}

Let's see some actual example, where we can borrow from the previous calculation.\footnote{The previous examples include \hyperref[eg:ARE-normal]{normal} and \hyperref[eg:ARE-Laplace]{Laplace}.}

\begin{eg}[Gaussian]
	If \(\epsilon \sim \mathcal{N} (\mu , \sigma ^2)\), then \(f(x) = \frac{1}{\sigma \sqrt{2\pi } } e^{- x^2 / 2\sigma ^2}\), so \(f(0) = 1 / \sigma \sqrt{2\pi } \), hence
	\[
		\ARE(\widetilde{T} _n, \overline{\mathrm{sign}} _n)
		= \left( \frac{2\sigma }{\sqrt{2\pi } \sigma } \right) ^2
		= \frac{2}{\pi }
		< 1
	\]
\end{eg}

\begin{eg}[Laplace]
	If \(\epsilon \sim \operatorname{Laplace}(\mu , b) \) with \(\sigma ^2 = 2 b^2\), since \(f(0) = 1 /\sqrt{2} \sigma \), hence
	\[
		\ARE(\widetilde{T} _n, \overline{\mathrm{sign}} _n)
		= 4 \sigma ^2 \frac{1}{2 \sigma ^2}
		= 2
		> 1,
	\]
\end{eg}

\begin{eg}[Uniform]
	If \(\epsilon \sim \mathcal{U} (-c, c)\) for some \(c\) such that \(\Var_{}[\epsilon ] = \sigma ^2\), we have
	\[
		\ARE(\widetilde{T} _n, \overline{\mathrm{sign}} _n)
		= \frac{1}{3}.
	\]
\end{eg}
\begin{explanation}
	We see that since \(\sigma ^2 = (2c)^2 / 12 = c^2 / 3\), \(c\) should be \(\sqrt{3} \sigma\). Hence, \(f(0) = 1 / 2c = 1 / 2 \sqrt{3} \sigma \). Plugging in \((2 f(0) \sigma )^2\) gives \(1 / 3\).
\end{explanation}

Finally, let's consider the Wilcoxon signed-rank test. Recall \hyperref[eg:Wilcoxon-signed-rank-statistic-asymptotic-normality]{what we have shown}.

\begin{prev}
	Under \(H_0\), i.e., \(X\) is continuous and \(X \overset{D}{=} -X\), with \(U_n \coloneqq \binom{n}{2}^{-1} \sum_{i < j} h(X_i, X_j)\) where \(h(x_1, x_2) = \mathbbm{1}_{x_1 + x_2 \geq 0} \), we have
	\[
		\frac{\sqrt{n}}{2\cdot \binom{n}{2}}  \left( W_n - \sum_{i=1}^{n} \sgn (X_i) \right)
		= \sqrt{n} \left( U_n - \frac{1}{2} \right)
		\overset{D}{\to} \mathcal{N} (0, 1 / 3).
	\]
\end{prev}

In our case, since \(X_i = \theta _n + \epsilon _i\), we have
\[
	U_n
	= \frac{1}{\binom{n}{2}} \sum_{\{ i, j \} \subseteq [n]} \mathbbm{1}_{X_i + X_j > 0}
	= \frac{1}{\binom{n}{2}} \sum_{\{ i, j \} \subseteq [n]} \mathbbm{1}_{\epsilon _i + \epsilon _j > - 2 \theta _n}.
\]

\begin{eg}[Wilcoxon signed-rank test]
	The \hyperref[def:slope]{slope} of the corresponding \hyperref[def:U-statistic]{\(U\)-statistic} \(U_n\) of the \hyperref[def:Wilcoxon-signed-rank-statistic]{Wilcoxon signed-rank statistic} \(W_n\) is \(2 \sqrt{3} \cdot \int f^2(x) \,\mathrm{d} x\).
\end{eg}
\begin{explanation}
	Let \(h_n(\epsilon _1, \epsilon _2) = \mathbbm{1}_{\epsilon _1 + \epsilon _2 > - 2 \theta _n} \) (note that it depends on \(n\)), then from \autoref{thm:U-statistic},
	\[
		\sqrt{n} \left( U_n - \mathbb{E}_{}[U_n] \right)
		= \frac{2}{\sqrt{n} } \sum_{k=1}^{n} \left( h^{\ast} _n(\epsilon _k) - \mathbb{E}_{}[U_n] \right) + o_p(1)
	\]
	where the \(2 = m\) is the number of arguments of \(h\). Here, \(\mathbb{E}_{}[h_n(\epsilon _1, \epsilon _2)] = \mathbb{E}_{}[U_n] = \mathbb{E}_{}[h^{\ast} _n (\epsilon )] \) where
	\[
		h^{\ast} _n(x)
		= \mathbb{E}_{}[h(x, \epsilon )]
		= \mathbb{P} (x + \epsilon > - 2\theta _n)
		= \mathbb{P} (\epsilon > -x - 2\theta _n)
		= \mathbb{P} (\epsilon \leq x + 2 \theta _n)
		= F(x + 2 \theta _n),
	\]
	with \(\epsilon _i\)'s being i.i.d., by dividing both sides by \(2 \sqrt{\Var_{}[F(\epsilon + 2 \theta _n)] } \) and the \hyperref[thm:CLT]{central limit theorem},
	\[
		\sqrt{n} \frac{U_n - \mathbb{E}_{}[F(\epsilon + 2 \theta _n)] }{2 \sqrt{\Var_{}[F(\epsilon + 2 \theta _n)] } }
		= \frac{1}{\sqrt{n} } \sum_{k=1}^{n} \frac{h^{\ast} _n(\epsilon _k) - \mathbb{E}_{}[F(\epsilon + 2 \theta _n)] }{\sqrt{\Var_{}[F(\epsilon + 2 \theta _n)] } } + o_p(1)
		\overset{D}{\to} \mathcal{N} (0, 1)
	\]

	as long as \(\Var_{}[F(\epsilon + 2 \theta _n)] > 0\). We see that
	\begin{itemize}
		\item \(\mu (\theta ) = \mathbb{E}_{}[F(\epsilon + 2 \theta )] \), if we assume \(f\) exists, it's just
		      \[
			      \mu (\theta ) = \int_{\mathbb{R} } F(x + 2 \theta ) F(\mathrm{d} x)
			      = \int_{\mathbb{R} } F(x + 2 \theta ) f(x) \,\mathrm{d} x.
		      \]
		      If we further assume that we can interchange the derivative and the integral, we then have \(\mu ^{\prime} (\theta ) = \int_{\mathbb{R} } 2 f(x + 2\theta ) f(x) \,\mathrm{d} x\), giving \(\mu ^{\prime} (0) = 2 \int_{\mathbb{R}} f^2(x) \,\mathrm{d} x\).
		\item \(\sigma (\theta ) = 2 \sqrt{\Var_{}[F(\epsilon + 2 \theta )] } \), hence \(\sigma (0) = 2 \sqrt{\Var_{}[F(\epsilon )]} = 2 \cdot \sqrt{1 / 12} = 1 / \sqrt{3}  \).
	\end{itemize}
	Hence, the \hyperref[def:slope]{slope} of \(U_n\) is \(2 \sqrt{3} \cdot \int_{\mathbb{R}} f^2(x) \,\mathrm{d} x\).
\end{explanation}

This gives us a way to compare \(U_n\) and \(\widetilde{T} _n\), i.e.,
\[
	\ARE(\widetilde{T} _n, U_n)
	= \left( \frac{2 \sqrt{3} \int_{\mathbb{R} } f^2(x)\,\mathrm{d} x}{1 / \sigma } \right) ^2
	= 12 \sigma ^2 \left( \int_{\mathbb{R} } f^2(x) \,\mathrm{d} x \right) ^2.
\]