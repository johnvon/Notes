\lecture{21}{4 Apr.\ 9:30}{Slope of a Statistics and Pitman (Local) Alternatives}
\subsection{Deriving the Slope}
Formally, let \(\xi \geq 0\) such that \(\sqrt{n} (\theta _n - \theta _0) \to \xi \), and suppose there exists \(\mu (\theta )\) and \(\sigma (\theta )\) such that
\[
	\sqrt{n} \frac{T_n - \mu (\theta _n)}{\sigma (\theta _n)}
	\overset{D}{\to} \mathcal{N} (0, 1)
	\iff
	\mathbb{P} _{\theta _n} \left( \frac{T_n - \mu (\theta _n)}{\sigma (\theta _n) / \sqrt{n} } \leq x \right)
	\to \Phi (x)
\]
for all \(x \in \mathbb{R} \). Firstly, when \(\xi = 0\), then we know that
\[
	\sqrt{n} \frac{T_n - \mu (\theta _0)}{\sigma (\theta _0)}
	\overset{D}{\to} \mathcal{N} (0, 1).
\]
Hence, we reject \(H_0\) if \(T_n > \mu (\theta _0) + \sigma (\theta _0) Z_\alpha / \sqrt{n} \). We see that this happens with probability
\[
	\mathbb{P} _{\theta _n}(\text{reject} )
	= \mathbb{P} _{\theta _n} \left( T_n > \mu (\theta _0) + \frac{\sigma (\theta _0)}{\sqrt{n} } Z_\alpha \right)
	= \mathbb{P} _{\theta _n} \left( \frac{T_n - \mu (\theta _n)}{\sigma (\theta _n) / \sqrt{n} } > \frac{\mu (\theta _0) - \mu (\theta _n)}{\sigma (\theta _n) / \sqrt{n} } + Z_\alpha \frac{\sigma (\theta _0)}{\sigma (\theta _n)} \right).
\]
If \(\mu \) is differentiable at \(\theta _0\) and \(\sigma \) is continuous at \(\theta _0\), then as \(\sqrt{n} (\theta _n - \theta _0) \to \xi\), the above converges to
\[
	\Phi \left( - \left( - \frac{\mu ^{\prime} (\theta _0)}{\sigma (\theta _0)}\xi + Z_\alpha \right) \right)
	= \Phi \left( \frac{\mu ^{\prime} (\theta _0)}{\sigma (\theta _0)}\xi - Z_\alpha \right).
\]
Let \(\theta ^{\ast} \) to be defined as \(\mathbb{P} _{\theta ^{\ast} }(\text{reject} ) = 1 - \beta \) for some \(\beta > 0\). Then, denote \(n^{\ast} \) such that \(\theta _{n^{\ast} } = \theta ^{\ast} \), and define \(\xi > 0\) such that \(\sqrt{n^{\ast} } (\theta ^{\ast} - \theta _0) = \xi\), i.e., \(\theta ^{\ast} = \theta _0 + \xi / \sqrt{n^{\ast} } \). Then \(\mathbb{P} _{\theta ^{\ast} }(\text{reject} )\) will converge to
\[
	\Phi \left( \frac{\mu ^{\prime} (\theta _0)}{\sigma (\theta _0)}\xi - Z_\alpha \right)
	= 1 - \beta
	\implies \frac{\mu ^{\prime} (\theta _0)}{\sigma (\theta _0)}\xi - Z_\alpha
	= Z_\beta
	\implies \sqrt{n^{\ast} }(\theta ^{\ast} - \theta _0)
	= \xi = \frac{Z_\alpha + Z_\beta }{\mu ^{\prime} (\theta _0) / \sigma (\theta _0)},
\]
solving w.r.t.\ \(\sqrt{n^{\ast} } \) gives
\[
	\sqrt{n^{\ast} }
	= \frac{Z_\alpha + Z_\beta }{\frac{\mu ^{\prime} (\theta _0)}{\sigma (\theta _0)} (\theta ^{\ast} - \theta _0)}.
\]

\begin{remark}
	This confirms our heuristic argument. Moreover, \(n^{\ast} \) still only depends on \(\mu ^{\prime} (\theta _0) / \sigma (\theta _0)\) from the same reason.
\end{remark}

\subsection{Asymptotic Relative Efficiency for Statistics}
If we have another statistics \(\widetilde{T} \) which has \(\widetilde{\mu} \), \(\widetilde{\sigma} \), and \(\widetilde{n} ^{\ast} \), such that it also satisfies all the assumptions, i.e., asymptotic normality, differentiability for \(\mu \), and continuity for \(\sigma \), then from the same analysis, we can then compare how many samples we need to reach \(\alpha \), \(\beta \), given \(\theta _0\).

\begin{definition}[Asymptotic relative efficiency for statistics]\label{def:asymptotic-relative-efficiency-statistics}
	Given \(\theta _0\), \(\alpha \), and \(\beta \), the \emph{asymptotic relative efficiency} between two statistics \(T_n\) and \(\widetilde{T} _n\) is defined as
	\[
		\operatorname{ARE}(T, \widetilde{T} )
		= \frac{n^{\ast} }{\widetilde{n} ^{\ast} }
		= \left( \frac{\widetilde{\mu} ^{\prime} (\theta _0) / \widetilde{\sigma} (\theta _0)}{\mu ^{\prime} (\theta _0) / \sigma (\theta _0)} \right) ^2.
	\]
\end{definition}

Let compare the \(t\)-test, sign test, and Wilcoxon signed-rank test on the problem of testing symmetry.

\begin{problem*}
	Let \(\epsilon , \epsilon _1, \dots, \epsilon _n \overset{\text{i.i.d.} }{\sim } F\) where \(F\) is continuous and symmetric around \(0\), i.e., \(\epsilon \overset{D}{=} -\epsilon \). Furthermore, assuming \(X_{n1}, \dots , X_{nn} \overset{\text{i.i.d.} }{\sim } \theta _n + \epsilon _i\) such that \(\sqrt{n} (\theta _n - \theta _0) = \xi \) for some fixed \(\xi \geq 0\). We're interested in testing whether \(H_0 \colon X_{n1} \overset{D}{=} -X_{n1}\). In other words, \(H_0 \colon \theta _0 = 0\).
\end{problem*}

\begin{eg}[Sign test]
	The \hyperref[def:slope]{slope} of the averaged sign statistics \(\overline{\mathrm{sign}} _n \coloneqq \frac{1}{n} \sum_{i=1}^{n} \mathbbm{1}_{X_{ni} > 0} \) is \(2 f(0)\).
\end{eg}
\begin{explanation}
	We first see that
	\[
		\overline{\mathrm{sign}} _n
		\coloneqq \frac{1}{n} \sum_{i=1}^{n} \mathbbm{1}_{X_{ni} > 0}
		= \frac{1}{n} \sum_{i=1}^{n} \mathbbm{1}_{\epsilon _i > - \theta _n}.
	\]
	The mean of \(\mathbbm{1}_{\epsilon _i > -\theta _n} \) is
	\[
		\mathbb{P} (\epsilon > -\theta _n)
		= \mathbb{P} (\epsilon \leq \theta _n)
		= F(\theta _n)
	\]
	since \(\epsilon \) is symmetric and continuous. Hence, the \hyperref[thm:Lindeberg-CLT]{Lindeberg central limit theorem} gives
	\[
		\frac{\sum_{i=1}^{n} \left( \mathbbm{1}_{\epsilon _i  > -\theta _n} - F(\theta _n) \right) }{\sqrt{n} \sqrt{F(\theta _n) (1 - F(\theta _n))} }
		= \sqrt{n} \frac{\frac{1}{n} \sum_{i=1}^{n} \mathbbm{1}_{\epsilon _i > -\theta _n} - F(\theta _n) }{\sqrt{F(\theta _n) (1 - F(\theta _n)) } }
		= \sqrt{n} \frac{\overline{\mathrm{sign}} _n - F(\theta _n) }{\sqrt{F(\theta _n) (1 - F(\theta _n)) } }
		\overset{D}{\to} \mathcal{N} (0, 1)
	\]
	by checking the \hyperref[col:Lyapunov-CLT]{Lyapunov condition}, indeed, since we have
	\[
		\Var_{}\left[\sum_{i=1}^{n} \mathbbm{1}_{\epsilon _i > -\theta _n} \right]
		= n F(\theta _n) (1 - F(\theta _n))
		\to n F(\theta _0) (1 - F(\theta _0))
		\to \infty.
	\]
	We see that
	\begin{itemize}
		\item \(\mu (\theta ) = F(\theta ) \), hence if \(F\) is differentiable at \(0\) with \(F^{\prime} (0) \eqqcolon f(0)\), then \(\mu ^{\prime} (0) = f(0)\);
		\item \(\sigma (\theta ) = \sqrt{F(\theta )(1 - F(\theta ))} \), so \(\sigma (0) = 1 / 2\) since \(F(0) = 1 / 2\) by the symmetry of \(F\).
	\end{itemize}
	We conclude that the \hyperref[def:slope]{slope} of \(\overline{\mathrm{sign}} _n\) is \(\mu ^{\prime} (0) / \sigma (0) = 2f(0)\).
\end{explanation}

Note that for the sign test, we don't need any moment assumption. However, for \(t\)-test, we need the second moment to exist.

\begin{eg}[\(t\)-test]
	Suppose \(\Var_{}[\epsilon ] = \sigma ^2 < \infty \), then the \hyperref[def:slope]{slope} of the normalized \(t\)-statistics \(\widetilde{T} _n \coloneqq T_n / \sqrt{n}\) is \(1 / \sigma \).
\end{eg}
\begin{explanation}
	Firstly, since \(\widetilde{T} _n \coloneqq T_n / \sqrt{n} = \overline{X} _n / \hat{\sigma} _n\), with
	\[
		\hat{\sigma} _n^2
		= \frac{1}{n} \sum_{i=1}^{n} (X_i - \overline{X} _n)^2
		= \frac{1}{n} \sum_{i=1}^{n} (\epsilon _i - \overline{\epsilon} _n)^2,
	\]
	we have
	\[
		\widetilde{T} _n
		= \frac{\overline{X} _n}{\hat{\sigma} _n}
		= \frac{\theta _n + \overline{\epsilon} _n}{\hat{\sigma} _n}
		= \left( \frac{\theta _n}{\hat{\sigma} _n} - \frac{\theta _n}{\sigma } \right) + \left( \frac{\overline{\epsilon} _n}{\hat{\sigma} _n} + \frac{\theta _n}{\sigma } \right)
		\implies
		\sqrt{n} \left( \widetilde{T} _n - \frac{\theta _n}{\sigma } \right)
		= \sqrt{n} \theta _n \left( \frac{1}{\hat{\sigma} _n} - \frac{1}{\sigma } \right) + \sqrt{n} \frac{\overline{\epsilon} _n}{\hat{\sigma} _n}.
	\]
	Since \(\theta _0 = 0\) and \(\sqrt{n} (\theta _n - \theta _0) = \xi \), we have \(\sqrt{n} \theta _n \to \xi \geq 0\), \(1 / \hat{\sigma} _n - 1 / \sigma \overset{p}{\to} 0\), so the first term goes to \(0\). On the other hand, by the usual \hyperref[thm:CLT]{central limit theorem}, \(\sqrt{n} \overline{\epsilon} _n \overset{D}{\to} \mathcal{N} (0, \sigma ^2)\), hence
	\[
		\sqrt{n} \left( \widetilde{T} _n - \frac{\theta _n}{\sigma } \right)
		= \sqrt{n} \frac{\widetilde{T} _n - \frac{\theta _n}{\sigma }}{1}
		\overset{D}{\to} \mathcal{N} (0, 1).
	\]
	We see that
	\begin{itemize}
		\item \(\mu (\theta ) = \theta / \sigma \), which is clearly differentiable with \(\mu ^{\prime} (\theta ) = 1 / \sigma \);
		\item \(\sigma (\theta ) = 1\), so \(\sigma (0) = 1\).
	\end{itemize}
	We conclude that the \hyperref[def:slope]{slope} of \(\widetilde{T} _n\) is \(\mu ^{\prime} (0) / \sigma (0) = 1 / \sigma \).
\end{explanation}

This gives us a way to compare \(\overline{\mathrm{sign} }_n \) and \(\widetilde{T} _n\), i.e.,
\[
	\operatorname{ARE}(\widetilde{T} _n, \overline{\mathrm{sign}} _n)
	= \left( 2 f(0) \sigma \right) ^2
\]

\begin{prev}
	From \autoref{prop:sample-quantile-ARE}, \(\operatorname{ARE}(\overline{X} _n , \hat{\theta} _{1 / 2}) = (2 f(0) \sigma )^2\), exactly the same!
\end{prev}

Let's see some actual example, where we can borrow from the previous calculation.\footnote{The previous example include \hyperref[eg:ARE-normal]{normal} and \hyperref[eg:ARE-Laplace]{Laplace}.}

\begin{eg}[Gaussian]
	If \(\epsilon \sim \mathcal{N} (\mu , \sigma ^2)\), then \(f(x) = \frac{1}{\sigma \sqrt{2\pi } } e^{- x^2 / 2\sigma ^2}\), so \(f(0) = 1 / \sigma \sqrt{2\pi } \), hence
	\[
		\operatorname{ARE}(\widetilde{T} _n, \overline{\mathrm{sign}} _n)
		= \left( \frac{2\sigma }{\sqrt{2\pi } \sigma } \right) ^2
		= \frac{2}{\pi }
		< 1
	\]
\end{eg}

\begin{eg}[Laplace]
	If \(\epsilon \sim \operatorname{Laplace}(\mu , b) \) with \(\sigma ^2 = 2 b^2\), since \(f(0) = 1 /\sqrt{2} \sigma \), hence
	\[
		\operatorname{ARE}(\widetilde{T} _n, \overline{\mathrm{sign}} _n)
		= 4 \sigma ^2 \frac{1}{2 \sigma ^2}
		= 2
		> 1,
	\]
\end{eg}

\begin{eg}[Uniform]
	If \(\epsilon \sim \mathcal{U} (-c, c)\) for some \(c\) such that \(\Var_{}[\epsilon ] = \sigma ^2\), we have
	\[
		\operatorname{ARE}(\widetilde{T} _n, \overline{\mathrm{sign}} _n)
		= \frac{1}{3}.
	\]
\end{eg}
\begin{explanation}
	We see that \(c\) should be
	\[
		\sigma ^2
		= \frac{(2c)^2}{12}
		= \frac{c^2}{3}
		\implies c = \sqrt{3} \sigma ,
	\]
	so \(f(0) = 1 / 2c = 1 / 2 \sqrt{3} \sigma \). Plugging in \((2 f(0) \sigma )^2\) gives \(1 / 3\).
\end{explanation}

Finally, let's consider the Wilcoxon signed-rank test. Recall what we have shown.

\begin{prev}
	When testing \(X \overset{D}{=} -X\), from \autoref{eq:Wilcoxon-signed-rank-test}, we have
	\[
		\frac{\sqrt{n}}{\binom{n}{2}}  \left( W_n - \sum_{i=1}^{n} \sgn (X_i) \right)
		= 2 \sqrt{n} \left( U_n - \frac{1}{2} \right)
		\overset{D}{\to} \mathcal{N} (0, 1)
	\]
	where \(U_n \coloneqq \frac{1}{\binom{n}{2}} \sum_{i < j} h(X_i, X_j)\) where \(h(x_1, x_2) = \mathbbm{1}_{x_1 + x_2 \geq 0} \).
\end{prev}

In our case, since \(X_i = \theta _n + \epsilon _i\), we have
\[
	U_n
	= \frac{1}{\binom{n}{2}} \sum_{\{ i, j \} \subseteq [n]} \mathbbm{1}_{X_i + X_j > 0}
	= \frac{1}{\binom{n}{2}} \sum_{\{ i, j \} \subseteq [n]} \mathbbm{1}_{\epsilon _i + \epsilon _j > - 2 \theta _n}.
\]

\begin{eg}[Wilcoxon signed-rank test]
	The \hyperref[def:slope]{slope} of the corresponding \(U\)-statistics \(U_n\) of the Wilcoxon signed-rank statistics \(W_n\) is \(2 \sqrt{3} \cdot \int f^2(x) \,\mathrm{d} x\).
\end{eg}
\begin{explanation}
	Let \(h_n(\epsilon _1, \epsilon _2) = \mathbbm{1}_{\epsilon _1 + \epsilon _2 > - 2 \theta _n} \) (note that it depends on \(n\)), then from the theory of \(U\)-statistics,
	\[
		U_n - \mathbb{E}_{}[h_n(\epsilon _1, \epsilon _2)]
		= \frac{2}{n} \sum_{k=1}^{n} \left( \widetilde{h} _n(\epsilon _k) - \mathbb{E}_{}[\widetilde{h} _n(\epsilon _k)] \right) + o_p(1)
	\]
	where the \(2\) comes from \(m\), i.e., the number of arguments in \(h\). Here, we have
	\[
		\widetilde{h} _n(x)
		= \mathbb{E}_{}[h(x, \epsilon )]
		= \mathbb{P} (x + \epsilon > - 2\theta _n)
		= \mathbb{P} (\epsilon > -x - 2\theta _n)
		= \mathbb{P} (\epsilon \leq x + 2 \theta _n)
		= F(x + 2 \theta _n),
	\]
	with \(\epsilon _i\)'s being i.i.d., by multiplying and dividing both sides by \(\sqrt{n} \) and \(\sqrt{\Var_{}[F(\epsilon + 2 \theta _n)] } \),
	\[
		\sqrt{n} \frac{U_n - \mathbb{E}_{}[h(\epsilon _1 , \epsilon _2)]}{\sqrt{\Var_{}[F(\epsilon + 2 \theta _n)] } }
		= \frac{2}{\sqrt{n} } \sum_{k=1}^{n} \frac{\widetilde{h} _n(\epsilon _k) - \mathbb{E}_{}[F(\epsilon + 2 \theta _n)] }{\sqrt{\Var_{}[F(\epsilon + 2 \theta _n)] } } + o_p(1).
	\]
	We finally see that by the usual \hyperref[thm:CLT]{central limit theorem}, as long as \(\Var_{}[F(\epsilon + 2 \theta _n)] > 0\), we have
	\[
		\sqrt{n} \frac{U_n - \mathbb{E}_{}[F(\epsilon + 2 \theta _n)] }{2 \sqrt{\Var_{}[F(\epsilon + 2 \theta _n)] } }
		\overset{D}{\to} \mathcal{N} (0, 1).
	\]
	We see that
	\begin{itemize}
		\item \(\mu (\theta ) = \mathbb{E}_{}[F(\epsilon + 2 \theta )] \), if we assume \(f\) exists, it's just
		      \[
			      \mu (\theta ) = \int F(x + 2 \theta ) F(\mathrm{d} x)
			      = \int F(x + 2 \theta ) f(x) \,\mathrm{d} x.
		      \]
		      If we further assume that we can interchange the derivative and the integral, we have
		      \[
			      \mu ^{\prime} (\theta )
			      = \int 2 f(x + 2\theta ) f(x) \,\mathrm{d} x,
		      \]
		      hence \(\mu ^{\prime} (0) = 2 \int f^2(x) \,\mathrm{d} x\).
		\item \(\sigma (\theta ) = 2 \sqrt{\Var_{}[F(\epsilon + 2 \theta )] } \), hence \(\sigma (0) = 2 \sqrt{\Var_{}[F(\epsilon )]} = 2 \cdot \sqrt{1 / 12} = 1 / \sqrt{3}  \).
	\end{itemize}
	Hence, the \hyperref[def:slope]{slope} of \(U_n\) is \(2 \sqrt{3} \cdot \int f^2(x) \,\mathrm{d} x\).
\end{explanation}

This gives us a way to compare \(U_n\) and \(\widetilde{T} _n\), i.e.,
\[
	\operatorname{ARE}(\widetilde{T} _n, U_n)
	= \frac{2 \sqrt{3} \int f^2(x)\,\mathrm{d} x}{1 / \sigma }
	= 2\sqrt{3} \sigma \int f^2(x) \,\mathrm{d} x.
\]