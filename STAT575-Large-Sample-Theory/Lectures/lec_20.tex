\lecture{20}{2 Apr.\ 9:30}{Comparing Different Tests for Symmetry}
We can finally establish the asymptotic normality of \(U_n\).

\begin{theorem}\label{thm:U-statistic}
	If for every \(r \in [m]\), \(J_{r, m}\) is bounded away from \(0\) and infinity as \(n \to \infty \), then
	\[
		\sqrt{n} (U_n - \mathbb{E}_{}[U_n] )
		= \frac{m}{\sqrt{n} } \sum_{k=1}^{n} (\widetilde{h} (X_k) - \mathbb{E}_{}[U_n] ) + o_p(1).
	\]
	Furthermore, if \(\Var_{}[\widetilde{h} (X)] > 0\), then \(\sqrt{n} (U_n - \mathbb{E}_{}[U_n] ) \overset{D}{\to} \mathcal{N} (0, m^2 \Var_{}[\widetilde{h} (X)] )\).
\end{theorem}
\begin{proof}
	Recall that by \autoref{prop:U-statistic-projection} and \autoref{col:projection}, it suffices to show that \(\Var_{}[U_n] \sim \Var_{}[\widetilde{U} _n] = m^2 / n \cdot \Var_{}[\widetilde{h} (X_i)]\) for all \(n \in \mathbb{N} \). By \autoref{prop:U-statistic-variance}, and the Stirling's approximation,\footnote{Recall that as \(n \to \infty \), for any fixed \(m\), \(\binom{n}{m} \sim n^m / m!\) by the Sterling's approximation.}
	\[
		\Var_{}[U_n]
		= \frac{1}{\binom{n}{m}} \sum_{r=1}^{m} \binom{m}{r} \binom{n-m}{m-r} J_{r, m}
		\sim \frac{\binom{m}{1} \binom{n-m}{m-1}}{\binom{n}{m}} J_{1, m}
		\sim \frac{m^2}{n} J_{1, m}
	\]
	with some algebra (and our assumptions). It remains to show that \(\Var_{}[\widetilde{h} (X)] = J_{1, m}\). Indeed,
	\[
		\begin{split}
			J_{1, m}
			 & = \Cov_{}[(X_1, X_2, \dots , X_m), h(X_1, X_{m+1}, \dots , X_{2m-1})]                                                       \\
			 & = \mathbb{E}_{}[(h(X_1, X_2, \dots , X_m) - \theta ) \cdot (h(X_1, X_{m+1}, \dots , X_{2m-1}) - \theta ) ]                  \\
			 & = \int \mathbb{E}_{}[h(x, X_2, \dots , X_m) - \theta ] \cdot \mathbb{E}_{}[h(x, X_{m+1}, \dots , X_{2m-1})] F(\mathrm{d} x) \\
			 & = \int (\widetilde{h} (x) - \theta ) (\widetilde{h} (x) - \theta ) F(\mathrm{d} x)                                          \\
			 & = \mathbb{E}_{}[(\widetilde{h} (X) - \theta )^2]                                                                            \\
			 & = \Var_{}[\widetilde{h} (X)] ,
		\end{split}
	\]
	which is exactly what we want to show.
\end{proof}

Let's conclude the discussion by looking at our initial motivation.

\begin{eg}[Wilcoxon signed-rank statistic]\label{eg:Wilcoxon-signed-rank-statistic-asymptotic-normality}
	For \(d = 1\), consider \(h(x_1, x_2) = \mathbbm{1}_{x_1 + x_2 > 0} \). Then, \(\theta = \mathbb{E}_{}[h(X_1, X_2)] = \mathbb{P} (X_1 + X_2 > 0)\) and \(\widetilde{h} (x) = \mathbb{P} (x + X > 0) = 1 - F(-x)\). If \(X\) is not trivial, then \(\Var_{}[\widetilde{h} (X)] = \Var_{}[F(-X)] > 0\). For example, under \(H_0 \colon X \overset{D}{=} -X \) and \(X\) is continuous, \(\Var_{}[\widetilde{h} (X)] = \Var_{}[F(X)] = 1 / 12\) as \(F(X) \sim \mathcal{U} (0, 1)\) since \(F\) is assumed to be continuous. Moreover,
	\[
		\theta
		= \mathbb{P} (X_1 > -X_2)
		= \mathbb{E}_{}[\widetilde{h} (X)]
		= \mathbb{E}_{}[1 - F(-X)]
		= 1 - \frac{1}{2}
		= \frac{1}{2}.
	\]
	Therefore, \(\sqrt{n} ( U_n - 1 / 2 ) \overset{D}{\to} \mathcal{N} ( 0, 2^2 / 12 ) = \mathcal{N} ( 0, 1 / 3 ) \) from \autoref{thm:U-statistic}. Finally, recall the \hyperref[eq:Wilcoxon-signed-rank-test]{representation} that the \hyperref[def:Wilcoxon-signed-rank-statistic]{Wilcoxon signed-rank statistic} \(W_n\) admits under \(H_0\), i.e.,
	\[
		\frac{\sqrt{n}}{2 \cdot \binom{n}{2}}  \left( W_n - \sum_{i=1}^{n} \sgn (X_i) \right)
		= \frac{\sqrt{n}}{n(n-1)} \left( W_n - \sum_{i=1}^{n} \sgn (X_i) \right)
		= \sqrt{n} \left( U_n - \frac{1}{2} \right)
		\overset{D}{\to} \mathcal{N} \left( 0, \frac{1}{3} \right),
	\]
	where \(U\) is a \hyperref[def:U-statistic]{\(U\)-statistic} with \(h(x_1, x_2) = \mathbbm{1}_{x_1 + x_2 \geq 0} \), hence the above calculation\footnote{Ignore the difference between \(\mathbbm{1}_{x_1 + x_2 \geq 0} \) for \(W_n\) and \(\mathbbm{1}_{x_1 + x_2 > 0} \) in the above example as now \(X\) is continuous.} applies, giving the asymptotic normality. This further implies that under \(H_0\), \(W_n / n^{3 / 2} \overset{D}{\to} \mathcal{N} (0, 1 / 3)\).\footnote{Since from the \hyperref[not:Wilcoxon-signed-rank-test-alone]{previous note}, the sum of \(\sgn (X_i)\)'s will vanish.}
\end{eg}

\begin{note}
	This is the same guarantee when we condition on \(\lvert X_j \rvert \)'s as seen in the \hyperref[eg:Wilcoxon-signed-rank-statistic-two-sample-probelm]{previous example}.
\end{note}

We make one last remark before we move on to the next topic.

\begin{remark}
	\(U_n\) is a function of \(X_{(1)}, \dots , X_{(n)}\), i.e., the function of the order statistics, which is an UMVUE since in this non-parametric formulation,\footnote{Since we didn't assume anything about \(F\) excepts for the continuity.} the order statistic is complete and sufficient.
\end{remark}

\section{Asymptotic Relative Efficiency of Tests}
For the \hyperref[prb:testing-symmetry]{testing for symmetry problem}, we have three test statistics, i.e., \hyperref[def:t-statistic]{ \(t\)-statistic} \(T_n = \sqrt{n} \overline{X} _n / \hat{\sigma} _n\), the \hyperref[def:sign-statistic]{sign statistic} \(\mathrm{sign} _n = \sum_{i=1}^{n} \sgn (X_i) \), and the \hyperref[def:Wilcoxon-signed-rank-statistic]{Wilcoxon signed-rank statistic} \(W_n = \sum_{i=1}^{n} R_i \sgn (X_i)\).

\begin{problem*}
	How does \(T_n\), \(\mathrm{sign} _n\), and \(W_n\) compared in terms of their efficiency? Can we say something similar for the estimator's efficiency we have developed, i.e., the \hyperref[def:asymptotic-relative-efficiency-estimator]{asymptotic relative efficiency}?
\end{problem*}

To answer the above question, we'll look into their \emph{powers}. Consider an alternative hypothesis testing, where we have \(H_0 \colon \theta = \theta _0\) and \(H_1 \colon \theta > \theta _0\).

\begin{eg}
	One example of \(\theta \) is \(\theta = \mathbb{P} (X_1 + X_2 > 0)\).
\end{eg}

Let \(T_n\) now denote a generic statistic (not necessarily the \hyperref[def:t-statistic]{\(t\)-statistic}) such that there exists an increasing \(\mu (\theta )\) such that when the true parameter is \(\theta \),
\[
	\frac{T_n - \mu (\theta )}{\sigma (\theta ) / \sqrt{n} }
	\overset{D}{\to} \mathcal{N} (0, 1).
\]
Then, we reject \(H_0\) whenever \(T_n\) is large.

\begin{eg}
	One can write all the \hyperref[def:t-statistic]{\(t\)-statistic} ``\(T_n\)'', the sign test statistic \(\mathrm{sign} _n\), and also the \hyperref[def:Wilcoxon-signed-rank-statistic]{Wilcoxon signed-rank statistic} \(W_n\) in this form.
\end{eg}

Specifically, under this setup, by treating the asymptotic normality as exact, we reject \(H_0\) if
\[
	T_n \geq \mu (\theta _0) + Z_\alpha \frac{\sigma (\theta _0)}{\sqrt{n} },
\]
which gives that \(\mathbb{P} _{\theta _0}(\text{reject} ) \to \alpha \) as we usually get.

\begin{note}
	It's easy to see that we can always control the type-I error.
\end{note}

Since we can always control \(\alpha \), the interesting question might be whether we can control the type-II error \(\beta \). We see that under \(H_1\),
\[
	\mathbb{P} _{\theta }(\text{reject} )
	= \mathbb{P} _{\theta }\left( \frac{T_n - \mu (\theta )}{\sigma (\theta ) / \sqrt{n} } \geq \frac{\mu (\theta _0) - \mu (\theta )}{\sigma (\theta ) / \sqrt{n} } + Z_\alpha \frac{\sigma (\theta _0)}{\sigma (\theta )} \right)
	\overset{\theta > \theta _0}{\to } 1
\]
as \(n \to \infty \) since \(\mu (\theta _0) - \mu (\theta ) < 0\).

\begin{remark}
	The power always approaches \(1\), not very interesting.
\end{remark}

Hence, we might turn our focus to some non-asymptotic results. One way to look at this problem is by fixing the type-I and type-II error, and see how many samples we need to achieve them.

\subsection{A Heuristic Approach}
Given some fixed \(\alpha \), \(\beta \), and \(\theta _0\), suppose we want \(\mathbb{P} _{\theta ^{\ast} }(\text{reject} ) = 1 - \beta \) for some \(\theta ^{\ast} \) with some \(n\). From the above calculation with asymptotic normality we assume for \(T_n\), we have
\[
	1 - \beta
	= \mathbb{P} _{\theta ^{\ast} }\left( \frac{T_n - \mu (\theta ^{\ast} )}{\sigma (\theta ^{\ast} ) / \sqrt{n} } \geq \frac{\mu (\theta _0) - \mu (\theta ^{\ast} )}{\sigma (\theta ^{\ast} ) / \sqrt{n} } + Z_\alpha \frac{\sigma (\theta _0)}{\sigma (\theta ^{\ast} )} \right)
	\to 1 - \Phi \left( \frac{\mu (\theta _0) - \mu (\theta ^{\ast} )}{\sigma (\theta ^{\ast} ) / \sqrt{n} } + Z_\alpha \frac{\sigma (\theta _0)}{\sigma (\theta ^{\ast} )} \right) ,
\]
where the convergent is not rigorous since the right-hand side still depend on \(n\). Anyway, this leads to\footnote{Recall that \(Z_\alpha \) is defined for the right-tail.}
\[
	Z_{1 - \beta }
	= - Z_{\beta }
	= \frac{\mu (\theta _0) - \mu (\theta ^{\ast} )}{\sigma (\theta ^{\ast} ) / \sqrt{n} } + Z_\alpha \frac{\sigma (\theta _0)}{\sigma (\theta ^{\ast} )}
	\implies \sqrt{n} \frac{\mu (\theta ^{\ast} ) - \mu (\theta _0)}{\sigma (\theta ^{\ast} )}
	= Z_\beta + \frac{\sigma (\theta _0)}{\sigma (\theta ^{\ast} )} Z_\alpha .
\]
Let \(\theta _i\) be the \(\theta ^{\ast} \) for which the above holds when \(n = i\), and denote the corresponding sequence as \((\theta _n)\).

\begin{note}
	Obviously we then have \(\theta _n \to \theta _0\) for \((\theta _n)\).
\end{note}

In this case, by replacing \(\theta ^{\ast} \) by \(\theta _n\) for every \(n \in \mathbb{N} \), we have
\[
	\sqrt{n} \frac{\mu (\theta _n) - \mu (\theta _0)}{\sigma (\theta _n)}
	= Z_\beta + \frac{\sigma (\theta _0)}{\sigma (\theta _n)} Z_\alpha.
\]
If \(\sigma \) is continuous at \(\theta _0\), then as \(\sigma (\theta _n) \to \sigma (\theta _0)\), the right-hand side becomes \(Z_\alpha + Z_\beta \). If we further assume that \(\mu \) is differentiable at \(\theta _0\), with \(\sqrt{n} (\mu (\theta _n) - \mu (\theta _0)) = \mu ^{\prime} (\theta _0) \sqrt{n} (\theta _n - \theta _0) + \sqrt{n} \cdot o(\theta _n - \theta _0)\),
\[
	\sqrt{n} (\theta _n - \theta _0)
	\to \frac{Z_\alpha + Z_\beta }{\mu ^{\prime} (\theta _0) / \sigma (\theta _0)}.
\]
Let \(n^{\ast} \) be the \(n\) such that \(\theta _{n^{\ast} } = \theta _n = \theta ^{\ast} \). Then, we have
\[
	\sqrt{n^{\ast} }
	\to \frac{Z_\alpha + Z_\beta }{\frac{\mu ^{\prime} (\theta _0)}{\sigma (\theta _0)} (\theta _{n^{\ast} } - \theta _0)}.
\]

\begin{note}
	\(n^{\ast} \) only depends on \(\mu ^{\prime} (\theta _0) / \sigma (\theta _0)\).
\end{note}
\begin{explanation}
	Since \(Z_\alpha + Z_\beta \) is fixed while \(\theta _{n^{\ast} } - \theta _0\) is assumed to be independent of the statistic also since we treat their asymptotic normality as exact, so \(\theta _{n^{\ast} }\) will be the same across different statistics.
\end{explanation}

\begin{definition}[Slope]\label{def:slope}
	For any statistics \(T_n\) with \(\mu \) and \(\sigma \), its \emph{slope} is defined as  \(\mu ^{\prime} (\theta _0) / \sigma (\theta _0)\).
\end{definition}

We see that if the analysis can be made formal, then we can compare two statistics \(T_n\) and \(\widetilde{T} _n\) in terms of their required sample sizes to achieve \(\alpha \) and \(\beta \) for a fixed \(\theta _0\).

\begin{remark}
	This analysis relies on the fact that when \(\sqrt{n} (\theta _n - \theta _0)\) converges, then
	\[
		\frac{T_n - \mu (\theta _n)}{\sigma (\theta _n) / \sqrt{n} } \overset{D}{\to} \mathcal{N} (0, 1).
	\]
\end{remark}