\lecture{18}{26 Mar.\ 9:30}{Two-Sample Problem and Wilcoxon Signed-Rank Test}
As we have seen, we might just consider \(X, X_1, \dots , X_n \overset{\text{i.i.d.} }{\sim } F \) where \(F\) is continuous, and we want to test \(H_0 \colon \mathbb{E}_{}[X] = 0\). If, also, \(\Var_{}[X] < \infty \), then we reject \(H_0\) if \(T_n \geq Z_\alpha \) where
\[
	T_n
	\coloneqq \sqrt{n} \frac{\overline{X} _n - 0}{\hat{\sigma} _n}
	= \frac{\sum_{i=1}^{n} X_i}{\sqrt{\sum_{i=1}^{n} (X_i - \overline{X} _n)^2} }.
\]
Since \(\Var_{}[X] < \infty \), \(T_n \overset{D}{\to} \mathcal{N} (0, 1)\) under \(H_0\).

\begin{problem*}
	What if \(\Var_{}[X] \) doesn't exist? Moreover, what if \(\mathbb{E}_{}[X] \) doesn't exist?
\end{problem*}

Another question is that since this is all asymptotic, and we might wonder what's the rate of convergence.

\begin{problem*}
	How fast does \(T_n \overset{D}{\to} \mathcal{N} (0, 1)\) under \(H_0\)?
\end{problem*}
\begin{answer}
	Depends on \(F\).
\end{answer}

\begin{problem*}
	What if \(X_1, \dots , X_n\) are not i.i.d.?
\end{problem*}
\begin{answer}
	Consider the following \(H_0\) such that for every \(1 \leq i \leq n\),
	\[
		\mathbb{P} (X_i = \pm \vert X_i \vert \mid \vert X_j \vert , 1 \leq j \leq n) = \frac{1}{2},
	\]
	i.e., \(\mathbb{P} (\sgn (X_i) = \pm 1 \mid \vert X_j \vert , 1 \leq j \leq n) = 1 / 2\). Hence, the inference conditionally on \(\vert X_j \vert \)'s. In this case, we have \(\sum_{i=1}^{n} X_i = \sum_{i=1}^{n} \vert X_i \vert \sgn (X_i)\), and by treating \(\vert X_i \vert \)'s as constants, then by
	\[
		\frac{\sum_{i=1}^{n} \vert X_i \vert \sgn (X_i)}{\sqrt{\sum_{i=1}^{n} X_i^2} }
		\overset{D}{\to} \mathcal{N} (0, 1)
	\]
	under \(H_0\) by the \hyperref[thm:Lindeberg-CLT]{Lindeberg central limit theorem}. Alternatively, reject if \(\sum_{i=1}^{n} \sgn (X_i) \) is large. We see that
	\[
		\sum_{i=1}^{n} \sgn (X_i)
		= 2 \sum_{i=1}^{n} \mathbbm{1}_{X_i > 0} - n,
	\]
	where \(\sum_{i=1}^{n} \mathbbm{1}_{X_i > 0} \overset{H_0}{\sim} \operatorname{Bin}(n, 1 / 2)\). On the other hand,
	\[
		\frac{\sum_{i=1}^{n} \sgn (X_i)}{\sqrt{n} } \overset{D}{\to} \mathcal{N} (0, 1)
	\]
	under \(H_0\), so we can also reject when \(\sum_{i=1}^{n} \sgn (x_i) / \sqrt{n} \geq Z_\alpha \). Consider the rank \(R_i\) for \(1 \leq i \leq n\) is defined as
	\[
		R_i = \sum_{j=1}^{n} \mathbbm{1}_{\vert X_j \vert \leq \vert X_i \vert }.
	\]
	It's clear that \(R_i\)'s are now dependent.

	\begin{prev}
		This is different from the previous definition.
	\end{prev}

	\begin{intuition}
		Replace \(\vert X_i \vert \) by \(R_i\).
	\end{intuition}

	Consider reject \(H_0\) if
	\[
		W_n = \sum_{i=1}^{n} R_i \sgn (X_i)
	\]
	is large. This is the so-called \emph{Wilcoxon signed-rank test}. To figure out the critical value, since we condition on \(\{ \vert X_j \vert \}_{j = 1}^n \), without loss of generality, \(R_i = i\) for every \(1 \leq i \leq n\).\footnote{This is doable since \(\vert X_j \vert \)'s are treated as constants.} In this case, \(W_n = \sum_{i=1}^{n} i \sgn (X_i)\). By the \hyperref[thm:Lindeberg-CLT]{Lindeberg central limit theorem},
	\[
		\frac{\sum_{i=1}^{n} i \sgn (X_i)}{\sqrt{\sum_{i=1}^{n} i^2} }
		\overset{D}{\to} \mathcal{N} (0, 1)
	\]
	under \(H_0\). This is independent of \(F\).
\end{answer}

Now, if the data is indeed i.i.d., consider \(H_0 \colon \sgn (X_i) = \epsilon _i\) where \(\epsilon _i\) is a Rademacher random variable, i.e., \(\mathbb{P} (X > 0) = \mathbb{P} (X < 0) = 1 / 2\). This doesn't even need the assumption that \(\mathbb{E}_{}[X] \) exists.

If we now have \(\Var_{}[X] < \infty \), then the Wilcoxon signed-rank test still makes sense. But since we're not conditioning on \(\vert X_j \vert \)'s we only have
\[
	W_n = \sum_{i=1}^{n} R_i \sgn (X_i).
\]
To understand this, consider \(W_n = W_n^+ - W_n^-\) where
\[
	W_n^+ = \sum_{i=1}^{n} R_i \mathbbm{1}_{X_i > 0} ,\text{ and }
	W_n^- = \sum_{i=1}^{n} R_i \mathbbm{1}_{X_i < 0}.
\]
We see that
\[
	W_n^+
	= \sum_{i=1}^{n} \left( \sum_{j=1}^{n} \mathbbm{1}_{\vert X_j \vert \leq \vert X_i \vert } \right) \mathbbm{1}_{X_i > 0}
	= \sum_{i=1}^{n} \mathbbm{1}_{X_i > 0} + \sum_{\substack{1 \leq i, j \leq n \\ i \neq j}} \mathbbm{1}_{\vert X_j \vert \leq \vert X_i \vert , X_i > 0},
\]
hence
\[
	\begin{split}
		W_n^+ - \sum_{i=1}^{n} \mathbbm{1}_{X_i > 0}
		 & = \sum_{i \neq j} \mathbbm{1}_{\vert X_j \vert \leq X_i}                                                      \\
		 & = \sum_{i \neq j} \mathbbm{1}_{-X_i \leq X_j \leq X_i}                                                        \\
		 & = \sum_{i \neq j} \mathbbm{1}_{X_i + X_j \geq 0} \mathbbm{1}_{X_j \leq X_i}                                   \\
		 & = \sum_{i \neq j} \mathbbm{1}_{X_i + X_j \geq 0} - \sum_{i \neq j} \mathbbm{1}_{X_i + X_j \geq 0, X_j > X_i},
	\end{split}
\]
and hence
\[
	W_n^+ - \sum_{i=1}^{n} \mathbbm{1}_{X_i > 0}
	= \frac{1}{2} \sum_{i \neq j} \mathbbm{1}_{X_i + X_j \geq 0}.
\]
Similarly,
\[
	W_n^- - \sum_{i=1}^{n} \mathbbm{1}_{X_i < 0}
	= \frac{1}{2} \sum_{i \neq j} \mathbbm{1}_{X_i + X_j < 0} .
\]
Hence, by subtracting, since \(\mathbbm{1}_{X_i > 0} - \mathbbm{1}_{X_i < 0} = \sgn (X_i) \), we have
\[
	W_n - \sum_{i=1}^{n} \sgn (X_i)
	= \frac{1}{2} \sum_{i \neq j} \left( \mathbbm{1}_{X_i + X_j \geq 0} - \mathbbm{1}_{X_i + X_j < 0} \right)
	= \frac{1}{2} \sum_{i \neq j} \left( 2 \mathbbm{1}_{X_i + X_j \geq 0} - 1 \right)
	= \sum_{i \neq j} \left( \mathbbm{1}_{X_i + X_j \geq 0} - \frac{1}{2} \right).
\]

\begin{notation}
	Let \(h(x_1, x_2) \coloneqq \mathbbm{1}_{x_1 + x_2 \geq 0} \).
\end{notation}

Hence,
\[
	W_n - \sum_{i=1}^{n} \sgn (X_i)
	= \sum_{i \neq j} \left( h(X_i , X_j) - \frac{1}{2} \right).
\]
To understand \(\sum_{i \neq j} h(X_i, X_j)\), let \(H_0 \colon X, X_1, \dots , X_n \overset{\text{i.i.d.} }{\sim } F\) where \(F\) is continuous, such that \(\mathbb{P} (X \geq x) = \mathbb{P} (X \leq -x)\) for all \(x \in \mathbb{R} \), i.e., \(X \overset{D}{=} -X\). Observe that we don't need \(\mathbb{E}_{}[X] \) to exists. Under \(H_0\), we see that for \(i \neq j\),
\[
	\begin{split}
		\mathbb{E}_{H_0}[h(X_i, X_j)]
		= \mathbb{E}_{H_0}[h(X_1, X_2)]
		 & = \mathbb{P} (X_1 + X_2 \geq 0)                             \\
		 & = \mathbb{P} (X_1 \geq -X_2)                                \\
		 & = \int _\mathbb{R} \mathbb{P} (X_1 \geq -x) F(\mathrm{d} x)
		= \int _\mathbb{R} \mathbb{P} (X \leq x) F(\mathrm{d} x)
		= \mathbb{E}_{}[F(X)]
		= \frac{1}{2}
	\end{split}
\]
since \(F\) is continuous, and \(F(X) \sim \mathcal{U} (0, 1)\) as we have shown in the homework. Hence,
\[
	\sum_{i \neq j} \left( h(X_i, X_j) - \frac{1}{2} \right)
	= 2 \sum_{i < j} \left( h(X_i, X_j) - \frac{1}{2} \right)
	= 2 \sum_{i < j} \left( h(X_i, X_j) - \mathbb{E}_{}[h(X_i, X_j)] \right),
\]
and we know that under \(H_0\), \(h(X_i, X_j)\) are identically distributed but not independent. Overall,
\[
	\frac{1}{\binom{n}{2}} \left( W_n - \sum_{i=1}^{n} \sgn (X_i) \right)
	= \frac{2}{\binom{n}{2}} \sum_{i < j} \left( h(X_i, X_j) - \frac{1}{2} \right)
	= 2 \left( U_n - \frac{1}{2} \right)
\]
where
\[
	U_n \coloneqq \frac{1}{\binom{n}{2}} \sum_{i < j} h(X_i, X_j).
\]
This is an \(U\)-statistic, where \(U\) stands for unbiased. Indeed, since \(\mathbb{E}_{}[h(X_1, X_2)] = \mathbb{P} (X_1 + X_2 \geq 0)\). We will show that by multiplying \(\sqrt{n} \) on the both sides, this \hyperref[def:converge-in-distribution]{converges} to a standard normal, i.e.,
\[
	\frac{\sqrt{n}}{\binom{n}{2}}  \left( W_n - \sum_{i=1}^{n} \sgn (X_i) \right)
	= \sqrt{n} 2 \left( U_n - \frac{1}{2} \right)
	\overset{D}{\to} \mathcal{N} (0, 1).
\]
Note that is we have this,
\[
	2 \frac{\sqrt{n} }{n(n-1)} \sum_{i=1}^{n} \sgn (X_i)
	= \frac{2}{n-1} \left( \frac{1}{\sqrt{n} } \sum_{i=1}^{n} \sgn (X_i) \right)
	\overset{p}{\to} 0,
\]
hence we will get an asymptotic distribution for \(W_n\) exactly.

In general, to show
\[
	\frac{S_n - \mathbb{E}_{}[S_n] }{\sqrt{\Var_{}[S_n] } }
	\overset{D}{\to} Y,
\]
we find \((\widetilde{S} _n)\) such that
\[
	\frac{\widetilde{S} _n - \mathbb{E}_{}[\widetilde{S} _n] }{\sqrt{\Var_{}[\widetilde{S} _n] } }
	\overset{D}{\to} Y
\]
such that
\[
	\left\vert \frac{S_n - \mathbb{E}_{}[S_n] }{\sqrt{\Var_{}[S_n] } } - \frac{\widetilde{S} _n - \mathbb{E}_{}[\widetilde{S} _n] }{\sqrt{\Var_{}[S_n] } }\right\vert
	\overset{p}{\to} 0.
\]

Recall the following.

\begin{proposition}
	If \(\Var_{}[S_n]\sim \Var_{}[\widetilde{S} _n] \) and \(\Corr_{}(S_n, \widetilde{S} _n) \to 1\), then
	\[
		\left\vert \frac{S_n - \mathbb{E}_{}[S_n] }{\sqrt{\Var_{}[S_n] } } - \frac{\widetilde{S} _n - \mathbb{E}_{}[\widetilde{S} _n] }{\sqrt{\Var_{}[S_n] } } \right\vert
		\overset{p}{\to} 0.
	\]
\end{proposition}