\lecture{18}{26 Mar.\ 9:30}{Wilcoxon Signed-Rank Test}
Before we explain the above answer, let's point out other potential problems: firstly, since even for i.i.d.\ \(X_i\)'s, the results for \(T_n\) is asymptotic, and we might wonder what's the rate of convergence.

\begin{problem*}
	How fast does \(T_n \overset{D}{\to} \mathcal{N} (0, 1)\) under \(H_0\)?
\end{problem*}
\begin{answer}
	It depends heavily on \(F\), so it's unlikely to get a universal answer.
\end{answer}

On the other hand, what if the underlying distribution has heavy tails?

\begin{problem*}
	What if \(\Var_{}[X_i] \), or even \(\mathbb{E}_{}[X_i] \), doesn't exist?
\end{problem*}
\begin{answer}
	This is something we will solve along the way when we deal with the i.i.d.\ problem.
\end{answer}

Now, we focus on the non-i.i.d.\ problem. It turns out that if each unit in each pair is equally likely to be in the treatment and the control group, independently of what happens in the other pairs, then \(H_0 \colon \mu = 0\) can be expressed as for all \(1 \leq i \leq n\), \(\mathbb{P} (X_i = \pm \vert X_i \vert \mid \vert X_j \vert = \lvert x_j \rvert , 1 \leq j \leq n) = 1 / 2\), i.e.,
\[
	H_0 \colon
	\mathbb{P} (\sgn (X_i) = \pm 1 \mid \vert X_j \vert = \lvert x_j \rvert , 1 \leq j \leq n)
	= \frac{1}{2} \text{ for all } 1 \leq i \leq n.
\]
Hence, we can test this ``conditionally on the observed absolute values of \(X_1, \dots , X_n\),'' i.e., given \(\lvert X_j \rvert = \lvert x_j \rvert \) for \(1 \leq j \leq n\). This is exactly the problem of \hyperref[prb:testing-symmetry]{testing for symmetry}.

\begin{intuition}
	We see that now under \(H_0\), \(\sgn (X_i)\)'s are i.i.d., overcoming the problem of non-i.i.d.
\end{intuition}

\begin{note}
	In practice, when doing inference, we usually implicitly condition on \(\vert X_j \vert \)'s too: by doing inference on the selected ``design points'', we're essentially condition on those.
\end{note}

\begin{eg}[\(t\)-statistic for two-sample problem]\label{eg:two-sample-t-statistic}
	Consider writing \(\sum_{i=1}^{n} X_i = \sum_{i=1}^{n} \vert X_i \vert \sgn (X_i) = \sum_{i=1}^{n} \lvert x_i \rvert \sgn (X_i)\). Then by treating \(\vert x_i \vert \)'s as constants, under \(H_0\),
	\begin{equation}\label{eq:two-sample}
		T_n
		= \frac{\sum_{i=1}^{n} X_i}{\sqrt{\sum_{i=1}^{n} x_i^2} } \sqrt{\frac{\sum_{i=1}^{n} x_i^2}{\sum_{i=1}^{n} (x_i - \overline{x} _n)^2}}
		= \frac{\sum_{i=1}^{n} \vert X_i \vert \sgn (X_i)}{\sqrt{\sum_{i=1}^{n} x_i^2} }\sqrt{\frac{\sum_{i=1}^{n} x_i^2}{\sum_{i=1}^{n} (x_i - \overline{x} _n)^2}}
		\overset{D}{\to} \mathcal{N} (0, 1)
	\end{equation}
	by the \hyperref[thm:Hajek-Sidak-CLT]{HÃ¡jek-Sidak central limit theorem} as long as \(\max _{1 \leq i \leq n} x_i^2 / \sum_{j=1}^{n} x_j^2 \to 0\). We can then use the left-hand side for out test as usual.
\end{eg}

Let's use this example as our running example for the problem of \hyperref[prb:testing-symmetry]{testing for symmetry}.

\subsection{Student's \(t\)-Test, Sign Test, and Wilcoxon Signed-Rank Test}
Returning to the general problem of \hyperref[prb:testing-symmetry]{testing for symmetry}, let \(X, X_1, \dots , X_n \overset{\text{i.i.d.} }{\sim } F\).

\begin{prev}
	Now \(H_0\) is defined to be whether \(F\) is symmetric about \(0\), or \(X \overset{D}{=} -X\).
\end{prev}

As suggested by the \hyperref[eg:two-sample-t-statistic]{running example}, one standard way to solve this is still considering the \hyperref[def:t-statistic]{\(t\)-statistic},
\[
	T_n
	= \frac{\sum_{i=1}^{n} X_i}{\sqrt{\sum_{i=1}^{n} (X_i - \overline{X} _n)^2} },
\]
and reject \(H_0\) when \(T_n\) is larger than some critical value.

\begin{prev}
	If \(X\) has positive and finite variance, \(\mathbb{P} _{H_0}(T_n > Z_\alpha ) \to \alpha \) as \(n \to \infty \). However, as we have seen, the rate of such an approximation depends on the unknown \(F\).
\end{prev}

An alternative and even more classical way to solve this, which works even if \(X\) does not have any moments, is to consider the \hyperref[def:sign-statistic]{sign statistic}.

\begin{definition}[Sign statistic]\label{def:sign-statistic}
	Given a sample \(X_1, \dots , X_n\), the \emph{sign statistic} is defined as
	\[
		\mathrm{sign} _n
		\coloneqq \sum_{i=1}^{n} \sgn (X_i)
		= 2 \sum_{i=1}^{n} \mathbbm{1}_{X_i > 0} - n .
	\]
\end{definition}

Observe that under \(H_0\), the distribution of \(\mathrm{sign} _n\) is independent of \(F\) since \(\sum_{i=1}^{n} \mathbbm{1}_{X_i > 0} \overset{H_0}{\sim} \operatorname{Bin}(n, 1 / 2)\), i.e., we can conduct the hypothesis test non-asymptotically. On the other hand, under \(H_0\), even if we consider its asymptotic behavior, i.e., from the usual \hyperref[thm:CLT]{central limit theorem},
\[
	\frac{1}{\sqrt{n} } \mathrm{sign} _n
	= \frac{\sum_{i=1}^{n} \sgn (X_i)}{\sqrt{n} }
	\overset{D}{\to} \mathcal{N} (0, 1),
\]
the quality of the approximation doesn't depend on \(F\), and we reject \(H_0\) when \(\sum_{i=1}^{n} \sgn (x_i) / \sqrt{n} \geq Z_\alpha \).

\begin{eg}[Sign-statistic for two-sample problem]
	One can also motivate the \hyperref[def:sign-statistic]{sign statistic} from the use of \hyperref[def:t-statistic]{\(t\)-statistic} \(T_n\) in our \hyperref[eg:two-sample-t-statistic]{running example}, i.e., by simply ignoring \(\lvert X_i \rvert \)'s used in \(T_n\) to avoid conditioned on them. Hence, we reject \(H_0\) if \(\sum_{i=1}^{n} \sgn (X_i) = \mathrm{sign} _n\) is large.
\end{eg}

The sign test with the \hyperref[def:sign-statistic]{sign statistic} is simple, but it's questionable that how powerful it is as we're not using \(\vert X_i \vert \)'s at all. In view of the above intuition, let's utilize \(\lvert X_i \rvert \)'s in other ways.

\begin{intuition}
	We can utilize \(\vert X_i \vert \)'s by replacing \(\lvert X_i \rvert \)'s in \autoref{eq:two-sample} by their ``rank'' information.
\end{intuition}

In particular, consider the \emph{rank} \(R_i\) for \(1 \leq i \leq n\)
\[
	R_i
	\coloneqq \sum_{j=1}^{n} \mathbbm{1}_{\vert X_j \vert \leq \vert X_i \vert }.
\]
It's clear that \(R_i\)'s are dependent to each other.

\begin{prev}
	This is different from the previous definition \(R_i = \sum_{j=1}^{i} \mathbbm{1}_{X_j \leq X_i} \).
\end{prev}

Following the intuition, consider the so-called \hyperref[def:Wilcoxon-signed-rank-statistic]{Wilcoxon signed-rank statistic}.

\begin{definition}[Wilcoxon signed-rank statistic]\label{def:Wilcoxon-signed-rank-statistic}
	Given a sample \(X_1, \dots , X_n\), the \emph{Wilcoxon signed-rank statistic} is defined as
	\[
		W_n = \sum_{i=1}^{n} R_i \sgn (X_i).
	\]
\end{definition}

Then, the corresponding Wilcoxon signed-rank test is to reject \(H_0\) if \(W_n\) is ``large'' as usual.

\begin{note}
	Under \(H_0\), \(W_n\) is again independent of \(F\).
\end{note}

However, it turns out that it's quite hard to get the exact critical value for a general problem of \hyperref[prb:testing-symmetry]{testing for symmetry}.

\begin{eg}[Wilcoxon signed-rank statistic for two-sample probelm]\label{eg:Wilcoxon-signed-rank-statistic-two-sample-probelm}
	If we condition on \(\vert X_j \vert \)'s as in the \hyperref[eg:two-sample-t-statistic]{running example}, without loss of generality, \(R_i = i\) for every \(1 \leq i \leq n\).\footnote{This is doable since \(\vert X_j \vert \)'s are treated as constants.} In this case, \(W_n = \sum_{i=1}^{n} i \sgn (X_i)\), hence under \(H_0\),
	\[
		\frac{\sum_{i=1}^{n} i \sgn (X_i)}{\sqrt{\sum_{i=1}^{n} i^2} }
		\overset{D}{\to} \mathcal{N} (0, 1)
		\implies \frac{W_n}{\sqrt{n^3} }
		\overset{D}{\to} \mathcal{N} (0, 1 / 3)
	\]
	by the \hyperref[thm:Lindeberg-CLT]{Lindeberg central limit theorem}. We can then use this to test \(H_0\).
\end{eg}

Without conditioning on \(\vert X_j \vert \)'s, the above doesn't hold. However, some sign tests are still possible.

\begin{intuition}[Another sign test]
	Consider \(H_0 \colon \sgn (X_i) = \epsilon _i\) where \(\epsilon _i\) is a Rademacher random variable, i.e., \(\mathbb{P} (X > 0) = \mathbb{P} (X < 0) = 1 / 2\) if \(X\) is symmetric. This doesn't need to assume \(\mathbb{E}_{}[X_i] < \infty \).
\end{intuition}

If we further assume \(\Var_{}[X_i] < \infty \), then it's possible to say something about \(W_n\) without conditioning on \(\lvert X_j \rvert \)'s. Let's first see a useful characterization of \(W_n\) to get started.

\begin{proposition}\label{prop:Wilcoxon-signed-rank-test}
	Let \(h(x_1, x_2) \coloneqq \mathbbm{1}_{x_1 + x_2 \geq 0}\). Then we have
	\[
		W_n
		= \sum_{i=1}^{n} \sgn (X_i) + \sum_{i \neq j} \left( h(X_i , X_j) - \frac{1}{2} \right).
	\]
\end{proposition}
\begin{proof}
	Consider \(W_n \eqqcolon W_n^+ - W_n^-\) where
	\[
		W_n^+ = \sum_{i=1}^{n} R_i \mathbbm{1}_{X_i > 0} ,\text{ and }
		W_n^- = \sum_{i=1}^{n} R_i \mathbbm{1}_{X_i < 0}.
	\]
	Some calculation gives the following.

	\begin{claim}
		\(W_n^+ - \sum_{i=1}^{n} \mathbbm{1}_{X_i > 0} = \frac{1}{2} \sum_{i \neq j} \mathbbm{1}_{X_i + X_j \geq 0}\) and \(W_n^- - \sum_{i=1}^{n} \mathbbm{1}_{X_i < 0} = \frac{1}{2} \sum_{i \neq j} \mathbbm{1}_{X_i + X_j < 0}\).
	\end{claim}
	\begin{explanation}
		Let's first focus on \(W_n^+\). We see that
		\[
			W_n^+
			= \sum_{i=1}^{n} \left( \sum_{j=1}^{n} \mathbbm{1}_{\vert X_j \vert \leq \vert X_i \vert } \right) \mathbbm{1}_{X_i > 0}
			= \sum_{i=1}^{n} \mathbbm{1}_{X_i > 0} + \sum_{\substack{1 \leq i, j \leq n \\ i \neq j}} \mathbbm{1}_{\vert X_j \vert \leq \vert X_i \vert , X_i > 0} ,
		\]
		Let's abbreviate the argument \(1 \leq i, j \leq n \colon i \neq j\) in the double summation by \(i \neq j\), we have
		\[
			\begin{split}
				W_n^+ - \sum_{i=1}^{n} \mathbbm{1}_{X_i > 0}
				 & = \sum_{i \neq j} \mathbbm{1}_{\vert X_j \vert \leq X_i}                                       \\
				 & = \sum_{i \neq j} \mathbbm{1}_{-X_i \leq X_j \leq X_i}                                         \\
				 & = \underbrace{\sum_{i \neq j} \mathbbm{1}_{X_i + X_j \geq 0} \mathbbm{1}_{X_j \leq X_i}}_{S_1}
				= \sum_{i \neq j} \mathbbm{1}_{X_i + X_j \geq 0} - \underbrace{\sum_{i \neq j} \mathbbm{1}_{X_i + X_j \geq 0} \mathbbm{1}_{X_j > X_i}}_{S_2},
			\end{split}
		\]
		where the last equality can be justified by the fact that \(1 = \mathbbm{1}_{X_j \leq X_i} + \mathbbm{1}_{X_j > X_i} \). Observe that
		\[
			S_1
			= \sum_{i \neq j} \mathbbm{1}_{X_i + X_j \geq 0} \mathbbm{1}_{X_j \leq X_i}
			= \sum_{i \neq j} \mathbbm{1}_{X_j + X_i \geq 0} \mathbbm{1}_{X_i \leq X_j}
			= \sum_{i \neq j} \mathbbm{1}_{X_i + X_j \geq 0} \mathbbm{1}_{X_j > X_i}
			= S_2,
		\]
		since \(F\) is continuous. Hence, by letting \(S \coloneqq S_1 = S_2\), the above calculation gives
		\[
			W_n^+ - \sum_{i=1}^{n} \mathbbm{1}_{X_i > 0}
			= S
			= \sum_{i \neq j} \mathbbm{1}_{X_i + X_j \geq 0} - S
			\implies S
			= W_n^+ - \sum_{i=1}^{n} \mathbbm{1}_{X_i > 0}
			= \frac{1}{2} \sum_{i \neq j} \mathbbm{1}_{X_i + X_j \geq 0}.
		\]
		The results for \(W_n^-\) follows similarly.
	\end{explanation}

	By subtracting the above, since \(\mathbbm{1}_{X_i > 0} - \mathbbm{1}_{X_i < 0} = \sgn (X_i) \), we have
	\[
		W_n - \sum_{i=1}^{n} \sgn (X_i)
		= \frac{1}{2} \sum_{i \neq j} \left( \mathbbm{1}_{X_i + X_j \geq 0} - \mathbbm{1}_{X_i + X_j < 0} \right)
		= \frac{1}{2} \sum_{i \neq j} \left( 2 \mathbbm{1}_{X_i + X_j \geq 0} - 1 \right).
	\]
	Plugging the definition of \(h\) yields the result.
\end{proof}

Hence, from \autoref{prop:Wilcoxon-signed-rank-test}, we should first study \(\sum_{i \neq j} h(X_i, X_j)\). First, consider testing
\[
	H_0 \colon X, X_1, \dots , X_n \overset{\text{i.i.d.} }{\sim } F \text{ where \(F\) is continuous such that } X \overset{D}{=} -X,
\]
i.e., \(\mathbb{P} (X \geq x) = \mathbb{P} (X \leq -x)\) for all \(x \in \mathbb{R}\). Note that we're not assuming \(\mathbb{E}_{}[X_i] \) to exists.

\begin{note}
	Since \(X_i\)'s are i.i.d.\ under \(H_0\), \(h(X_i, X_j)\)'s are identically distributed under \(H_0\) for \(i \neq j\).
\end{note}

However, it's clear that \(h(X_i, X_j)\)'s are not independent. Anyway, recall what we're trying to study.

\begin{prev}
	From \autoref{prop:Wilcoxon-signed-rank-test}, under \(H_0\) (in particular, \(F\) being continuous),
	\[
		W_n - \sum_{i=1}^{n} \sgn (X_i)
		= \sum_{i \neq j} \left( h(X_i , X_j) - \frac{1}{2} \right)
	\]
\end{prev}
This is a sum of identically distributed random variables subtracting something.

\begin{intuition}
	If \(1 / 2\) is the expectation of \(h(X_i, X_j)\) for \(i \neq j\), we're getting closer to the familiar form.
\end{intuition}

Indeed, under \(H_0\), \(\mathbb{E}_{}[h(X_i, X_j)] = 1 / 2\) for \(i \neq j\) as
\[
	\begin{split}
		\mathbb{E}_{H_0}[h(X_i, X_j)]
		= \mathbb{E}_{H_0}[h(X_1, X_2)]
		 & = \mathbb{P} (X_1 + X_2 \geq 0)                             \\
		 & = \mathbb{P} (X_1 \geq -X_2)                                \\
		 & = \int _\mathbb{R} \mathbb{P} (X_1 \geq -x) F(\mathrm{d} x)
		= \int _\mathbb{R} \mathbb{P} (X \leq x) F(\mathrm{d} x)
		= \mathbb{E}_{}[F(X)]
		= \frac{1}{2}
	\end{split}
\]
since \(F\) is continuous, and \(F(X) \sim \mathcal{U} (0, 1)\) as we have shown in the homework. Hence, under \(H_0\),
\[
	\sum_{i \neq j} \left( h(X_i, X_j) - \frac{1}{2} \right)
	= 2 \sum_{i < j} \left( h(X_i, X_j) - \frac{1}{2} \right)
	= 2 \sum_{i < j} \left( h(X_i, X_j) - \mathbb{E}_{}[h(X_i, X_j)] \right).
\]
By rescaling by the number of terms in the summation, we're looking at
\[
	\frac{1}{\binom{n}{2}} \left( W_n - \sum_{i=1}^{n} \sgn (X_i) \right)
	= \frac{2}{\binom{n}{2}} \sum_{i < j} \left( h(X_i, X_j) - \mathbb{E}_{}[h(X_i, X_j)] \right)
	\eqqcolon 2 \left( U_n - \frac{1}{2} \right)
\]
where we define \(U_n\) for some permutation symmetric function \(h\)\footnote{Indeed, since \(\mathbb{E}_{}[h(X_1, X_2)] = \mathbb{P} (X_1 + X_2 \geq 0)\), so \(h(X_1, X_2) = h(X_2, X_1)\).} as
\[
	U_n \coloneqq \frac{1}{\binom{n}{2}} \sum_{i < j} h(X_i, X_j).
\]
This is an \hyperref[def:U-statistic]{\(U\)-statistic}, where \(U\) stands for unbiased. We will formally define it later, and we will show that by multiplying \(\sqrt{n} \) on the both sides, this \hyperref[def:converge-in-distribution]{converges} to a standard normal, i.e.,
\begin{equation}\label{eq:Wilcoxon-signed-rank-test}
	\frac{\sqrt{n}}{2 \binom{n}{2}}  \left( W_n - \sum_{i=1}^{n} \sgn (X_i) \right)
	= \sqrt{n} \left( U_n - \frac{1}{2} \right)
	\overset{D}{\to} \mathcal{N} (0, 1 / 3).
\end{equation}

\begin{note}\label{not:Wilcoxon-signed-rank-test-alone}
	This will imply an asymptotic distribution for \(W_n\) exactly alone since under \(H_0\),
	\[
		\frac{\sqrt{n} }{n(n-1)} \sum_{i=1}^{n} \sgn (X_i)
		= \frac{1}{n-1} \left( \frac{1}{\sqrt{n} } \sum_{i=1}^{n} \sgn (X_i) \right)
		\overset{p}{\to} 0.
	\]
\end{note}

\section{\(U\)-Statistics}
In this section, we develop the asymptotic theory for \hyperref[def:U-statistic]{\(U\)-statistics}. While postponing defining \hyperref[def:U-statistic]{\(U\)-statistics}, we first outline what we're going to do. The main tool is the following.

\begin{proposition}\label{prop:correlation-converging-together}
	Given two sequences \((S_n)\), \((\widetilde{S} _n)\) of random variables in \(L^2\), if they satisfy \(\Var_{}[S_n] / \Var_{}[\widetilde{S} _n] \to 1\) and \(\Corr_{}(S_n, \widetilde{S} _n) \to 1\), then
	\[
		\frac{\Var_{}[S_n - \widetilde{S} _n] }{\Var_{}[S_n] } \to 0
		\iff \frac{S_n - \mathbb{E}_{}[S_n] }{\sqrt{\Var_{}[S_n] } } - \frac{\widetilde{S} _n - \mathbb{E}_{}[\widetilde{S} _n] }{\sqrt{\Var_{}[S_n] } }
		\overset{L^2}{\to} 0.
	\]
\end{proposition}
\begin{proof}
	Since for \(n \in\mathbb{N} \), \(\Var_{}[S_n - \widetilde{S} _n] = \Var_{}[S_n] - \Var_{}[\widetilde{S} _n] - 2 \sqrt{\Var_{}[S_n] \Var_{}[\widetilde{S} _n] } \Corr_{}(S_n, \widetilde{S} _n)\), i.e.,
	\[
		\frac{\Var_{}[S_n - \widetilde{S} _n] }{\Var_{}[S_n] }
		= 1 + \frac{\Var_{}[\widetilde{S} _n] }{\Var_{}[S_n] } - 2 \sqrt{\frac{\Var_{}[\widetilde{S} _n] }{\Var_{}[S_n] }} \Corr_{}(S_n, \widetilde{S} _n).
	\]
	The right-hand side goes to \(0\) under the assumptions, proving the result.
\end{proof}

Hence, to show \((S_n - \mathbb{E}_{}[S_n] ) / \sqrt{\Var_{}[S_n] } \overset{D}{\to} Y\), we may find \((\widetilde{S} _n)\) such that \((\widetilde{S} _n - \mathbb{E}_{}[\widetilde{S} _n]) / \sqrt{\Var_{}[S_n] } \overset{D}{\to} Y\) and apply \autoref{prop:correlation-converging-together} with \autoref{thm:converging-together}. In particular, after defining what's an \hyperref[def:U-statistic]{\(U\)-statistic}, we will then apply the above strategy to which.