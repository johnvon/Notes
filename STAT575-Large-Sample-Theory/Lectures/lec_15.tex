\lecture{15}{5 Mar.\ 9:30}{Inference for Cumulative Density Function}
Surprisingly, this \hyperref[def:consistent]{consistency} property holds uniformly over \(t\).

\begin{theorem}[Glivenko-Cantelli]\label{thm:Glivenko-Cantelli}
	Given a cdf \(F\), \(\lVert \hat{F} _n - F \rVert _\infty \overset{\text{a.s.} }{\to} 0\) as \(n \to \infty \).
\end{theorem}
\begin{proof}
	Fix some \(\epsilon > 0\), and let \(\epsilon > 2 / k\) for some \(k \in \mathbb{N} \). Then, for finitely many \(t_1, \dots , t_{k-1}\), \(\hat{F} _n (t_i) \overset{\text{a.s.} }{\to } F(t_i)\) and \(\hat{F} _n(t_i^-) \overset{\text{a.s.} }{\to} F(t_i^-)\) for every \(1 \leq i \leq k-1\) from the \hyperref[thm:SLLN]{strong law of large number}. This means that there exists \(n_0\) such that for \(n \geq n_0\), for every \(\omega \notin N\) such that \(\mathbb{P} (N) = 0\),
	\[
		\vert \hat{F} _n(t_i, \omega ) - F(t_i, \omega ) \vert < \frac{1}{k} ,\text{ and }
		\vert \hat{F} _n(t_i^-, \omega ) - F(t_i^-, \omega ) \vert < \frac{1}{k},
	\]
	so when \(t \in \{ t_i \} _{i=1}^{k-1}\) for some finite \(k\), the desired bound is established. In particular, consider \(t_i = \inf \{ t \in \mathbb{R} \colon F(t) > i / k \}\) for \(1 \leq i \leq k-1\), and \(t_0 \coloneqq -\infty \), \(t_k = \infty \). Then for any \(t \in \mathbb{R} \setminus \{ t_i \}_{i=1}^{k-1} \), there exists a unique \(i\) such that \(t \in (t_{i-1}, t_i)\), and furthermore, for all \(n \geq n_0\),
	\[
		\hat{F} _n(t) - F(t)
		\leq \hat{F} _n(t_i^-) - F(t_{i-1})
		= \hat{F} _n(t_i^-) - F(t_i^-) + F(t_i^-) - F(t_{i-1})
		\leq \frac{1}{k} + \frac{i}{k} - \frac{i-1}{k}
		= \frac{2}{k}
		< \epsilon
	\]
	Similarly, we can show \(\hat{F} _n(t) - F(t) > - \epsilon \) for all \(t \in \mathbb{R} \setminus \{ t_i \}_{i=1}^{k-1} \), which completes the proof.
\end{proof}

\subsection{Donsker's Theorem}
On the other hand, for distributional result, first recall the \emph{empirical process}
\[
	Z_n(t) \coloneqq \sqrt{n} (F(t) - \hat{F} (t))
\]
for \(t \in \mathbb{R} \) introduced in the \hyperref[pf:Bahadur-representation]{proof} of \hyperref[thm:Bahadur-representation]{Bahadur representation theorem}. Recall the following.

\begin{prev}
	We have seen that
	\[
		Z_n(t) \coloneqq \sqrt{n} (F(t) - \hat{F} _n(t))
		\overset{D}{\to} B_F(t) \coloneqq \mathcal{N} (0, F(t) (1 - F(t))).
	\]
	Furthermore, for any \(t_1, \dots , t_m \in \mathbb{R} \),
	\[
		(Z_n(t_1), \dots , Z_n(t_m)) \overset{D}{\to} (B_F(t_1), \dots , B_F(t_m)) \sim \mathcal{N} (0, \Sigma _F(t_1, \dots , t_m))
	\]
	where for \(1 \leq i \leq j \leq m\),
	\[
		\Cov_{}[B_F(t_i), B_F(t_j)]
		= \Cov_{}[\mathbbm{1}_{X \leq t_i} , \mathbbm{1}_{X \leq t_j} ]
		= F(t_i \land t_j) - F(t_i) F(t_j).
	\]
\end{prev}

We now ask the same question, i.e., does the convergence hold uniformly over \(t\)?

\begin{intuition}
	As the theory of \hyperref[def:converge-weakly]{weak convergence} applies to sequences of random elements that take values in general metric spaces, it's reasonable to conjecture that \((Z_n)\) \hyperref[def:converge-weakly]{converges weakly} to some random process \(B_F\) with index set \(\mathbb{R} \), i.e., \(B_F = \{ B_F(t) \} _{t\in \mathbb{R} }\), such that for every \(t, s\in \mathbb{R} \),
	\[
		\mathbb{E}_{}[B_F(t)] = 0 \text{ and } \Cov_{}[B_F(t), B_F(s)] = F(t \land s) - F(t) F(s).
	\]
\end{intuition}

The conjecture is indeed correct, and it's an extension of \href{https://en.wikipedia.org/wiki/Donsker%27s_theorem}{Donsker's theorem}.

\begin{note}
	The formal setup is to view each \(Z_n\) as a random element that takes values on the space \(\mathcal{D} \) of right continuous functions with left limits with the norm \(\lVert \cdot \rVert _\infty \).
\end{note}

\begin{eg}[Brownian bridge]
	A \emph{Brownian bridge} is \(B \coloneqq B_F\) when \(F(t) = t\), i.e., \(F \sim \mathcal{U} ([0, 1])\).
\end{eg}

\begin{note}
	For any cdf \(F\), \(B_F(t)\) is just \(B\) index at \(F(t)\) for any \(t \in \mathbb{R} \), i.e., \(B_F(t) = B(F(t))\).
\end{note}

Taking this convergence as granted, i.e., \((Z_n) \coloneqq (t \mapsto Z_n(t))_{n \geq 1} \overset{\text{w} }{\to} B_F \coloneqq \{ t \mapsto B(t) \}_{t\in \mathbb{R} } \). One immediate consequence is the following.

\begin{proposition}\label{prop:Donsker-continuous-mapping-theorem}
	If \(T \colon \mathcal{D} \to \mathbb{R} \) is continuous,\footnote{I.e., \(T(G_n) \to T(F)\) if \((G_n), F \in \mathcal{D} \) such that \(\lVert G_n - F \rVert _\infty \to 0\).} then \(T(Z_n) \overset{D}{\to} T(B_F)\).
\end{proposition}
\begin{proof}
	Since \(Z_n \overset{\text{w} }{\to} B_F\), \hyperref[thm:continuous-mapping]{continuous mapping theorem} proves the result.
\end{proof}

\subsection{Confidence Bands and Goodness of Fit Tests}
One immediate application is the following.

\begin{corollary}\label{col:Donsker-norm}
	We have \(\sqrt{n} \lVert \hat{F} _n - F \rVert _\infty \overset{D}{\to} \lVert B_F \rVert _\infty \).
\end{corollary}
\begin{proof}
	From \autoref{prop:Donsker-continuous-mapping-theorem}, we just note that \(\lVert \cdot \rVert _\infty \) is continuous since it's a norm.
\end{proof}

In particular, \autoref{col:Donsker-norm} allows us to do inference.

\begin{eg}[Confidence bands]
	Consider \(\alpha = \mathbb{P} (\lVert B_F \rVert _\infty \geq d_\alpha )\) for some \(d_\alpha \), then if \(F\) is continuous, \(\mathbb{P} (\sqrt{n} \lVert F - \hat{F} _n \rVert _\infty \geq d_\alpha ) \to \alpha\), i.e.,
	\[
		\mathbb{P} \left(\forall t \in \mathbb{R} \colon \hat{F} _n(t) - \frac{d_\alpha }{\sqrt{n} } \leq F(t) \leq \hat{F} _n(t) + \frac{d_\alpha }{\sqrt{n} } \right) \to 1 - \alpha .
	\]
\end{eg}

Another application of \autoref{prop:Donsker-continuous-mapping-theorem} is the following.

\begin{corollary}\label{col:Donsker}
	We have
	\[
		n \int _\mathbb{R} \left( \hat{F} _n(t) - F(t) \right) ^2 F(\mathrm{d} t)
		\overset{D}{\to} \int _\mathbb{R} B_F^2(t) F(\mathrm{d} t).
	\]
\end{corollary}
\begin{proof}
	From \autoref{prop:Donsker-continuous-mapping-theorem}, it suffices to show that \(G \mapsto \int _\mathbb{R} G^2 \,\mathrm{d} F\) is continuous for \(G \in \mathcal{D} \). Firstly, let \((G_n), G \in \mathcal{D} \) such that \(\lVert G_n - G \rVert _\infty \to 0\). Then
	\[
		\begin{split}
			\vert T(G_n) - T(G) \vert
			 & = \left\vert \int _\mathbb{R} G^2_n - G^2 \,\mathrm{d} F \right\vert \\
			 & \leq \int _\mathbb{R} \vert G_n^2 - G^2 \vert \,\mathrm{d} F
			\leq \lVert G_n - G \rVert _\infty \int _\mathbb{R} \vert G_n(t) + G(t) \vert F(\mathrm{d} t)
			\leq 2 \lVert G_n - G \rVert _\infty
		\end{split}
	\]
	since \(\lVert G_n - G \rVert _\infty = \sup _t \vert G_n(t) - G(t) \vert \). As \(\lVert G_n - G \rVert _\infty \to 0\), we're done.
\end{proof}

Suppose \(F\) is continuous, then \autoref{col:Donsker-norm} and \autoref{col:Donsker} suggest we can test the null hypothesis \(H_0 \colon F = F_0\) using the \emph{Kolmogorov-Smirnov statistic}
\[
	K_n \coloneqq \lVert \hat{F} _n - F_0 \rVert _\infty ,
\]
and the \emph{Cramér-von Mises statistic}
\[
	C_n \coloneqq \int _\mathbb{R} \left( \hat{F} _n(t) - F_0(t) \right) ^2 F_0(\mathrm{d} t).
\]
Specifically, we
\begin{itemize}
	\item reject \(H_0\) when \(\sqrt{n} \lVert \hat{F} _n - F_0 \rVert _\infty = \sqrt{n} K_n \geq d_\alpha \), where \(d_\alpha \) is defined as \(\alpha = \mathbb{P} (\lVert B_F \rVert _\infty > d_\alpha )\);
	\item reject \(H_0\) when \(n \int _\mathbb{R} (\hat{F} _n - F_0)^2 \,\mathrm{d} F_0 = n C_n \geq c_{\alpha }^2\), where \(c_\alpha \) is defined as \(\alpha = \mathbb{P} (\int_{0}^{1} B_F^2(t) \,\mathrm{d}t \geq c_\alpha ^2)\).
\end{itemize}
It's clear that under \(H_0\), the above two tests will reject with probability approaching \(\alpha \). On the other hand, we have the following.

\begin{proposition}\label{prop:Kolmogorov-Smirnov}
	Suppose \(F\) is continuous. Consider the test \(H_0 \colon F = F_0\) using Kolmogorov-Smirnov statistic, then for any \(F \neq F_0\), \(\mathbb{P} _F(\text{reject} ) = \mathbb{P} (\lVert B_F \rVert _\infty \geq d_\alpha ) \to 1\) as \(n \to \infty \).
\end{proposition}
\begin{proof}
	For any metric \(d\), we have
	\[
		\mathbb{P} _F (\sqrt{n} d(\hat{F} _n, F_0) \geq d_\alpha )
		\geq \mathbb{P} _F(\sqrt{n} (d(\hat{F} _n, F) - d(F, F_0))\geq d_\alpha )
		= \mathbb{P} _F(\sqrt{n} d(\hat{F} _n, F) \geq d_\alpha + \sqrt{n} d(F, F_0)).
	\]
	As \(F \neq F_0\), \(d(F, F_0) > 0\) is a fixed number, so \(\sqrt{n} d(F, F_0) \to \infty \); on the other hand, since \(\sqrt{n} \lVert \hat{F} _n, F  \rVert _\infty = O_p(1)\) \autoref{col:Donsker-norm} with \(d = \lVert \cdot \rVert _\infty \), the right-hand side goes to \(1\).
\end{proof}

The same result can be obtained for the case of using the Cramér-von Mises statistic as follows.

\begin{prev}
	From \autoref{col:Donsker}, under \(H_0 \colon F = F_0\),
	\[
		n C_n
		= n \int _\mathbb{R} (\hat{F} _n - F_0)^2 \,\mathrm{d} F_0
		\overset{D}{\to } \int _\mathbb{R} B_F^2 \,\mathrm{d} F_0.
	\]
\end{prev}

However, if \(F \neq F_0\), what's the distribution now? Consider
\[
	h(F) \coloneqq \int _\mathbb{R} (F - F_0)^2 \,\mathrm{d} F_0.
\]
Since \(h\) is continuous, \(C_n = h(\hat{F} _n) \to h(F) \). As for a distributional result, we have the following.

\begin{proposition}\label{prop:Cramer-von Mises}
	There is a function \(g\) so that \(\mathbb{E}_{}[g(X)] = 0\) and
	\[
		\sqrt{n} \left( C_n - \int _\mathbb{R} (F - F_0)^2 \,\mathrm{d} F_0 \right)
		= \frac{1}{\sqrt{n} } \sum_{i=1}^{n} g(X_i) + o_p(1).
	\]
	If we further have \(F \neq F_0\), then the above converges to \(\mathcal{N} (0, \Var_{}[g(X)] )\).
\end{proposition}
\begin{proof}
	We first note that the left-hand side is just \(\sqrt{n} (h(\hat{F} _n) - h(F))\). Now, since
	\[
		\begin{split}
			h(\hat{F} _n)
			= C_n
			 & = \int _\mathbb{R} (\hat{F} _n - F_0)^2 \,\mathrm{d} F_0                                                                                                                                     \\
			 & = \int _\mathbb{R} (\hat{F} _n - F + F - F_0)^2 \,\mathrm{d} F_0                                                                                                                             \\
			 & = \int _\mathbb{R} (\hat{F} _n - F)^2 \,\mathrm{d} F_0 + \underbrace{\int _\mathbb{R} (F - F_0)^2 \,\mathrm{d} F_0}_{h(F)} + 2 \int _\mathbb{R} (\hat{F} _n - F) (F - F_0) \,\mathrm{d} F_0,
		\end{split}
	\]
	we have
	\[
		\sqrt{n} \left( h(\hat{F} _n) - h(F) \right)
		= \sqrt{n} \int _\mathbb{R} (\hat{F} _n - F)^2 \,\mathrm{d} F_0 + 2 \sqrt{n} \int _\mathbb{R} (\hat{F} _n - F) (F - F_0) \,\mathrm{d} F_0.
	\]
	As \(n \int _\mathbb{R} (\hat{F} _n - F)^2 \,\mathrm{d} F_0 \overset{\text{w} }{\to} \int _\mathbb{R} B_F^2 \,\mathrm{d} F_0\), \autoref{prop:convergence-in-distirbution-bounded-in-probability} implies
	\[
		\sqrt{n} \int _\mathbb{R} (\hat{F} _n - F)^2 \,\mathrm{d} F_0
		= \frac{n}{\sqrt{n} } \int _\mathbb{R} (\hat{F} _n - F)^2 \,\mathrm{d} F_0
		= \frac{O_p(1)}{\sqrt{n} }
		= o_p(1),
	\]
	which gives
	\[
		\sqrt{n} \left( h(\hat{F} _n) - h(F) \right)
		= 2 \sqrt{n} \int _\mathbb{R} (\hat{F} _n - F) (F - F_0) \,\mathrm{d} F_0 + o_p(1)
		\eqqcolon \frac{1}{\sqrt{n} } \sum_{i=1}^{n} g(X_i) + o_p(1)
	\]
	where we define the function \(g \colon \mathbb{R} \to \mathbb{R} \) as
	\[
		g(x)
		\coloneqq 2 \int _\mathbb{R} (\mathbbm{1}_{x \leq t} - F(t)) (F(t) - F_0(t)) F_0(\mathrm{d} t)
	\]
	since
	\[
		\begin{split}
			\frac{1}{\sqrt{n} } \sum_{i=1}^{n} g(X_i)
			 & = \frac{2}{\sqrt{n} } \sum_{i=1}^{n} \int _\mathbb{R} (\mathbbm{1}_{X_i \leq t} - F(t)) (F(t) - F_0(t)) F_0(\mathrm{d} t)                                            \\
			 & = \frac{2}{\sqrt{n} } \int _\mathbb{R} \sum_{i=1}^{n} (\mathbbm{1}_{X_i \leq t} - F(t)) (F(t) - F_0(t)) F_0(\mathrm{d} t)                                            \\
			 & = \frac{2}{\sqrt{n} } \int _\mathbb{R} \sum_{i=1}^{n} \mathbbm{1}_{X_i \leq t} (F(t) - F_0(t)) - n F(t) (F(t) - F_0(t)) F_0(\mathrm{d} t)                            \\
			 & = \frac{2}{\sqrt{n} } \int _\mathbb{R} n \left( \frac{1}{n} \sum_{i=1}^{n} \mathbbm{1}_{X_i \leq t} (F(t) - F_0(t)) - F(t) (F(t) - F_0(t)) \right) F_0(\mathrm{d} t) \\
			 & = 2\sqrt{n} \int _\mathbb{R} (\hat{F} _n - F)(F - F_0) \,\mathrm{d} F_0.
		\end{split}
	\]
	To show \(\mathbb{E}_{}[g(X)] = 0\), as \(F(t), F_0(t), \mathbbm{1}_{x \leq t}\) are all bounded by \(1\), \href{https://en.wikipedia.org/wiki/Fubini's_theorem}{Fubini's theorem} gives
	\[
		\mathbb{E}_{}[g(X)]
		= 2 \int _\mathbb{R} (\mathbb{P} (X \leq t) - F(t)) (F(t) - F_0(t)) F_0(\mathrm{d} t)
		= 0
	\]
	since \(\mathbb{P} (X \leq t) = F(t)\). Finally, when \(F \neq F_0\), \(0 < \mathbb{E}_{}[g^2(X)] < \infty \) follows from the same calculation, hence \hyperref[thm:CLT]{central limit theorem} gives the distributional result.
\end{proof}