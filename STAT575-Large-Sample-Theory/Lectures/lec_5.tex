\lecture{5}{1 Feb.\ 9:30}{Convergence in Distribution and Weak Convergence}
Now we prove \hyperref[thm:converging-together]{converging together}.

\begin{proof}[Proof of \autoref{thm:converging-together}]
	From \hyperref[thm:Portmanteau]{Portmanteau theorem} \autoref{thm:Portmanteau-b}, we want to prove that \(\mathbb{E}_{n}\left[g(Y_n) \right] \to \mathbb{E}_{}\left[g(X) \right]\) for all bounded and Lipschitz \(g \colon \mathbb{R} ^d \to \mathbb{R} \). Specifically, let \(\vert g(x) \vert \leq C\) for all \(x \in \mathbb{R} ^d\) and \(\vert g(x) - g(y) \vert \leq K \lVert x - y \rVert \) for all \(x, y \in \mathbb{R} ^d\). From triangle inequality,
	\[
		\left\vert \mathbb{E}_{n}\left[g(Y_n) \right] - \mathbb{E}_{}\left[g(X) \right]  \right\vert
		\leq \left\vert \mathbb{E}_{n}\left[g(Y_n) \right] - \mathbb{E}_{n}\left[g(X_n) \right] \right\vert + \left\vert \mathbb{E}_{n}\left[g(X_n) \right] - \mathbb{E}_{}\left[g(X) \right] \right\vert .
	\]
	Since \(X_n \overset{\text{w} }{\to } X\), the second term goes to \(0\). As for the first term, we see that
	\[
		\begin{split}
			\left\vert \mathbb{E}_{n}\left[g(Y_n) \right] - \mathbb{E}_{n}\left[g(X_n) \right]  \right\vert
			 & = \left\vert \mathbb{E}_{n}\left[g(Y_n) - g(X_n) \right]  \right\vert                                                                                                                                                                  \\
			 & \leq \mathbb{E}_{n}\left[ \vert g(Y_n) - g(X_n) \vert \right]                                                                                                                                                                          \\
			 & = \mathbb{E}_{n}\left[\vert g(Y_n) - g(X_n) \vert \cdot \mathbbm{1}_{\lVert X_n - Y_n \rVert > \epsilon } \right] + \mathbb{E}_{n}\left[\vert g(Y_n) - g(X_n) \vert \cdot \mathbbm{1}_{\lVert X_n - Y_n \rVert \leq \epsilon } \right] \\
			 & \leq 2C \mathbb{P} _{n}(\lVert X_n - Y_n \rVert > \epsilon ) + K \epsilon \mathbb{P} _{n}(\lVert X_n - Y_n \rVert \leq \epsilon )                                                                                                      \\
			 & \leq 2C \mathbb{P}_{n} (\lVert X_n - Y_n \rVert > \epsilon ) + K \epsilon .
		\end{split}
	\]
	As \(n \to \infty \), \(\limsup_{n \to \infty} \left\vert \mathbb{E}_{n}\left[g(Y_n) \right] - \mathbb{E}_{}\left[g(X) \right]  \right\vert \leq K \epsilon\) for all \(\epsilon > 0\), by letting \(\epsilon \to 0\), we're done.
\end{proof}

Another characterization regards the difference between marginal and joint \hyperref[def:converge-weakly]{weak convergence}.

\begin{prev}
	\(X_n \overset{p}{\to } X\) and \(Y_n \overset{p}{\to } Y\) if and only if \((X_n , Y_n) \overset{p}{\to } (X, Y)\). Same for \(\overset{\text{a.s.} }{\to} \).
\end{prev}

However, even if \((\Omega _n, \mathscr{F} _n, \mathbb{P} _n) = (\Omega , \mathscr{F} , \mathbb{P} )\), the marginal and joint \hyperref[def:converge-weakly]{weak convergences} are not equivalent. Specifically, in the case of \hyperref[def:converge-weakly]{weak convergence}, from \hyperref[thm:continuous-mapping]{continuous mapping theorem}, if \((X_n , Y_n) \overset{\text{w} }{\to } (X, Y)\), then \(X_n \overset{\text{w} }{\to } X\) and \(Y_n \overset{\text{w} }{\to } Y\). However, the converse needs not be true.

\begin{eg}\label{eg:counter-example-continuous-mapping}
	Let \(X_n = X\), \(Y_n = -X\) for all \(n \geq 1\). If \(X \sim \mathcal{N} (0, 1)\), we see that \(\mathbb{P} (X \in A) = \mathbb{P} (-X \in A)\) for all \(A \subseteq \mathbb{R} ^d\), implying \(X_n \overset{\text{w} }{\to } X\) and \(Y_n \overset{\text{w} }{\to } X\).

	However, this does not imply \((X_n , Y_n) \overset{\text{w} }{\to } (X, X)\) since otherwise, by \hyperref[thm:continuous-mapping]{continuous mapping theorem}, \(X_n + Y_n \overset{\text{w} }{\to } X + X = 2X\), which is not true since \(X_n + Y_n = 0\).
\end{eg}

But in the case of \(Y\) is a constant, the converse is actually true, and the result is quite useful.

\begin{theorem}[Slutsky's theorem]\label{thm:Slutsky}
	If \(X_n \overset{\text{w} }{\to } X \) in \(\mathbb{R} ^d\) and \(Y_n \overset{p}{\to } c\) in \(\mathbb{R} ^m\),\footnote{Recall that from \autoref{col:weak-probability-constant}, for a constant \(c\), \hyperref[def:converge-weakly]{weak convergence} is equivalent to \hyperref[def:converge-in-probability]{convergence in probability}.} then \((X_n , Y_n) \overset{\text{w} }{\to } (X, c)\).
\end{theorem}
\begin{proof}
	Firstly, we show that \((X_n , c) \overset{\text{w} }{\to } (X, c)\). Indeed, since for every continuous and bounded \(g \colon \mathbb{R} ^{d+m} \to \mathbb{R} \), from \(X_n \overset{\text{w} }{\to } X\) with \(g(\cdot, c)\) being continuous and bounded, \(\mathbb{E}_{n}\left[g(X_n, c) \right] \to \mathbb{E}_{}\left[g(X, c) \right]\).

	Secondly, we show that \(\lVert (X_n, Y_n) - (X_n, c) \rVert \overset{p}{\to } 0\). This is easy since
	\[
		\lVert (X_n, Y_n) - (X_n, c) \rVert
		\leq \lVert X_n - X_n \rVert + \lVert Y_n - c \rVert
		= \lVert Y_n - c \rVert ,
	\]
	which goes to \(0\) \hyperref[def:converge-in-probability]{in probability}. Combining the above with \hyperref[thm:converging-together]{converging together} gives the result.
\end{proof}

Revisiting the \hyperref[eg:counter-example-continuous-mapping]{counter-example}, we see that now it's not the case when \(Y\) is a constant.

\begin{corollary}\label{col:Slutsky}
	If \(X_n \overset{\text{w} }{\to } X \) and \(Y_n \overset{p}{\to } c\) in \(\mathbb{R} ^d\), \(X_n \pm Y_n \overset{\text{w} }{\to } X \pm c\), \(X_n \cdot Y_n \overset{\text{w} }{\to } X \cdot c\). If \(d = 1\) and \(c \neq 0\), then \(X_n / Y_n \overset{\text{w} }{\to } X / c\).
\end{corollary}
\begin{proof}
	This follows directly from \hyperref[thm:Slutsky]{Slutsky's theorem} and \hyperref[thm:continuous-mapping]{continuous mapping theorem}.
\end{proof}

\section{Convergence in Distribution}
The convergences we have been talking about applies to general probability space, not necessarily \(\mathbb{R} ^d\). However, compared to \hyperref[def:converge-weakly]{weak convergence}, \(\mathbb{R} ^d\) is considered first in terms of distributional convergence.

\begin{intuition}
	There's a conical ordering available in \(\mathbb{R} ^d\) to define \(F_X\) and \(F_{X_n}\).
\end{intuition}

\begin{definition}[Converge in distribution]\label{def:converge-in-distribution}
	Let \((X_n)\) and \(X\) be random vectors in \(\mathbb{R} ^d\). Then \((X_n)\) \emph{converges in distribution} to \(X\), denoted as \(X_n \overset{D}{\to } X\), if for all \((t_1, \dots , t_d) \in C_{F_X}\),
	\[
		F_{X_n} (t_1, \dots , t_d) \to F_X (t_1, \dots , t_d).
	\]
\end{definition}

Specifically, to see how this relates to what we have seen, recall that
\[
	F_{X_n}(t_1, \dots , t_d)
	= \mathbb{P} _{n}(X_n^i \leq t_i , \forall1 \leq i \leq d)
	= \mathbb{P} _{n}(X_n \in (-\infty , t_1] \times \dots \times (-\infty , t_d]),
\]
same for \(F_X\). So this reduces to the form we're familiar with, i.e., \(\mathbb{P} _{n}(X_n \in A)\) for some \(A\). Let's make some remarks for this new notion of convergence.

\begin{remark}
	\(X_n \overset{\operatorname{TV} }{\to } X\) implies \(X_n \overset{D}{\to } X\).
\end{remark}
\begin{explanation}
	Since \(X_n \overset{\operatorname{TV} }{\to } X \) means \(\mathbb{P} _{n}(X_n \in A) \to \mathbb{P} (X \in A)\) uniformly in \(A\), but \(X_n \overset{D}{\to } X\) only requires the above holds for \(A\) in the form of \((-\infty , t_1] \times \dots \times (-\infty , t_d]\), which is weaker.
\end{explanation}

There are more classical results that are worth mentioning.

\begin{remark}[De Moivre's central limit theorem]
	Let \(X_n \sim \operatorname{Bin}(n, p) \), then for every \(t \in \mathbb{R} \), as \(n \to \infty \),
	\[
		\mathbb{P} \left( \frac{X_n - np}{\sqrt{np (1 - p)} } \leq t \right)
		\to \frac{1}{\sqrt{2\pi } } \int_{-\infty}^{t} e^{- u^2 / 2} \,\mathrm{d}u
		= \Phi (t).
	\]
\end{remark}

\begin{proposition}
	Let \(X_n\) and \(X\) be in \(\mathbb{Z} \) such that \(f_n\) and \(f\) are their corresponding pmf's, then
	\[
		f_n \to f
		\iff X_n \overset{\operatorname{TV} }{\to } X
		\iff X_n \overset{D}{\to } X.
	\]
\end{proposition}
\begin{proof}
	The forward implications are clear, so we just need to show \(X_n \overset{D}{\to } X \) implies \(f_n \to f\). Since for every \(t \in \mathbb{Z} \), since \(X_n\) and \(X\) are discrete in \(\mathbb{Z} \), for some \(\epsilon > 0\) small enough,
	\[
		f_n(t)
		= \mathbb{P} _{n}(X_n = t)
		= \mathbb{P} _{n}(X_n \leq t + \epsilon ) - \mathbb{P} _{n}(X_n \leq t - \epsilon ).
	\]
	Since \(t \pm \epsilon \in C_X\), \(X_n \overset{D}{\to } X\) implies \(\mathbb{P} _{n}(X_n \leq t + \epsilon ) \to \mathbb{P} (X \leq t + \epsilon )\). The same holds for \(t - \epsilon \), hence
	\[
		\begin{split}
			f_n(t)
			= \mathbb{P} _{n}(X_n = t)
			 & = \mathbb{P} _{n}(X_n \leq t + \epsilon ) - \mathbb{P} _{n}(X_n \leq t - \epsilon ) \\
			 & \to \mathbb{P} (X \leq t + \epsilon ) - \mathbb{P} (X \leq t - \epsilon )
			= \mathbb{P} (X = t)
			= f(t).
		\end{split}
	\]
	As this holds for every \(t\in \mathbb{Z} \), we're done.
\end{proof}

One important remark is the following.

\begin{remark}
	It's necessary to not require the condition for all \(t \in \mathbb{R} ^d\), but only \(t \in C_{F_X}\).
\end{remark}
\begin{explanation}
	Consider for \(d = 1\) with \(X = c \in \mathbb{R} \), i.e., \(F_X\) is the step function at \(c\). To show \(X_n \overset{D}{\to } c\), we don't have to show \(\mathbb{P} _{n}(X_n \leq c) \to \mathbb{P} (X \leq c) = 1\). Otherwise, if we need to show this for all \(t\), in particular, \(c\), \(X_n = c + 1 / n\) would not satisfy this.
\end{explanation}

In terms of continuity, if \(X_n \overset{D}{\to } X\) and \(X\) is continuous, then \(F_{X_n}\) converges to \(F_X\) not only point-wise, but uniformly. Specifically, we have the following.

\begin{remark}[PÃ³lya's theorem]
	If \(F_X\) is continuous, \(X_n \overset{D}{\to } X\) is equivalent as
	\[
		\sup _{t\in \mathbb{R} ^d} \left\vert F_{X_n} (t) - F_X(t) \right\vert \to 0.
	\]
\end{remark}

\subsection{Equivalency of Convergence in Distribution and Weak Convergence}
Surprisingly, \hyperref[def:converge-in-distribution]{convergence in distribution} is actually just a renaming of \hyperref[def:converge-weakly]{weak convergence} in \(\mathbb{R} ^d\).

\begin{theorem}\label{thm:weak-convergence-is-convergence-in-distribution}
	Given \((X_n)\) and \(X\) in \(\mathbb{R} ^d\), \(X_n \overset{\text{w} }{\to } X\) if and only if \(X_n \overset{D}{\to } X\).
\end{theorem}
\begin{proof}\let\qed\relax
	We prove for the case of \(d = 1\), then it's easy to see the same holds for \(d \geq 1\). For the forward direction, we want to show that for all \(t \in C_{F_X}\), \(\mathbb{P} _{n}(X_n \leq t) \to \mathbb{P} (X \leq t)\). Note that
	\[
		\mathbb{P} (X \leq t) = \mathbb{P} (X \in (-\infty , t]), \text{ and }
		\mathbb{P} _{n}(X_n \leq t) = \mathbb{P} _{n}(X_n \in (-\infty , t]),
	\]
	hence, from \hyperref[thm:Portmanteau]{Portmanteau theorem} \autoref{thm:Portmanteau-e} with \(A = (-\infty , t]\), \(X_n \overset{\text{w} }{\to } X\) is equivalently to \(\mathbb{P} _{n}(X_n \leq t) \to \mathbb{P} (X \leq t)\) if \(\mathbb{P} (X \in \partial A) = 0\), i.e.,
	\[
		\mathbb{P} (X \in \partial (-\infty , t]) = \mathbb{P} (X \in \{ t \} ) = \mathbb{P} (X = t) = 0,
	\]
	which is true since \(t \in C_{F_X}\).

	To show the backward direction, we need the following lemma.
	\begin{lemma}\label{lma:distribution-function-limit-inequality}
		\(X_n \overset{D}{\to } X\) if and only if for all \(x \in \mathbb{R} ^d\),
		\[
			F_X(x^-)
			\leq \liminf_{n \to \infty} F_{X_n} (x^-)
			\leq \liminf_{n \to \infty} F_{X_n} (x)
			\leq \limsup_{n \to \infty} F_{X_n} (x)
			\leq F_X(x).
		\]
	\end{lemma}
	\begin{proof}
		The backward direction is clear, so we prove the forward direction. When \(x\in C_{F_X}\), we're clearly done, so consider \(x \notin C_{F_X}\). Firstly, note that \(\vert C_{F_X}^{c} \vert \) is countable, so there exists \((x_k) \nearrow x\) and \((y_k) \searrow x\), both in \(C_{F_X}\). Hence, for all \(n \geq 1\) and \(k \geq 1\),
		\[
			F_{X_n}(x_k)
			\leq F_{X_n}(x)
			\leq F_{X_n}(y_k)
		\]
		as \(F_{X_n}\) is increasing. We now have for every \(k \geq 1\),
		\begin{align*}
			F_X(x_k)
			 & = \lim_{n \to \infty} F_{X_n}(x_k) \tag*{\(x_k \in C_{F_X}\)}           \\
			 & \leq \liminf_{n \to \infty} F_{X_n}(x^-)                                \\
			 & \leq \liminf_{n \to \infty} F_{X_n}(x) \tag*{\(F_{X_n}\) is increasing} \\
			 & \leq \limsup_{n \to \infty} F_{X_n}(x)                                  \\
			 & \leq \limsup_{n \to \infty} F_{X_n}(y_k)
			= F_X(y_k). \tag*{\(y_k \in C_{F_X}\)}
		\end{align*}
		By taking \(k \to \infty \), \(F_X(x_k) \to F_X(x^-)\), while \(F_X(y_k) \to F_X(x)\),\footnote{Recall that the distribution function is always right-continuous.} and we're done.
	\end{proof}
	\emph{The proof will be \hyperref[pf:thm:weak-convergence-is-convergence-in-distribution]{continued}\dots}
\end{proof}