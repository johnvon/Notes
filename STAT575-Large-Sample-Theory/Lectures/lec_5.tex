\lecture{5}{1 Feb.\ 9:30}{Convergence in Distribution and Weak Convergence}
Now we prove \autoref{thm:converging-together}.

\begin{proof}
	From \hyperref[thm:Portmanteau]{Portmanteau theorem} \autoref{thm:Portmanteau-b}, we want to prove that \(\mathbb{E}_{}\left[g(Y_n) \right] \to \mathbb{E}_{}\left[g(X) \right]\) for all bounded and Lipschitz \(g \colon \mathbb{R} ^d \to \mathbb{R} \). Specifically, let \(\vert g(x) \vert \leq C\) for all \(x \in\mathbb{R} ^d\) and \(\vert g(x) - g(y) \vert \leq K \lVert x - y \rVert \) for all \(x, y \in \mathbb{R} ^d\). From triangle inequality,
	\[
		\left\vert \mathbb{E}_{}\left[g(Y_n) \right] - \mathbb{E}_{}\left[g(X) \right]  \right\vert
		\leq \left\vert \mathbb{E}_{}\left[g(Y_n) \right] - \mathbb{E}_{}\left[g(X_n) \right]  \right\vert + \left\vert \mathbb{E}_{}\left[g(X_n) \right] - \mathbb{E}_{}\left[g(X) \right]  \right\vert .
	\]
	Since \(X_n \overset{\text{w} }{\to } X\), the second term goes to \(0\). As for the first term, since \(Y_n\) and \(X_n\) are in the same probability space, we see that
	\[
		\begin{split}
			\left\vert \mathbb{E}_{}\left[g(Y_n) \right] - \mathbb{E}_{}\left[g(X_n) \right]  \right\vert
			 & = \left\vert \mathbb{E}_{}\left[g(Y_n) - g(X_n) \right]  \right\vert                                                                                                                                                                 \\
			 & \leq \mathbb{E}_{}\left[ \vert g(Y_n) - g(X_n) \vert \right]                                                                                                                                                                         \\
			 & = \mathbb{E}_{}\left[\vert g(Y_n) - g(X_n) \vert \cdot \mathbbm{1}_{\lVert X_n - Y_n \rVert > \epsilon } \right] + \mathbb{E}_{}\left[\vert g(Y_n) - g(X_n) \vert \cdot \mathbbm{1}_{\lVert X_n - Y_n \rVert \leq \epsilon } \right] \\
			 & \leq 2C \mathbb{P} (\lVert X_n - Y_n \rVert > \epsilon ) + K \epsilon \mathbb{P} (\lVert X_n - Y_n \rVert \leq \epsilon )                                                                                                            \\
			 & \leq 2C \mathbb{P} (\lVert X_n - Y_n \rVert > \epsilon ) + K \epsilon .
		\end{split}
	\]
	As \(n \to \infty \), we finally have
	\[
		\limsup_{n \to \infty} \left\vert \mathbb{E}_{}\left[g(Y_n) \right] - \mathbb{E}_{}\left[g(X) \right]  \right\vert
		\leq K \epsilon
	\]
	for all \(\epsilon > 0\), by letting \(\epsilon \to 0\), we're done.
\end{proof}

We can now apply \autoref{thm:converging-together} to prove something similar as we have seen before in the case of \hyperref[def:converge-in-probability]{convergence in probability}.

\begin{prev}
	\(X_n \overset{p}{\to } X\) and \(Y_n \overset{p}{\to } Y\) if and only if \((X_n , Y_n) \overset{p}{\to } (X, Y)\).
\end{prev}

Now, in the case of \hyperref[def:weak-convergence]{weak convergence}, from \hyperref[thm:continuous-mapping]{continuous mapping theorem}, we see that if \((X_n , Y_n) \overset{\text{w} }{\to } (X, Y)\), then \(X_n \overset{\text{w} }{\to } X\) and \(Y_n \overset{\text{w} }{\to } Y\). However, the converse needs not be true.

\begin{eg}\label{eg:counter-example-continuous-mapping}
	Consider a random variable \(X\) on \((\Omega , \mathscr{F} , \mathbb{P} )\), and let \(X_n = X\), \(Y_n = -X\) for all \(n \geq 1\). If \(X \sim \mathcal{N} (0, 1)\), we see that \(\mathbb{P} (X \in A) = \mathbb{P} (-X \in A)\) for all \(A \subseteq \mathbb{R} ^d\), implying \(X_n \overset{\text{w} }{\to } X\) and \(Y_n \overset{\text{w} }{\to } X\).

	However, this does not imply \((X_n , Y_n) \overset{\text{w} }{\to } (X, X)\) since otherwise, by \hyperref[thm:continuous-mapping]{continuous mapping theorem}, \(X_n + Y_n \overset{\text{w} }{\to } X + X = 2X\), which is not true since \(X_n + Y_n = 0\).
\end{eg}

But in the case of \(Y\) is a constant, the converse is actually true, and the result is quite useful.

\begin{theorem}[Slutsky's theorem]\label{thm:Slutsky}
	If \(X_n \overset{\text{w} }{\to } X \) in \(\mathbb{R} ^d\) and \(Y_n \overset{p}{\to } c\) in \(\mathbb{R} ^m\),\footnote{Recall that from \autoref{col:weak-probability-constant}, for a constant \(c\), \hyperref[def:weak-convergence]{weak convergence} is equivalent to \hyperref[def:converge-in-probability]{convergence in probability}.} then \((X_n , Y_n) \overset{\text{w} }{\to } (X, c)\).
\end{theorem}
\begin{proof}
	Firstly, we show that \((X_n , c) \overset{\text{w} }{\to } (X, c)\). Indeed, since for every continuous and bounded \(g \colon \mathbb{R} ^{d+m} \to \mathbb{R} \), \(\mathbb{E}_{}\left[g(X_n, c) \right] \to \mathbb{E}_{}\left[g(X, c) \right]\) follows directly from \(X_n \overset{\text{w} }{\to } X\) with \(g(\cdot, c)\) being continuous and bounded.

	Secondly, we show that \(\lVert (X_n, Y_n) - (X_n, c) \rVert \overset{p}{\to } 0\). This is easy since
	\[
		\lVert (X_n, Y_n) - (X_n, c) \rVert
		\leq \lVert X_n - X_n \rVert + \lVert Y_n - c \rVert
		= \lVert Y_n - c \rVert ,
	\]
	which goes to \(0\) \hyperref[def:converge-in-probability]{in probability} as we wish. Combining both with \autoref{thm:converging-together} gives the result.
\end{proof}

Revisiting the \hyperref[eg:counter-example-continuous-mapping]{counter-example}, we see that now it's not the case when \(Y\) is a constant.

\begin{corollary}\label{col:Slutsky}
	If \(X_n \overset{\text{w} }{\to } X \) and \(Y_n \overset{p}{\to } c\) in \(\mathbb{R} ^d\), \(X_n \pm Y_n \overset{\text{w} }{\to } X \pm c\).
\end{corollary}

\subsection{Convergence in Distribution}
So far, the notions of convergence we have talked about applies to general probability space, which needs not to be in \(\mathbb{R} ^d\) in general. However, traditionally, the case in \(\mathbb{R} ^d\) is considered first.

\begin{intuition}
	There's a conical ordering available in \(\mathbb{R} ^d\) to define \(F_X\) and \(F_{X_n}\).
\end{intuition}

This allows us to define the following.

\begin{definition}[Converge in distribution]\label{def:converge-in-distribution}
	Let \((X_n)\) and \(X\) be random variables in \(\mathbb{R} ^d\). Then \((X_n)\) \emph{converges in distribution} to \(X\), denoted as \(X_n \overset{D}{\to } X\), if for all \((t_1, \dots , t_d) \in C_{F_X}\),
	\[
		F_{X_n} (t_1, \dots , t_d) \to F_X (t_1, \dots , t_d).
	\]
\end{definition}

\begin{note}
	\(X_n\) and \(X\) (in \(\mathbb{R} ^d\)) do not have to be on the same probability space.
\end{note}

Specifically, to see how this relates to what we have seen, recall that
\[
	F_{X_n}(t_1, \dots , t_d)
	= \mathbb{P} (X_n^i \leq t_i , \forall1 \leq i \leq d)
	= \mathbb{P} (X_n \in (-\infty , t_1] \times \dots \times (-\infty , t_d]),
\]
same for \(F_X\). So this reduces to the form we're familiar with, i.e., \(\mathbb{P} (X_n \in A)\) for some \(A\).

\begin{remark}
	\(X_n \overset{\operatorname{TV} }{\to } X\) implies \(X_n \overset{D}{\to } X\).
\end{remark}
\begin{explanation}
	Since \(X_n \overset{\operatorname{TV} }{\to } X \) means \(\mathbb{P} (X_n \in A) \to \mathbb{P} (X \in A)\) uniformly in \(A\), but \(X_n \overset{D}{\to } X\) only requires the above holds for \(A\) in the form of \((-\infty , t_1] \times \dots \times (-\infty , t_d]\), which is weaker.
\end{explanation}

There are more classical results that are worth mentioning.

\begin{remark}[De Moivre]
	Let \(X_n \sim \operatorname{Bin}(n, p) \), then for every \(t \in \mathbb{R} \), as \(n \to \infty \),
	\[
		\mathbb{P} \left( \frac{X_n - np}{\sqrt{np (1 - p)} } \leq t \right)
		\to \frac{1}{\sqrt{2\pi } } \int_{-\infty}^{\infty} e^{- u^2 / 2} \,\mathrm{d}u
		\eqqcolon \phi (u)
		= \mathbb{P} (Z \leq t)
	\]
	where \(Z \sim \mathcal{N} (0, 1)\).
\end{remark}

\begin{remark}[Polya's theorem]
	If \(F_X\) is continuous, \(X_n \overset{D}{\to } X\) is equivalent as
	\[
		\sup _{t\in \mathbb{R} ^d} \left\vert F_{X_n} (t) - F_X(t) \right\vert \to 0.
	\]
\end{remark}

\begin{remark}
	Let \(X_n\) and \(X\) be in \(\mathbb{Z} \). Let \(f_n\) and \(f\) be their corresponding PMF's, then
	\[
		f_n \to f
		\iff X_n \overset{\operatorname{TV} }{\to } X
		\iff X_n \overset{D}{\to } X.
	\]
\end{remark}
\begin{explanation}
	The forward implications are clear, so we just need to show \(X_n \overset{D}{\to } X \) implies \(f_n \to f\). Since for every \(t \in \mathbb{Z} \), since \(X_n\) and \(X\) are discrete in \(\mathbb{Z} \),
	\[
		f_n(t) = \mathbb{P} (X_n = t)
		= \mathbb{P} (X_n \leq t + \epsilon ) - \mathbb{P} (X_n \leq t - \epsilon )
	\]
	for some \(\epsilon > 0\) small enough. Now, as \(t \pm \epsilon \) are in \(C_X\) clearly, \(X_n \overset{D}{\to } X\) implies
	\[
		\mathbb{P} (X_n \leq t + \epsilon ) \to \mathbb{P} (X \leq t + \epsilon ),
	\]
	and the same holds for \(t - \epsilon \), hence
	\[
		f_n(t) = \mathbb{P} (X_n = t)
		= \mathbb{P} (X_n \leq t + \epsilon ) - \mathbb{P} (X_n \leq t - \epsilon )
		\to \mathbb{P} (X \leq t + \epsilon ) - \mathbb{P} (X \leq t - \epsilon )
		= \mathbb{P} (X = t)
		= f(t).
	\]
	As this holds for every \(t\in \mathbb{Z} \), we're done.
\end{explanation}

Now, the problem one might have is the following.

\begin{problem*}
	Why not defined for all \(t \in \mathbb{R} ^d\), rather than \(t \in C_{F_X}\)?
\end{problem*}
\begin{answer}
	Consider for \(d = 1\) with \(X = c \in \mathbb{R} \), i.e., \(F_X\) is the step function at \(c\). To show \(X_n \overset{D}{\to } c\), we don't have to show \(\mathbb{P} (X_n \leq c) \to \mathbb{P} (X \leq c) = 1\). Otherwise, if we need to show this for all \(t\), in particular, \(c\), \(X_n = c + 1 / n\) would not satisfy this.
\end{answer}

Now we have seen various remarks and clarifications about \hyperref[def:converge-in-distribution]{convergence in distribution}, the upshot is that, it is actually just a renaming of \hyperref[def:weak-convergence]{weak convergence} in \(\mathbb{R} ^d\)!

\begin{theorem}\label{thm:weak-convergence-is-convergence-in-distribution}
	Given \(X_n\) and \(X\) in \(\mathbb{R} ^d\), then \(X_n \overset{\text{w} }{\to } X\) if and only if \(X_n \overset{D}{\to } X\).
\end{theorem}
\begin{proof}\let\qed\relax
	We prove for the case of \(d = 1\), then it's easy to see the same holds for \(d \geq 1\). For the forward direction, we want to show that for all \(t \in C_{F_X}\),
	\[
		\mathbb{P} (X_n \leq t) \to \mathbb{P} (X \leq t).
	\]
	Note that \(\mathbb{P} (X \leq t) = \mathbb{P} (X \in (-\infty , t])\) and \(\mathbb{P} (X_n \leq t) = \mathbb{P} (X_n \in (-\infty , t])\), hence, from \hyperref[thm:Portmanteau]{Portmanteau theorem} \autoref{thm:Portmanteau-e} with \(A = (-\infty , t]\), \(X_n \overset{\text{w} }{\to } X\) is equivalently as saying \(\mathbb{P} (X_n \leq t) \to \mathbb{P} (X \leq t)\) if
	\[
		\mathbb{P} (X \in \partial (-\infty , t])
		= \mathbb{P} (X \in \{ t \} )
		= \mathbb{P} (X = t)
	\]
	is \(0\). This is indeed the case since \(t \in C_{F_X}\), hence we're done.

	To show the backward direction, we need the following lemma.
	\begin{lemma}\label{lma:lec5}
		\(X_n \overset{D}{\to } X\) if and only if for all \(x \in \mathbb{R} ^d\),
		\[
			F_X(x^-)
			\leq \liminf_{n \to \infty} F_{X_n} (x^-)
			\leq \liminf_{n \to \infty} F_{X_n} (x)
			\leq \limsup_{n \to \infty} F_{X_n} (x)
			\leq F_X(x).
		\]
	\end{lemma}
	\begin{proof}
		The backward direction is clear, so we prove the forward direction. When \(x\in C_{F_X}\), we're clearly done, so consider \(x \notin C_{F_X}\). Firstly, note that \(\vert C_{F_X}^{c} \vert \) is countable, so there exists \((x_k) \nearrow x\) and \((y_k) \searrow x\), both in \(C_{F_X}\). Hence, for all \(n \geq 1\) and \(k \geq 1\),
		\[
			F_{X_n}(x_k)
			\leq F_{X_n}(x)
			\leq F_{X_n}(y_k)
		\]
		as \(F_{X_n}\) is increasing. We now have for every \(k \geq 1\),
		\begin{align*}
			F_X(x_k)
			 & = \lim_{n \to \infty} F_{X_n}(x_k) \tag*{\(x_k \in C_{F_X}\)}           \\
			 & \leq \liminf_{n \to \infty} F_{X_n}(x^-)                                \\
			 & \leq \liminf_{n \to \infty} F_{X_n}(x) \tag*{\(F_{X_n}\) is increasing} \\
			 & \leq \limsup_{n \to \infty} F_{X_n}(x)                                  \\
			 & \leq \limsup_{n \to \infty} F_{X_n}(y_k)
			= F_X(y_k). \tag*{\(y_k \in C_{F_X}\)}
		\end{align*}
		By taking \(k \to \infty \), \(F_X(x_k) \to F_X(x^-)\), while \(F_X(y_k) \to F_X(x)\),\footnote{Recall that the distribution function is always right-continuous.} and we're done.
	\end{proof}
	\emph{The proof will be \hyperref[pf:thm:weak-convergence-is-convergence-in-distribution]{continued}\dots}
\end{proof}