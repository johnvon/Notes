\lecture{11}{20 Feb.\ 9:30}{More on Inference For Variance}
Let's try to generalize what we have done. Let \(Y \coloneqq X - \mu = X - \mathbb{E}_{}[X] \) (and also \(Y_i = X_i - \mu \) as usual), \(\mu _k \coloneqq \mathbb{E}_{}[Y^k] = \mathbb{E}_{}[(X - \mu )^k] \) for all \(k \geq 2\), and finally
\[
	\widetilde{\mu} _k
	= \frac{\mu _k}{\sigma ^k}
	= \mathbb{E}_{}\left[ \frac{(X-\mu )^k}{\sigma ^k} \right] .
\]

\begin{prev}
	In this notation, we proved \(\sqrt{n} (\hat{\sigma} _n^2 - \sigma ^2) \to \mathcal{N} (0, (\widetilde{\mu} _4 - 1)\sigma ^4) \), or equivalently,
	\[
		\frac{\sqrt{n} }{\sqrt{\widetilde{\mu} _4 - 1} } \left( \frac{\hat{\sigma} _n^2}{\sigma ^2} - 1 \right) \to \mathcal{N} (0, 1).
	\]
\end{prev}
As hinted before, we ask the following.

\begin{problem*}
	How to estimate \(\widetilde{\mu} _4\), or more generally, how to estimate \(\widetilde{\mu} _k\) \hyperref[def:consistent]{consistently}?
\end{problem*}

As we already know how to estimate \(\sigma \) (i.e., by \(\hat{\sigma} _n\)) \hyperref[def:consistent]{consistently}, it reduces to estimating \(\mu _k\). Consider the \emph{\(k^{\text{th} }\) sample centered moment}
\[
	M_k
	\coloneqq \frac{1}{n} \sum_{i=1}^{n} (X_i - \overline{X} _n)^k .
\]

\begin{note}
	If we know \(\mu \), then \(\frac{1}{n} \sum_{i=1}^{n} (X_i - \mu )^k \overset{p}{\to} \mu _k\) by the \hyperref[thm:WLLN]{weak law of large number}.
\end{note}

However, since we don't know \(\mu \), we need to use \(\overline{X} _n\). But this still yields a \hyperref[def:consistent]{consistent} estimator.

\begin{proposition}\label{prop:sample-centered-moment-consistent}
	If \(\mu _k < \infty \), \(M_k \overset{p}{\to} \mathbb{E}_{}[Y^k] = \mu _k\).
\end{proposition}
\begin{proof}
	Let's denote \(\overline{X} _n \eqqcolon \overline{X} \) and \(\overline{Y} _n \eqqcolon \overline{Y} \). Then
	\[
		M_k
		= \frac{1}{n} \sum_{i=1}^{n} (X_i - \overline{X} )^k
		= \frac{1}{n} \sum_{i=1}^{n} (Y_i - \overline{Y} )^k
		= \frac{1}{n} \sum_{i=1}^{n} \sum_{\ell =0}^{k} \binom{k}{\ell } Y_i^{\ell } (-\overline{Y} )^{k-\ell }
		= \sum_{\ell =0}^{k} \binom{k}{\ell } (-\overline{Y} )^{k - \ell } \frac{1}{n} \sum_{i=1}^{n} Y_i^{\ell }.
	\]
	Let \(\frac{1}{n} \sum_{i=1}^{n} Y_i^{\ell } \eqqcolon \overline{Y^{\ell } } \), then we further get
	\[
		M_k
		= \sum_{\ell =0}^{k} \binom{k}{\ell } (- \overline{Y} )^{k - \ell } \overline{Y^{\ell } }
		= \overline{Y^k} + \sum_{\ell =0}^{k-1} \binom{k}{\ell } (-\overline{Y} )^{k-\ell }  \overline{Y^{\ell } }.
	\]
	By the \hyperref[thm:WLLN]{weak law of large number}, \(\overline{Y^k} \overset{p}{\to} \mathbb{E}_{}[Y^k] = \mu _k\) and \((-\overline{Y} )^{k-\ell } \overset{p}{\to} 0\) since \(-\overline{Y} \overset{p}{\to} 0\) with \hyperref[thm:continuous-mapping]{continuous mapping theorem}, hence \(M_k \overset{p}{\to} \mu _k\) by \hyperref[col:Slutsky]{Slutsky's theorem}.
\end{proof}

Furthermore, we can also ask for the asymptotic distribution of \(M_k\), i.e., \(\sqrt{n} (M_k - \mu _k)\). Firstly,
\[
	\sqrt{n} (M_k - \mu _k)
	= \sqrt{n} (\overline{Y^k} - \mu _k ) + \sum_{\ell =0}^{k-1} \binom{k}{\ell } (- \overline{Y} )^{k-\ell } \overline{Y^{\ell } } \sqrt{n}
	= \sqrt{n} (\overline{Y^k} - \mu _k ) + \sum_{\ell =0}^{k-1} \binom{k}{\ell } \frac{(- \overline{Y} \sqrt{n} )^{k-\ell }}{\sqrt{n} ^{k - \ell - 1}} \overline{Y^{\ell } }.
\]
We see that
\begin{itemize}
	\item \(\overline{Y} \sqrt{n} \) is asymptotically normal, hence is \hyperref[def:bounded-in-probability]{bounded in probability} from \autoref{prop:convergence-in-distirbution-bounded-in-probability};
	\item \(\overline{Y^{\ell } } \overset{p}{\to} \mathbb{E}_{}[Y^{\ell } ] = O(1)\);
	\item \(1 / \sqrt{n} ^{k - \ell - 1} = o(1)\) for \(\ell < k-1\).
\end{itemize}
Combining, every term in the summation is \(O(1) O_p(1) o(1) = o_p(1)\) except for \(\ell = k-1\). This means
\begin{align*}
	\sqrt{n} (M_k - \mu _k)
	 & = \sqrt{n} (\overline{Y^k} - \mu _k) - \binom{k}{k-1} \overline{Y^{k-1}} \sqrt{n} \overline{Y} + \sum_{\ell =0}^{k-2} \binom{k}{\ell } o_p(1)               \\
	 & = \sqrt{n} (\overline{Y^k} - \mu _k) - k \overline{Y^{k-1}} \sqrt{n} \overline{Y} + o_p(1)                                                                  \\
	\shortintertext{The problem is that although \(\sqrt{n} \overline{Y} = O_p(1)\), \(\overline{Y^{k-1}} \) is not \(o_p(1)\). By replacing \(\overline{Y^{k-1} } \) by \(\overline{Y^{k-1}} - \mu _{k-1} + \mu _{k-1}\),}
	 & = \sqrt{n} (\overline{Y^k} - \mu _k) - k \left( \overline{Y^{k-1}} - \mu _{k-1} \right) \sqrt{n} \overline{Y} - k \mu _{k-1} \sqrt{n} \overline{Y} + o_p(1) \\
	 & = \sqrt{n} (\overline{Y^k} - \mu _k) - k \mu _{k-1} \sqrt{n} \overline{Y} + o_p(1)                                                                          \\
	\shortintertext{since \(\overline{Y^{k-1}} - \mu _{k-1} \overset{p}{\to} 0\) from the \hyperref[thm:WLLN]{weak law of large number}, finally,}
	 & =\sqrt{n} \left( \frac{1}{n} \sum_{i=1}^{n} (Y_i^k - \mu _k) \right)  - k \mu _{k-1} \frac{1}{n} \sqrt{n} \sum_{i=1}^{n} Y_i  + o_p(1)                      \\
	 & = \frac{1}{\sqrt{n} } \sum_{i=1}^{n} \left( Y_i^k - \mu _k - k \mu _{k-1} Y_i \right) + o_p(1).
\end{align*}
Observe that \(Y_i^k - \mu _k - k \mu _{k-1} Y_i\)'s are i.i.d., the whole thing converges to \(\mathcal{N} (0, \Var_{}\left[Y^k - \mu _k - k \mu _k Y\right] )\) \hyperref[def:converge-in-distribution]{in distribution} by \hyperref[thm:CLT]{central limit theorem}, where the variance can be calculated as
\[
	\begin{split}
		\Var_{}\left[Y^k - \mu _k - k \mu _{k-1} Y\right]
		 & = \Var_{}\left[Y^k - k \mu _{k-1} Y\right]                                      \\
		 & = \Var_{}[Y^k] + k^2 \mu _{k-1}^2 \Var_{}[Y] - 2 k \mu _{k-1} \Cov_{}[Y, Y^k]   \\
		 & = \mu _{2k} - \mu _k^2 + k^2 \mu _{k-1}^2 \sigma ^2 - 2 k \mu _{k-1} \mu _{k+1}
	\end{split}
\]
since \(\mathbb{E}_{}[Y] = 0\), \(\Var_{}[Y] = \sigma ^2\), and \(\Cov_{}[Y, Y^k] = \mathbb{E}_{}[Y \cdot Y^k] - \mathbb{E}_{}[Y] \mathbb{E}_{}[Y^k] = \mathbb{E}_{}[Y^{k+1}] = \mu _{k+1}\).

\subsection{Testing Normality}
Let \(H_0 \colon X \sim \mathcal{N} (\mu , \sigma ^2)\) for some \(\mu , \sigma ^2\). Since Gaussian is symmetric, this implies that \(\mu _k = 0\) (and hence \(\widetilde{\mu} _k = 0\)) for all odd \(k\).

\begin{intuition}
	With \autoref{prop:sample-centered-moment-consistent}, the idea is that to reject \(H_0\) if \(\vert M_k \vert \) (or \(\vert M_k / \hat{\sigma} ^k \vert \)) is ``large''.
\end{intuition}

\begin{prev}
	Previously we have \(\sqrt{n} (M_k - \mu _k) \overset{D}{\to} \mathcal{N} (0, \Var_{}\left[Y^k - k \mu _{k-1} Y\right] )\).
\end{prev}

Formally, suppose \(k\) is odd, and \(X \sim \mathcal{N} (\mu , \sigma ^2)\), then \(\mu _k = 0\), hence
\[
	\sqrt{n} (M_k - \mu _k) \overset{D}{\to} \mathcal{N} (0, \Var_{}\left[Y^k - k \mu _{k-1} Y\right] )
	\implies \sqrt{n} \frac{M_k}{\sigma ^k} \overset{D}{\to} \mathcal{N} (0, \sigma ^{-2k} \Var_{}\left[Y^k - k \mu _{k-1} Y\right] )
\]
By \hyperref[col:Slutsky]{Slutsky's theorem}, \(\sqrt{n} M_k / \hat{\sigma} ^k\) also \hyperref[def:converge-in-distribution]{converges} to this normal. Now, in particular, if \(k = 3\), we have
\[
	\sqrt{n} \frac{M_3}{\hat{\sigma} ^3}
	\overset{D}{\to} \mathcal{N} (0, \sigma ^{-6} \Var_{}[Y^3 - 3 \sigma ^2 Y] )
	= \mathcal{N} (0, \sigma ^{-6} \left( \Var_{}[Y^3] + 9 \sigma ^4 \sigma ^2 - 6 \sigma ^2 \mathbb{E}_{}[Y^4] \right) )
\]
where  \(\mu _2 = \sigma ^2\) and \(\Cov_{}\left[Y^3 , Y\right] = \mathbb{E}_{}[Y^4] - \mathbb{E}_{}[Y] \mathbb{E}_{}[Y^3] = \mathbb{E}_{}[Y^4] \). We note the following.

\begin{note}
	For odd \(k\), \(\Var_{}[Y^k] = \mathbb{E}_{}[Y^{2k} ] - (\mathbb{E}_{}[Y^k] )^2 = \mathbb{E}_{}[Y^{2k} ] = \mu _{2k}\) since \((\mathbb{E}_{}[Y^k] )^2 = \mu _k^2 = 0\).
\end{note}

Hence, by plugging \(\Var_{}[Y^3] = \mu _{2 \times 3} = \mu _6\), the variance of the normal is further equal to
\[
	\frac{\mu _6 + 9 \sigma ^6 - 6 \sigma ^2 \mu _4}{\sigma ^6}
	= \widetilde{\mu} _6 + 9 - 6 \widetilde{\mu} _4
	= 15 + 9 - 6 \times 3
	= 6,
\]
i.e.,
\[
	\sqrt{\frac{n}{6}} \frac{M_3}{\hat{\sigma} _3}
	\overset{D}{\to} \mathcal{N} (0, 1).
\]

For even \(k\), we really need to work with \(\vert M_k / \hat{\sigma} _k \vert \). Since we know the asymptotic distribution of both \(M_k\) and \(\hat{\sigma} _k\), the joint can be considered by \(g(M_k , \hat{\sigma} _2) = \vert M_k / \hat{\sigma} ^k \vert \) with the \hyperref[thm:delta-method]{delta method}.

\subsection{Multivariate Central Limit Theorem}
\begin{theorem}[Cramér-Wold device]\label{thm:Cramer-Wold-device}
	Let \((X_n)\) be a sequence of random vectors and \(X\) be a random vector in \(\mathbb{R} ^d\). Then \(X_n \overset{D}{\to} X\) if and only if \(t \cdot X_n \overset{D}{\to} t \cdot X\) for every \(t \in \mathbb{R} ^d\).
\end{theorem}
\begin{proof}
	The forward direction is clear from \hyperref[thm:continuous-mapping]{continuous mapping theorem} for the linear functional induced from \(t\). For the backward direction, assume that \(t \cdot X_n \overset{D}{\to} t \cdot X\). Then
	\[
		\phi _{X_n}(t)
		= \mathbb{E}_{}[e^{i t \cdot X_n}]
		= \phi _{t \cdot X_n}(1)
		\to \phi _{t \cdot X}(1)
		= \mathbb{E}_{}[e^{i t \cdot X}]
		= \phi _X(t),
	\]
	which implies \(X_n \overset{D}{\to} X\) by the \hyperref[thm:Levy-Cramer-continuity]{Lévy-Cramer continuity theorem}.
\end{proof}

\begin{remark}
	Proving \(X_n \overset{D}{\to} X\) reduces to proving something in the scalar case.
\end{remark}

\begin{theorem}[Multivariate central limit theorem]\label{thm:multivariate-CLT}
	Let \((X_n)\) be i.i.d. random vectors in \(\mathbb{R} ^d\) with \(\mathbb{E}_{}[X_i] = \mu \), \(\Var_{}[X_i] = \Sigma \) for all \(1 \leq i \leq n\). Then
	\[
		\frac{1}{\sqrt{n} } \sum_{i=1}^{n} (X_i - \mu )
		\overset{D}{\to} \mathcal{N} (0, \Sigma ).
	\]
\end{theorem}