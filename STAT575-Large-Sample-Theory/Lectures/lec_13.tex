\lecture{13}{27 Feb.\ 9:30}{Bahadur's Representation Population Quantiles}
\section{Inference for Population Quantiles}
Let \(X, X_1, \dots , X_n \overset{\text{i.i.d.} }{\sim } F\) for some distribution function \(F\), and let \(\theta _p\) for some \(p \in (0, 1)\) be the \hyperref[def:quantile-function]{\(p^{\text{th} }\) quantile}, which we recall is defined as \(F^{-1} (p) = \inf \{ t \in \mathbb{R} \colon F(t) \geq p \} \).

\begin{intuition}
	Since \(F^{-1} (p)\) depends on \(F\), if we have an estimation of \(F\) itself, then we can have an estimation of \(F^{-1} (p)\).
\end{intuition}

Specifically, to estimate \(F\), consider the empirical cdf
\[
	\hat{F} _n(t) = \frac{1}{n} \sum_{i=1}^{n} \mathbbm{1}_{X_i \leq t}
\]
for all \(t \in \mathbb{R} \), and by \hyperref[thm:WLLN]{weak law of large number}, \(\hat{F} _n(t) \overset{p}{\to} \mathbb{P} (X \leq t) = F(t)\), i.e., for all \(\epsilon > 0\), \(\mathbb{P} (\vert \hat{F} _n(t) - F(t) \vert > \epsilon ) \to 0\). We note the following.

\begin{note}
	By fixing \(t\), \(\mathbbm{1}_{X \leq t} \) is \(\operatorname{Ber}(F(t)) \), hence \(\sqrt{n} (\hat{F} _n(t) - F(t)) \overset{D}{\to} \mathcal{N} (0, F(t) (1 - F(t)))\).
\end{note}

\begin{note}
	\href{https://en.wikipedia.org/wiki/Hoeffding%27s_inequality}{Hoeffding's inequality} implies that this sum of Bernoulli's converges exponentially fast.
\end{note}

Now, let's estimate \(\theta _p = F^{-1} (p)\) by
\[
	\hat{\theta} _p
	\coloneqq \hat{F} _n ^{-1} (p)
	\coloneqq \inf \{ t \in \mathbb{R} \colon \hat{F} _n(t) \geq p \}
	= \inf \left\{ t \in \mathbb{R} \colon \sum_{i=1}^{n} \mathbbm{1}_{X_i \leq t} \geq \lceil np \rceil \right\}
	= X_{(\lceil np \rceil )}
\]
since \(\sum_{i=1}^{n} \mathbbm{1}_{X_i \geq t} \in \mathbb{N} \).

\begin{notation}[Order statistic]
	The \emph{order statistics} of \(X_i\)'s is defined as \(X_{(1)} \leq \dots \leq X_{(n)}\).
\end{notation}

\begin{prev}
	\(t \geq F^{-1} (p) \iff F(t) \geq p\) and \(t < F^{-1} (p) \iff F(t) < p\). This is also true for \(\hat{F} _n\).
\end{prev}

\begin{theorem}
	If \(F(\theta _p + \epsilon ) > F(\theta _p) \geq p\) for any \(\epsilon > 0\), then \(\hat{\theta} _p \overset{p}{\to} \theta _p\). More generally, if \(p_n \to p\), then \(\hat{\theta} _{p_n} \overset{p}{\to} \theta _p\).
\end{theorem}
\begin{proof}
	We want to show that for any \(\epsilon > 0\), \(\mathbb{P} (\vert \hat{\theta} _{p_n} - \theta _p \vert > \epsilon ) \to 0\). We see that
	\[
		\mathbb{P} (\vert \hat{\theta} _{p_n} - \theta _p \vert > \epsilon )
		= \mathbb{P} (\hat{\theta} _{p_n} > \theta _p + \epsilon ) + \mathbb{P} (\hat{\theta} _{p_n} < \theta _p - \epsilon ).
	\]
	For the first term, \(\hat{\theta} _{p_n} = \hat{F} _n^{-1} (p_n) > \theta + \epsilon \), hence \(p_n > \hat{F} _n(\theta _p + \epsilon )\), i.e.,
	\[
		p_n - p + p  - F(\theta _p + \epsilon )
		> \hat{F} _n(\theta _p + \epsilon ) - F(\theta _p + \epsilon ).
	\]
	Since \(p < F(\theta _p + \epsilon )\), let \(- \delta \coloneqq p - F(\theta _p + \epsilon )\) for some \(\delta > 0\), then
	\[
		\hat{F} _n(\theta _p + \epsilon ) - F(\theta _p + \epsilon )
		< p_n - p - \delta
		< \frac{\delta}{2} - \delta
		= -\frac{\delta}{2}
	\]
	for large enough \(n\) such that \(\vert p_n - p \vert < \delta / 2\). Hence, \(\vert \hat{F} _n(\theta _p + \epsilon ) - F(\theta _p + \epsilon ) \vert > \delta / 2\), i.e.,
	\[
		\mathbb{P} (\hat{\theta} _{p_n} > \theta + \epsilon )
		\leq \mathbb{P} (\vert \hat{F} _n(\theta _p + \epsilon ) - F(\theta _p + \epsilon ) \vert > \delta / 2),
	\]
	which goes to \(0\) as we have seen. The second term can be proved similarly.
\end{proof}

\subsection{Bahadur's Representation Thoerem}
If \(F\) is differentiable, we can actually say more in this situation.

\begin{theorem}[Bahadur's representation]\label{thm:Bahadur-representation}
	If \(F^{\prime} (\theta _p) \coloneqq f(\theta _p) > 0\) and \(\sqrt{n} (p_n - p) = O(1)\), then
	\[
		\sqrt{n} (\hat{\theta} _{p_n} - \theta _p)
		= \frac{1}{\sqrt{n} } \sum_{i=1}^{n} \frac{p_n - \mathbbm{1}_{X_i \leq \theta _p}}{f(\theta _p)} + o_p(1).
	\]
\end{theorem}

Let's postpone the \hyperref[pf:Bahadur-representation]{proof} and discuss its implication first. In the case of \(p_n \neq p\), \hyperref[thm:Bahadur-representation]{Bahadur's representation} shows
\[
	\sqrt{n} (\hat{\theta} _{p_n} - \theta _p)
	= \frac{1}{\sqrt{n} } \sum_{i=1}^{n} \frac{p - \mathbbm{1}_{X_i \leq \theta _p} }{f(\theta _p)} + \frac{\sqrt{n} (p_n - p)}{f(\theta _p)} + o_p(1).
\]
We see that if \(\sqrt{n} (p_n - p) \to c\), then
\[
	\sqrt{n} (\hat{\theta} _{p_n} - \hat{\theta} _p)
	= \sqrt{n} \left( (\hat{\theta} _{p_n} - \theta _p) - (\hat{\theta} _p - \theta _p) \right)
	= \sqrt{n} \frac{p_n - p}{f(\theta _p)} + o_p(1)
	\overset{p}{\to} \frac{c}{f(\theta _p)}.
\]
On the other hand, if \(\sqrt{n} (p_n - p) \to 0\), then
\[
	\sqrt{n} (\hat{\theta} _{p_n} - \theta _p)
	\overset{D}{\to} \mathcal{N} \left( 0, \frac{F(\theta _p) (1 - F(\theta _p))}{f^2(\theta _p)} \right)
	= \mathcal{N} \left( 0, \frac{p (1 - p)}{f^2(\theta _p)} \right) ,
\]
which gives an asymptotically valid \(100(1 - \alpha )\%\) confidence interval for \(\theta _p\) as
\[
	\hat{\theta} _{p_n} \pm Z_{\alpha / 2} \frac{\sqrt{p(1 - p)} }{\sqrt{n} f(\theta _p)}.
\]

\begin{intuition}
	This is expected since if the density is low, then we don't have many data to evaluate \(\theta _p\) in the first place, hence the precision will be low (large variance).
\end{intuition}

However, to implement this confidence interval, we need to estimate \(f(\theta _p)\) \hyperref[def:consistent]{consistently}. On the other hand, to avoid this, consider a sequence of intervals \((\hat{\theta} _{\ell _n}, \hat{\theta} _{u_n})\) for some \(\ell _n < p_n < u_n\) such that
\[
	\hat{\theta} _{\ell _n}
	\to \hat{\theta} _p - Z_{\alpha / 2} \frac{\sqrt{p (1 - p)} }{\sqrt{n} f(\theta _p)}
	\text{ and }
	\hat{\theta} _{u_n}
	\to \hat{\theta} _p + Z_{\alpha / 2} \frac{\sqrt{p (1 - p)} }{\sqrt{n} f(\theta _p)}.
\]
Consider \(\hat{\theta} _{\ell _n}\) first. It's equivalent to say
\[
	\sqrt{n} (\hat{\theta} _{\ell _n} - \hat{\theta} _p)
	\to -Z_{\alpha / 2} \frac{\sqrt{p (1 - p)} }{f(\theta _p)},
\]
and if \(\sqrt{n} (\ell _n - p) \to c\), e.g., \(\ell _n = p + c / \sqrt{n} \), then
\[
	\sqrt{n} (\hat{\theta} _{\ell _n} - \hat{\theta} _p) \overset{p}{\to} \frac{c}{f(\theta _p)}.
\]
This suggests \(\ell _n = p - Z_{\alpha / 2} \sqrt{p (1 - p)} / \sqrt{n} \), and similarly, \(u_n = p + Z_{\alpha / 2} \sqrt{p (1 - p)} / \sqrt{n} \). Another implication is the following. Firstly, consider \(p = 1 / 2\), and in this case,
\[
	\sqrt{n} (\hat{\theta} _{1 / 2} - \theta _{1 / 2})
	\overset{D}{\to} \mathcal{N} \left( 0, \frac{1}{4 f^2(\theta _{1 / 2})} \right) .
\]

\begin{definition}[Median]\label{def:median}
	When \(p = 1 / 2\), \(\theta _{1 / 2}\) is called the \emph{median}.
\end{definition}

Suppose further, \(\theta _{1 / 2} = \mu \) and \(\Var_{}[X] = \sigma ^2 < \infty \). Then both \(\hat{\theta} _{1 / 2}\) and \(\overline{X} _n\) are two possible estimators of \(\mu \), and in this case, we might want to look at the \hyperref[def:asymptotic-relative-efficiency]{asymptotic relative efficiency}. Specifically,
\[
	\operatorname{ARE}(\overline{X} _n , \hat{\theta} _{1 / 2})
	= \frac{\sigma ^2}{\frac{1}{4 f^2(\theta _{1 / 2})}}
	= 4 \sigma ^2 f^2(\mu ).
\]

\begin{eg}
	If \(X \sim \mathcal{N} (\mu , \sigma ^2)\), then \(\overline{X} \) is a better estimator of \(\mu \) than \(\hat{\theta} _{1 / 2}\).
\end{eg}
\begin{explanation}
	Since \(f(x) = \frac{1}{\sigma \sqrt{2\pi } } e^{- \frac{(x - \mu )^2}{2 \sigma ^2}}\), hence \(f(\mu ) = 1 / \sigma \sqrt{2\pi } \), i.e.,
	\[
		\operatorname{ARE}(\overline{X} _n, \hat{\theta} _{1 / 2})
		= 4 \sigma ^2 \frac{1}{\sigma ^2 2\pi }
		= \frac{2}{\pi }
		< 1.
	\]
\end{explanation}

\begin{eg}
	If \(X \sim \operatorname{Laplace}(\mu , b) \) where \(\sigma ^2 = 2b^2\), then \(\hat{\theta} _{1 / 2}\) is a better estimator of \(\mu \) than \(\overline{X} \).
\end{eg}
\begin{explanation}
	Since \(f(x) = \frac{1}{2b} e^{- \frac{\vert x - \mu \vert }{b}} = \frac{1}{\sigma \sqrt{2} } e^{- \frac{\vert x - \mu \vert }{\sqrt{2} \sigma }}\), hence \(f(\mu ) = 1 / \sigma \sqrt{2} \), i.e.,
	\[
		\operatorname{ARE}(\overline{X} _n, \hat{\theta} _{1 / 2})
		= 4 \sigma ^2 \frac{1}{2 \sigma ^2}
		= 2 > 1.
	\]
\end{explanation}

One might want to consider \(c \overline{X} + (1 - c)\hat{\theta} _{1 / 2}\) for any \(c \in [0, 1]\). In this case, by \hyperref[thm:Bahadur-representation]{Bahadur's representation}, \hyperref[thm:delta-method]{delta method}, one can have
\[
	\sqrt{n} \left( (c \overline{X} + (1 - c)\hat{\theta} _{1 / 2}) - \mu \right)
	\overset{D}{\to} \mathcal{N} (0, V)
\]
where
\[
	V = c^2 \Var_{}[X] + (1 - c)^2 \frac{1}{4 f^2(\mu )} + 2c(1 - c) \Cov_{}\left[X - \mu , \frac{1 / 2 - \mathbbm{1}_{X \leq \mu } }{f(\mu )}\right].
\]

\begin{remark}
	\hyperref[thm:Bahadur-representation]{Bahadur's representation} is needed since if only have their asymptotic distribution, their joint is not necessary asymptotically independent, hence we really need their representations.
\end{remark}