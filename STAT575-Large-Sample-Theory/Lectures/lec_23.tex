\lecture{23}{11 Apr.\ 9:30}{Asymptotically Normality of Linear Rank Statistics}
We will first work with \(\alpha _N(i) = \mathbb{E}_{}[\phi (U_{N(i)})] \), where we recall that \(\alpha _N(R_{Ni}) = \mathbb{E}_{}[\phi (U_i) \mid \widetilde{R} _N]\).

\begin{proposition}
	For every \(N \geq 1\), we have \(T_N - \mathbb{E}_{}[T_N] = \mathbb{E}_{}[\widetilde{T} _N \mid \widetilde{R} _N] \) where
	\[
		\widetilde{T} _N
		\coloneqq \sum_{i=1}^{N} (c_{Ni} - \overline{c} _N) \phi (U_i).
	\]
\end{proposition}
\begin{proof}
	It'll be convenient to consider
	\[
		T_N
		= \sum_{i=1}^{N} (c_{Ni} - \overline{c} _N) \alpha _N(R_{Ni}) + \overline{c} _N \sum_{i=1}^{N} \alpha _N(R_{Ni})
		= \sum_{i=1}^{N} (c_{Ni} - \overline{c} _N) \alpha _N(R_{Ni}) + N \overline{c} _N \overline{\alpha} _N,
	\]
	with the fact that \(\mathbb{E}_{}[T_N] = N \overline{c} _N \overline{\alpha} _N\), we see that
	\begin{align*}
		T_N - \mathbb{E}_{}[T_N]
		 & = \sum_{i=1}^{N} (c_{Ni} - \overline{c} _N) \alpha _N(R_{Ni})                               \\
		\shortintertext{and since \(\alpha _N(R_{Ni}) = \mathbb{E}_{}[\phi (U_i) \mid \widetilde{R} _N]\), we further have}
		 & = \sum_{i=1}^{N} (c_{Ni} - \overline{c} _N) \mathbb{E}_{}[\phi (U_i) \mid \widetilde{R} _N]
		= \mathbb{E}_{}\left[\sum_{i=1}^{N} (c_{Ni} - \overline{c} _N) \phi (U_i) \mid \widetilde{R} _N\right]
		\eqqcolon \mathbb{E}_{}[\widetilde{T} _N \mid \widetilde{R} _N]
	\end{align*}
	where we let \(\widetilde{T} _N \coloneqq \sum_{i=1}^{N} (c_{Ni} - \overline{c} _N) \phi (U_i)\).
\end{proof}

To see how can we obtain asymptotic normality by the theory of \hyperref[def:projection]{projection}, observe the following.

\begin{claim}\label{clm:simple-linear-rank-statistic-asymptotic-normality}
	We can easily have asymptotically normality for \(\widetilde{T} _N\).
\end{claim}
\begin{explanation}
	We see that by the \hyperref[thm:Hajek-Sidak-CLT]{Hajek-Sidak central limit theorem}, if \(\phi (U_i)\)'s are i.i.d.\ such that
	\[
		0 < \mathbb{E}_{}[\phi ^2(U)] = \int_{0}^{1} \phi ^2(u) \,\mathrm{d}u < \infty, \text{ and }
		\max _{1 \leq i \leq N} \frac{(c_{Ni} - \overline{c} _N)^2}{\sum_{j=1}^{N} (c_{Nj} - \overline{c} _N)^2} \to 0,
	\]
	then \(\widetilde{T} _N / \sqrt{\Var_{}[\widetilde{T} _N] } \overset{D}{\to} \mathcal{N} (0, 1)\) as \(\mathbb{E}_{}[\widetilde{T} _N] = 0 \).
\end{explanation}

Applying the \hyperref[def:projection]{projection} theory we have developed, we have the following.

\begin{theorem}\label{thm:simple-linear-rank-statistic}
	Suppose that \((c_{Ni})\) satisfy \(\max _{1 \leq i \leq N} (c_{Ni} - \overline{c} _N)^2 / \sum_{i=1}^{N} (c_{Ni} - \overline{c} _N)^2 \to 0\) as \(N \to \infty \). If \(\Var_{}[\phi (U)] \in (0, \infty )\), then
	\[
		\frac{T_N - \mathbb{E}_{}[T_N] }{\sqrt{\Var_{}[T_N] } }
		\overset{D}{\to} \mathcal{N} (0, 1).
	\]
\end{theorem}
\begin{proof}
	From \autoref{col:projection} and the above \hyperref[clm:simple-linear-rank-statistic-asymptotic-normality]{claim}, it suffices to show that \(\Var_{}[T_N] / \Var_{}[\widetilde{T} _N] \to 1\) as \(N \to \infty \). Recall that \(\Var_{}[T_N] = \frac{N^2}{N-1} \sigma _{N \alpha }^2 \alpha _{Nc}^2\), furthermore, \(\Var_{}[\widetilde{T} _N] = N \sigma _{Nc}^2 \Var_{}[\phi (U_1)] \) since
	\[
		\Var_{}[\widetilde{T} _N]
		= \Var_{}[\phi (U_1)] \cdot \sum_{i=1}^{N} (c_{Ni} - \overline{c} _N)^2
		= N \sigma _{Nc}^2 \Var_{}[\phi (U)] ,
	\]
	so it suffices to show \(\sigma _{N \alpha }^2 = \Var_{}[\alpha _N(R_{N1})] \to \Var_{}[\phi (U_1)] \).

	\begin{note}
		To show \(\Var_{}[X] \to \Var_{}[Y] \), it suffices to show \(X \overset{L^2}{\to} Y \) since it implies convergence for both the first and second moments, hence the variance
	\end{note}

	In particular, it reduces to show \(\alpha _N(R_{N1}) = \mathbb{E}_{}[\phi (U_1) \mid \widetilde{R} _N] \overset{L^2}{\to} \phi (U_1)\). Firstly, we write
	\[
		\mathbb{E}_{}[\phi (U_1) \mid \widetilde{R} _N]
		= \mathbb{E}_{}[\phi (U_1) \mid \widetilde{R} _1, \dots , \widetilde{R} _N]
	\]
	as the condition on the right-hand side is equivalent to \(\widetilde{R} _N, U_2, \dots , U_N\), and \(U_1\) is independent of \(U_2, \dots , U_N\). From the martingale limit theorem, we have
	\[
		\mathbb{E}_{}[\phi (U_1) \mid \widetilde{R} _1, \dots , \widetilde{R} _N]
		\overset{\text{a.s.}}{\to} \mathbb{E}_{}[\phi (U_1) \mid \widetilde{R} _N, N \geq 1] ,
	\]
	which will equal to \(\phi (U_1)\) if \(\phi (U_1)\) is a function of the conditions. Hence, it remains to show that \(U_1\) is a measurable function of \(\{ \widetilde{R} _N \}_{N \geq 1}\).

	\begin{claim}\label{clm:simple-linear-rank-statistic}
		Knowing the first components of \(\widetilde{R} _N\) for every \(N\geq 1\), i.e., \(R_{N1}\), determines \(U_1\) (in \(L^2\)).
	\end{claim}
	\begin{explanation}
		We observe that \(\mathbb{E}_{}[U_1 \mid R_{N1}] = R_{N1} / (N+1)\), i.e., the expectation of \(U_{(R_{N1})}\). Hence, by \autoref{col:projection}, \(U_1 - R_{N1} / (N+1) \overset{L^2}{\to} 0\) if the ratio between variances converges to \(1\). Indeed,
		\[
			\cfrac{\Var_{}[U_1] }{\Var_{}\left[\cfrac{R_{N1}}{N+1}\right] }
			= \cfrac{1 / 12}{\cfrac{1}{(N+1)^2}\Var_{}[R_{N1}] }
			= \cfrac{1 / 12}{\cfrac{1}{(N+1)^2} \cfrac{N^2 - 1}{12}}
			= \cfrac{N+1}{N - 1}
			\to 1
		\]
		since \(R_{N1} \sim \mathcal{U} ([N])\), hence we're done.
	\end{explanation}
	Finally, note that the limits in \(L^2\) are unique up to a null set, hence we're done.
\end{proof}

We now discuss the connection between \(T_N\) and \(T_N^{\prime} \). Firstly, recall that
\[
	T_N = \sum_{i=1}^{N} c_{Ni} \alpha _N(R_{Ni}), \text{ and }
	T_N^{\prime} = \sum_{i=1}^{N} c_{Ni} \alpha _N^{\prime} (R_{Ni})
\]
where \(\alpha _{Ni} = \mathbb{E}_{}[\phi (U_{N(i)})] \) and \(\alpha _{Ni}^{\prime} \coloneqq \phi (\mathbb{E}_{}[U_{N(i)}] ) = \phi (i / (N+1))\).

\begin{theorem}\label{thm:simple-linear-rank-statistic-2}
	Suppose that \((c_{Ni})\) satisfy \(\max _{1 \leq i \leq N} (c_{Ni} - \overline{c} _N)^2 / \sum_{i=1}^{N} (c_{Ni} - \overline{c} _N)^2 \to 0\) as \(N \to \infty \). If \(\Var_{}[\phi (U)] \in (0, \infty )\), and in addition, if \(\phi \) is almost surely continuous and
	\[
		\limsup_{n \to \infty} \frac{1}{N} \sum_{i=1}^{N} \phi ^2\left( \frac{i}{N+1} \right)
		\leq \int_{0}^{1} \phi ^2(u) \,\mathrm{d}u,
	\]
	then
	\[
		\frac{T_N^{\prime} - \mathbb{E}_{}[T_N^{\prime} ] }{\sqrt{\Var_{}[T_N] } }
		\overset{D}{\to} \mathcal{N} (0, 1).
	\]
\end{theorem}
\begin{proof}
	From \autoref{thm:simple-linear-rank-statistic}, to show that \(T_N^{\prime} \) is asymptotically normal as \(T_N\), it suffices to show
	\[
		\frac{T_N - \mathbb{E}_{}[T_N] }{\sqrt{\Var_{}[T_N] } } - \frac{T_N^{\prime} - \mathbb{E}_{}[T_N^{\prime} ] }{\sqrt{\Var_{}[T_N] } }
		\overset{L^2}{\to} 0
		\iff \frac{\Var_{}[T_N - T_N^{\prime} ] }{\Var_{}[T_N]}
		\to 0
	\]
	Firstly, recall that \(\Var_{}[T_N] = \frac{N^2}{N-1} \sigma _{Nc}^2 \sigma _{N \alpha }^2\), so \(\Var_{}[T_N - T_N^{\prime} ] = \frac{N^2}{N-1} \sigma _{Nc}^2 \sigma _{N(\alpha - \alpha ^{\prime} )}^2\) since we can write
	\[
		T_N - T_N^{\prime}
		= \sum_{i=1}^{N} c_{Ni} (\alpha _N - \alpha _N^{\prime} )(R_{Ni}),
	\]
	which is again a \hyperref[def:simple-linrea-rank-statistic]{simple linear rank statistic}, so the same calculation applies. Hence, it suffices to show \(\sigma _{N(\alpha - \alpha ^{\prime} )}^2 / \sigma _{N \alpha }^2 \to 0\). Recall what we have already shown earlier.

	\begin{prev}
		In the proof of \autoref{thm:simple-linear-rank-statistic}, we have \(\sigma _{N \alpha }^2 \to \Var_{}[\phi (U_1)] > 0\).
	\end{prev}

	Hence, we just need to show that \(\sigma _{N(\alpha - \alpha ^{\prime} )}^2 = \Var_{}[(\alpha _N - \alpha _N^{\prime} )(R_{N1})] \to 0\). From the same reason as before, it suffices to show that \((\alpha _N - \alpha _N^{\prime} )(R_{N1}) \overset{L^2}{\to} 0\), i.e.,
	\[
		\mathbb{E}_{}[\phi (U_1) \mid \widetilde{R} _N] - \phi \left( \frac{R_{N1}}{N+1} \right)
		\overset{L^2}{\to} 0.
	\]
	Let's again recall what we have shown earlier.

	\begin{prev}
		In the proof of \autoref{thm:simple-linear-rank-statistic}, we have \(\mathbb{E}_{}[\phi (U_1) \mid \widetilde{R} _N] \overset{L^2}{\to} \phi (U_1)\).
	\end{prev}

	Hence, it reduces to show \(\phi ( R_{N1} / (N+1) ) \overset{L^2}{\to} \phi (U_1)\).

	\begin{intuition}
		If \(\phi = \id \), then we have showed that \(R_{N1} / (N+1) \overset{L^2}{\to} U_1\) in the \hyperref[clm:simple-linear-rank-statistic]{previous claim}.
	\end{intuition}

	Hence, \(R_{N1} / (N+1) \overset{p}{\to} U_1\). As \(\phi \) is assumed to be continuous almost surely, we know that
	\[
		\phi \left( \frac{R_{N1}}{N+1} \right)
		\overset{p}{\to} \phi (U_1)
		\overset{?}{\implies } \phi \left( \frac{R_{N1}}{N+1} \right) \overset{L^2}{\to} \phi (U_1).
	\]
	The implication can be provided by \hyperref[thm:Scheffe]{Scheff√©'s theorem}, i.e., we only need to check
	\[
		\limsup_{N \to \infty} \mathbb{E}_{}\left[\phi ^2\left( \frac{R_{N1}}{N+1} \right) \right]
		= \limsup_{N \to \infty} \frac{1}{N} \sum_{i=1}^{N} \phi ^2 \left( \frac{i}{N+1} \right)
		\leq \mathbb{E}_{}[\phi ^2(U_1)]
		= \int_{0}^{1} \phi ^2(u) \,\mathrm{d}u
	\]
	since \(R_{N1} \sim \mathcal{U} ([N])\) as we mentioned. This is what we assumed exactly.
\end{proof}

We end this chapter by noting that the condition required in \autoref{thm:simple-linear-rank-statistic-2}, in particular, the integral inequality, is not that hard to satisfy. The following is one example.

\begin{eg}
	The conditions required in \autoref{thm:simple-linear-rank-statistic-2} are satisfied when \(\phi \) is increasing, or, more generally, the difference of two increasing functions.
\end{eg}
\begin{explanation}
	Let's only illustrate the case of increasing functions. Observe that we can understand
	\[
		\limsup_{n \to \infty} \frac{1}{N}\sum_{i=1}^{N} \phi ^2(i / (N+1))
	\]
	in the condition of \autoref{thm:simple-linear-rank-statistic-2} as a Riemann integral. In particular,
	\begin{align*}
		\int_{0}^{1} \phi ^2(u) \,\mathrm{d}u
		 & = \lim_{N \to \infty} \sum_{i=1}^{N+1} \int_{\frac{i-1}{N+1}}^{\frac{i}{N+1}} \phi ^2 (u) \,\mathrm{d}u \\
		\shortintertext{since \(\phi \) is increasing, so is \(\phi ^2\), hence}
		 & \geq \lim_{N \to \infty} \sum_{i=1}^{N+1} \phi ^2\left( \frac{i-1}{N+1} \right) \cdot \frac{1}{N+1}
		\geq \lim_{N \to \infty} \frac{N}{N+1} \frac{1}{N} \sum_{i=1}^{N} \phi ^2\left( \frac{i}{N+1} \right),
	\end{align*}
	which is what we want.
\end{explanation}