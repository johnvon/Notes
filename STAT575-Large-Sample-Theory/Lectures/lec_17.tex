\lecture{17}{21 Mar.\ 9:30}{Rank Test and Two-Sample Problem}
\begin{proof}
	Firstly, we see that \(\Var_{}[S_n] = \sum_{j=1}^{k_n} c_{nj}^2 \sigma ^2\), hence with our usual notation, we define
	\[
		Y_{nj}
		\coloneqq \frac{c_{nj} (Z_j - \mu )}{\sqrt{\sum_{i=1}^{k_n} c_{ni}^2} \sigma }
		\eqqcolon \frac{c_{nj}}{\sqrt{\sum_{i=1}^{k_n} c_{ni}^2} } W_j,
	\]
	where \(W_j \coloneqq (Z_j - \mu ) / \sigma \). Then, for any \(\epsilon > 0\), we can check that \hyperref[def:Lindeberg-condition]{Lindeberg condition} as
	\begin{align*}
		\sum_{j=1}^{k_n} \mathbb{E}_{}[Y_{nj}^2 \mathbbm{1}_{\vert Y_{nj} \vert > \epsilon } ]
		 & = \sum_{j=1}^{k_n} \frac{c_{nj}^2}{\sum_{i=1}^{k_n} c_{ni}^2} \mathbb{E}_{}\left[ W_j^2 \mathbbm{1}_{\frac{\vert c_{nj} \vert }{\sqrt{\sum_{i=1}^{k_n} c_{ni}^2} } \vert W_j \vert > \epsilon } \right]                               \\
		\shortintertext{since the only dependence of \(j\) in the expectation is \(\vert c_{nj} \vert \) in the indicator,}
		 & \leq \sum_{j=1}^{k_n} \frac{c_{nj}^2}{\sum_{i=1}^{k_n} c_{ni}^2} \mathbb{E}_{}\left[ W_j^2 \mathbbm{1}_{ \max _{1 \leq k \leq k_n} \frac{\vert c_{nk} \vert }{\sqrt{\sum_{i=1}^{k_n} c_{ni}^2} } \vert W_k \vert > \epsilon } \right] \\
		\shortintertext{which makes the term in the expectation i.i.d., so we can replace both \(W_j\) and \(W_k\) by \(W \coloneqq W_1\),}
		 & = \sum_{j=1}^{k_n} \frac{c_{nj}^2}{\sum_{i=1}^{k_n} c_{ni}^2} \mathbb{E}_{}\left[ W^2 \mathbbm{1}_{ \max _{1 \leq k \leq k_n} \frac{\vert c_{nk} \vert }{\sqrt{\sum_{i=1}^{k_n} c_{ni}^2} } \vert W \vert > \epsilon } \right]        \\
		 & = \mathbb{E}_{}\left[ W^2 \mathbbm{1}_{ \max _{1 \leq k \leq k_n} \frac{\vert c_{nk} \vert }{\sqrt{\sum_{i=1}^{k_n} c_{ni}^2} } \vert W \vert > \epsilon } \right] .
	\end{align*}
	Hence, it reduces to show \(\mathbb{E}_{}[ W^2 \mathbbm{1}_{\vert W \vert > x} ] \to 0\) as \(n \to \infty \) for \(x \coloneqq \epsilon \sqrt{\sum_{i=1}^{k_n} c_{ni}^2} / \max _{1 \leq k \leq k_n} \vert c_{nk} \vert \). From our assumption, for any \(\epsilon > 0\), \(x \to \infty \) as \(n \to \infty \), hence the expectation indeed goes to \(0\) as long as \(W\) has finite second moment, which is indeed the case by our assumption.
\end{proof}

We see that \hyperref[thm:Hajek-Sidak-CLT]{Hajek-Sidak central limit theorem} is very common in practice.

\begin{intuition}
	Often time for every \(n\), \(c_{ni} = c_n\), the same for every \(1 \leq i \leq k_n\). In this case, we may write \(X_{n i} = c_n \cdot X_{ni} / c_n \eqqcolon c_n Z_i\) such that \(Z_i \coloneqq X_{ni} / c_n\) is i.i.d.\ distributed, ready for checking the \hyperref[thm:Hajek-Sidak-CLT]{Hajek-Sidak condition}.
\end{intuition}

Let's see three examples.

\begin{eg}[Uniform distribution]
	For every \(n \geq 1\), let \((X_{n k_n}) \overset{\text{i.i.d.} }{\sim } \mathcal{U} (-c_n, c_n)\) for some \(c_n > 0\). Then we see that \(Z_i \coloneqq X_{n i} / c_n \sim \mathcal{U} (-1, 1)\) is now i.i.d.\ distributed.
\end{eg}

\begin{eg}[Rademacher distribution]
	For every \(n \geq 1\), let \((X_{n k_n})\) be i.i.d.\ such that \(\mathbb{P} (X_{ni} / c_n = \pm 1) = 1 / 2\). Then \(Z_i \coloneqq X_{ni} / c_n\) is now i.i.d.\ distributed.
\end{eg}

\begin{eg}[Exponential distribution]
	For every \(n \geq 1\), let \((X_{n k_n}) \overset{\text{i.i.d.} }{\sim } \operatorname{Exp}(1 / c_n) \) for some \(c_n > 0\), hence \(Z_i \coloneqq X_{ni} / c_n \sim \operatorname{Exp}(1) \) is now i.i.d.\ distributed.
\end{eg}

On the other hand, if we insist the same setup as in the \hyperref[thm:Lindeberg-CLT]{Lindeberg central limit theorem}, it suffices to check a slightly higher moment rather than the truncated one used in the \hyperref[def:Lindeberg-condition]{Lindeberg condition}.

\begin{corollary}[Lyopunov's central limit theorem]\label{thm:Lyopunov-CLT}
	Consider the setup as in the \hyperref[thm:Lindeberg-CLT]{Lindeberg central limit theorem}. If \(\sum_{j=1}^{k_n} \mathbb{E}_{}[\vert Y_{nj} \vert ^{2 + \delta }] \to 0\) for some \(\delta > 0\), then the \hyperref[def:Lindeberg-condition]{Lindeberg condition} holds.
\end{corollary}
\begin{proof}
	Fix some \(\delta > 0\) such that the assumption holds. Then for any \(\epsilon > 0\), we have
	\[
		\sum_{j=1}^{k_n} \mathbb{E}_{}[\vert Y_{nj} \vert ^2 \mathbbm{1}_{\vert Y_{nj} \vert > \epsilon } ]
		\leq \sum_{j=1}^{k_n} \mathbb{E}_{}\left[ \left( \frac{\vert Y_{nj} \vert }{\epsilon } \right) ^\delta \vert Y_{nj} \vert ^2 \mathbbm{1}_{\vert Y_{nj} \vert > \epsilon } \right]
		\leq \sum_{j=1}^{k_n} \mathbb{E}_{}\left[ \left( \frac{\vert Y_{nj} \vert }{\epsilon } \right) ^\delta \vert Y_{nj} \vert ^2 \right]
		\to 0
	\]
	by taking \(\epsilon ^{-\delta }\) out and then the result follows from the assumption.
\end{proof}

For bounded random variable, we have a simpler form.

\begin{corollary}\label{col:Lyopunov-CLT}
	Let \(\vert X_{ni} \vert \leq C_n\) for all \(1 \leq i \leq k_n\). Then if \(C_n / \sqrt{\Var_{}[S_n] } \to 0\), the \hyperref[def:Lindeberg-condition]{Lindeberg condition} holds. In particular, when \(C_n \eqqcolon C\), it suffices to check \(\Var_{}[S_n] \to \infty \).
\end{corollary}
\begin{proof}
	We see that for every \(1 \leq j \leq k_n\),
	\[
		\vert Y_{nj} \vert
		= \frac{\vert X_{nj} - \mathbb{E}_{}[X_{nj}] \vert }{\sqrt{\Var_{}[S_n] } }
		\leq \frac{2 C_n}{\sqrt{\Var_{}[S_n] } }.
	\]
	In this case, for any \(\delta > 0\), recall that \(\sum_{j=1}^{k_n} \mathbb{E}_{}[Y_{nj}^2] = 1\), hence
	\[
		\sum_{j=1}^{k_n} \mathbb{E}_{}[\vert Y_{nj} \vert ^{2 + \delta }]
		\leq \left( \frac{2C_n}{\sqrt{\Var_{}[S_n] } } \right) ^\delta \sum_{j=1}^{k_n} \mathbb{E}_{}[Y_{nj}^2]
		= \left( \frac{2C_n}{\sqrt{\Var_{}[S_n] } } \right) ^\delta
		\to 0,
	\]
	hence the \hyperref[thm:Lyopunov-CLT]{Lyopunov's condition} holds.
\end{proof}

\begin{eg}[Bernoulli distribution]\label{eg:Lyopunov-CLT}
	For every \(n \geq 1\), let \(X_{ni} \sim \operatorname{Ber}(p_{ni}) \) for all \(1 \leq i \leq k_n\). Since \(X_{ni} \leq 1\), from \autoref{col:Lyopunov-CLT}, it suffices to check
	\[
		\Var_{}[S_n]
		= \sum_{i=1}^{k_n} p_{ni} (1 - p_{ni}) \to \infty .
	\]
	\begin{itemize}
		\item If \(p_{ni} = 1 / i\): then \(\Var_{}[S_n] = \sum_{i=1}^{k_n} 1 / i - \sum_{i=1}^{k_n} 1 / i^2 \to \infty \).
		\item If \(p_{ni} = p_n\): then \(\Var_{}[S_n] = k_n p_n (1 - p_n)\). In particular, for \(p_n = 1 / n\) and \(k_n = n\),
		      \[
			      \Var_{}[S_n]
			      = n \cdot \frac{1}{n} \cdot \frac{n-1}{n}
			      \to 1
			      \neq \infty .
		      \]
		      In general, if \(n p_n \to \lambda > 0\), then the sum \(S_n\) \hyperref[def:converge-in-distribution]{converges} to \(\operatorname{Pois}(\lambda ) \).
	\end{itemize}
\end{eg}

\section{The Two-Sample Problem}
With this new tool in hand, i.e., the \hyperref[thm:Lindeberg-CLT]{Lindeberg central limit theorem}, we now see some applications. Let's first see one illustrative example. Consider collecting a sequence of data \(X_1, \dots , X_n \sim F\) where \(F\) is continuous. We want to test whether \(X_i\)'s are i.i.d.\ from \(F\). To do this, consider that for all \(i \geq 1\), define the \emph{rank} to be
\[
	R_i = \sum_{j=1}^{i} \mathbbm{1}_{X_j < X_i}.
\]

\begin{eg}
	In particular, if \(R_i = i\) (\(R_i = 1\)), then \(X_i\) is the largest (smallest) of \(X_1, \dots , X_i\).
\end{eg}

\begin{note}
	We don't need to worry about the equality in the indicator since \(F\) is continuous.
\end{note}

To see how this helps us to decide whether \(X_i\)'s are i.i.d., under this hypothesis, we have the following.

\begin{theorem}\label{thm:rank}
	Let \(X_1, X_2 , \dots \overset{\text{i.i.d.} }{\sim } F\) where \(F\) is continuous. Then the following hold.
	\begin{enumerate}[(a)]
		\item\label{thm:rank-a} \(R_1, R_2, \dots \) are independent and \(\mathbb{P} (R_i = r) = 1 / i\) for all \(1 \leq r \leq i\), i.e., \(R_i \sim \mathcal{U} (\{ 1, \dots , i \} )\).
		\item\label{thm:rank-b} \(\sum_{i=1}^{n} (R_i - \mathbb{E}_{}[R_i] ) / \sqrt{\Var_{}[\sum_{i=1}^{n} R_i] } \overset{D}{\to} \mathcal{N} (0, 1)\).
	\end{enumerate}
\end{theorem}
\begin{proof}
	For \autoref{thm:rank-a}, since \(X_i\)'s are i.i.d.\ and \(F\) it continues (hence no ties),
	\[
		\mathbb{P} (X_{\sigma (1)} < X_{\sigma (2)} < \dots < X_{\sigma (i)})
		= \frac{1}{i!}
	\]
	for any permutation \(\sigma \) of \(\{ 1, \dots , i \} \). This implies \(\mathbb{P} (R_1 = r_1, \dots , R_i = r_i) = 1 / i!\), hence
	\[
		\mathbb{P} (R_i = r)
		= \sum_{r_1, \dots , r_{i-1}} \mathbb{P} (R_1 = r_1 , \dots , R_{i-1} = r_{i-1} , R_i = r)
		= (i - 1)! \cdot \frac{1}{i!}
		= \frac{1}{i},
	\]
	which proves \autoref{thm:rank-a}. We now see that \autoref{thm:rank-b} is immediate from \autoref{thm:rank-a} since from \autoref{col:Lyopunov-CLT},
	\[
		\frac{n}{\sqrt{\Var_{}[S_n] }}
		= \frac{n}{\sqrt{\sum_{i=1}^{n} \frac{i^2 - 1}{12}}}
		= \frac{6 \sqrt{2} n}{\sqrt{n (2 n^{2} + 3 n - 5)}}
		\to 0
	\]
	as \(n \to \infty \), proving the result.
\end{proof}

\begin{remark}
	Besides \(R_i\), we may instead consider \(Z_i = \mathbbm{1}_{R_i = i} \). Since \(R_i \leq i\), so \(\mathbb{P} (Z_i = 1) = 1 / i\), i.e., \(Z_i \sim \operatorname{Ber}(1 / i) \). Then from the \hyperref[eg:Lyopunov-CLT]{previous example}, we can also get
	\[
		\frac{\sum_{i=1}^{n} Z_i - \mathbb{E}_{}[Z_i] }{\sqrt{\Var_{}[\sum_{i=1}^{n} Z_i] } }
		\overset{D}{\to} \mathcal{N} (0, 1).
	\]
\end{remark}

This gives us some hints about how to deal with this kind of problems, i.e., we try to find some statistics that will follow certain distribution \emph{independent} of the underlying \(F\).

\subsection{Two-Sample Problem with Paired Observation}
We now turn to the famous two-sample problem.

\begin{problem}[Two-sample problem]\label{prb:two-sample}
Given the data of two random samples obtained from a different given population. The \emph{two-sample problem} considers the task of determining whether the difference between these two populations is statistically significant.
\end{problem}

Let's see one classical example of a \hyperref[prb:two-sample]{two-sample problem}, i.e., treatment effect testing.

\begin{eg}[Treatment effect]\label{eg:treatment-effect}
	Consider the task of testing the effect of a treatment. Ideally, we have the treatment group and the control group (the two populations), and we observe \(Y_i^{\text{T} }\)'s from the treatment group, and \(Y_i^{\text{C} }\)'s from the control group. It's common to pair the data together and get \((Y_1^{\text{T} }, Y_1^{\text{C} }), (Y_2^{\text{T} }, Y_2^{\text{C} }), \dots , (Y_n^{\text{T} }, Y_n^{\text{C} })\).
\end{eg}

Let's focus on this \hyperref[eg:treatment-effect]{treatment effect testing problem} in this section. Let \(X_i \coloneqq Y_i^{\text{T} } - Y_i^{\text{C} }\) for all \(1 \leq i \leq n\). Usually, we assume \(X_1 , \dots , X_n\) are i.i.d.\ with mean \(\mu \) and finite variance, and the null hypothesis is \(H_0 \colon \mu = 0\). Since we assume \(X_i\)'s are i.i.d., we may use the \(t\)-test, i.e., we reject \(H_0\) if
\[
	T_n
	= \sqrt{n} \frac{\overline{X} _n}{\hat{\sigma} _n}
	= \frac{\sum_{i=1}^{n} X_i}{\sqrt{n} \sqrt{\frac{1}{n} \sum_{i=1}^{n} X_i^2 - \overline{X} _n^2} }
	= \frac{\sum_{i=1}^{n} X_i}{\sqrt{\sum_{i=1}^{n} (X_i - \overline{X} _n)^2} }
	> Z_{\alpha }
\]
for some \(\alpha > 0\). Since \(\Var_{}[X_i] < \infty \), \(T_n \overset{D}{\to} \mathcal{N} (0, 1)\) under \(H_0\).

\begin{problem*}
	In reality, \(X_i\)'s are never i.i.d., but people still use the \(t\)-test. Why?
\end{problem*}
\begin{answer}
	This is because we usually give the treatment in a ``randomized'' way. In addition, we can ``condition'' on the observed data \(\vert X_i \vert \)'s.
\end{answer}

The above answer is pretty vague, which is intended. The key idea is the following:

\begin{intuition}
	In this case, we can design a hypothesis such that it only depends on statistics that are now i.i.d., and by applying the \hyperref[thm:CLT]{central limit theorem}, we will get something like \(\sum_{i=1}^{n} X_i / \sqrt{\sum_{i=1}^{n} X_i^2} \), pretty much like what we have in the \(t\)-test.
\end{intuition}