\lecture{17}{21 Mar.\ 9:30}{Rank Test and Two-Sample Problem}
\begin{proof}
	Firstly, we see that \(\Var_{}[S_n] = \sum_{j=1}^{k_n} c_{nj}^2 \sigma ^2\), hence with our usual notation, we define
	\[
		Y_{nj}
		\coloneqq \frac{c_{nj} (Z_j - \mu )}{\sqrt{\sum_{i=1}^{k_n} c_{ni}^2} \sigma }
		\eqqcolon \frac{c_{nj}}{\sqrt{\sum_{i=1}^{k_n} c_{ni}^2} } W_j,
	\]
	where \(W_j \coloneqq (Z_j - \mu ) / \sigma \). Then, for any \(\epsilon > 0\), we can check that \hyperref[def:Lindeberg-condition]{Lindeberg condition} as
	\begin{align*}
		\sum_{j=1}^{k_n} \mathbb{E}_{}[Y_{nj}^2 \mathbbm{1}_{\vert Y_{nj} \vert > \epsilon } ]
		 & = \sum_{j=1}^{k_n} \frac{c_{nj}^2}{\sum_{i=1}^{k_n} c_{ni}^2} \mathbb{E}_{}\left[ W_j^2 \mathbbm{1}_{\frac{\vert c_{nj} \vert }{\sqrt{\sum_{i=1}^{k_n} c_{ni}^2} } \vert W_j \vert > \epsilon } \right]                               \\
		\shortintertext{since the only dependence of \(j\) in the expectation is \(\vert c_{nj} \vert \) in the indicator,}
		 & \leq \sum_{j=1}^{k_n} \frac{c_{nj}^2}{\sum_{i=1}^{k_n} c_{ni}^2} \mathbb{E}_{}\left[ W_j^2 \mathbbm{1}_{ \max _{1 \leq k \leq k_n} \frac{\vert c_{nk} \vert }{\sqrt{\sum_{i=1}^{k_n} c_{ni}^2} } \vert W_k \vert > \epsilon } \right] \\
		\shortintertext{which makes the term in the expectation i.i.d., so we can replace both \(W_j\) and \(W_k\) by \(W \coloneqq W_1\),}
		 & = \sum_{j=1}^{k_n} \frac{c_{nj}^2}{\sum_{i=1}^{k_n} c_{ni}^2} \mathbb{E}_{}\left[ W^2 \mathbbm{1}_{ \max _{1 \leq k \leq k_n} \frac{\vert c_{nk} \vert }{\sqrt{\sum_{i=1}^{k_n} c_{ni}^2} } \vert W \vert > \epsilon } \right]        \\
		 & = \mathbb{E}_{}\left[ W^2 \mathbbm{1}_{ \max _{1 \leq k \leq k_n} \frac{\vert c_{nk} \vert }{\sqrt{\sum_{i=1}^{k_n} c_{ni}^2} } \vert W \vert > \epsilon } \right] .
	\end{align*}
	Hence, it reduces to show \(\mathbb{E}_{}[ W^2 \mathbbm{1}_{\vert W \vert > x} ] \to 0\) as \(n \to \infty \) for \(x \coloneqq \epsilon \sqrt{\sum_{i=1}^{k_n} c_{ni}^2} / \max _{1 \leq k \leq k_n} \vert c_{nk} \vert \). From our assumption, for any \(\epsilon > 0\), \(x \to \infty \) as \(n \to \infty \), hence the expectation indeed goes to \(0\) as long as \(W\) has finite second moment, which is indeed the case by our assumption.
\end{proof}

We see that \hyperref[thm:Hajek-Sidak-CLT]{Hajek-Sidak central limit theorem} is very common in practice.

\begin{intuition}
	Often time for every \(n\), \(c_{ni} = c_n\), the same for every \(1 \leq i \leq k_n\). In this case, we may write \(X_{n i} = c_n \cdot X_{ni} / c_n \eqqcolon c_n Z_i\) such that \(Z_i \coloneqq X_{ni} / c_n\) is i.i.d.\ distributed, ready for checking the \hyperref[thm:Hajek-Sidak-CLT]{Hajek-Sidak condition}.
\end{intuition}

Let's see three examples.

\begin{eg}[Uniform distribution]
	For every \(n \geq 1\), let \((X_{n k_n}) \overset{\text{i.i.d.} }{\sim } \mathcal{U} (-c_n, c_n)\) for some \(c_n > 0\). Then we see that \(Z_i \coloneqq X_{n i} / c_n \sim \mathcal{U} (-1, 1)\) is now i.i.d.\ distributed.
\end{eg}

\begin{eg}[Rademacher distribution]
	For every \(n \geq 1\), let \((X_{n k_n})\) be i.i.d.\ such that \(\mathbb{P} (X_{ni} / c_n = \pm 1) = 1 / 2\). Then \(Z_i \coloneqq X_{ni} / c_n\) is now i.i.d.\ distributed.
\end{eg}

\begin{eg}[Exponential distribution]
	For every \(n \geq 1\), let \((X_{n k_n}) \overset{\text{i.i.d.} }{\sim } \operatorname{Exp}(1 / c_n) \) for some \(c_n > 0\), hence \(Z_i \coloneqq X_{ni} / c_n \sim \operatorname{Exp}(1) \) is now i.i.d.\ distributed.
\end{eg}

On the other hand, if we insist the same setup as in the \hyperref[thm:Lindeberg-CLT]{Lindeberg central limit theorem}, it suffices to check a slightly higher moment rather than the truncated one used in the \hyperref[def:Lindeberg-condition]{Lindeberg condition}.

\begin{corollary}[Lyapunov's central limit theorem]\label{thm:Lyapunov-CLT}
	Consider the setup as in the \hyperref[thm:Lindeberg-CLT]{Lindeberg central limit theorem}. If \(\sum_{j=1}^{k_n} \mathbb{E}_{}[\vert Y_{nj} \vert ^{2 + \delta }] \to 0\) for some \(\delta > 0\), then the \hyperref[def:Lindeberg-condition]{Lindeberg condition} holds.
\end{corollary}
\begin{proof}
	Fix some \(\delta > 0\) such that the assumption holds. Then for any \(\epsilon > 0\), we have
	\[
		\sum_{j=1}^{k_n} \mathbb{E}_{}[\vert Y_{nj} \vert ^2 \mathbbm{1}_{\vert Y_{nj} \vert > \epsilon } ]
		\leq \sum_{j=1}^{k_n} \mathbb{E}_{}\left[ \left( \frac{\vert Y_{nj} \vert }{\epsilon } \right) ^\delta \vert Y_{nj} \vert ^2 \mathbbm{1}_{\vert Y_{nj} \vert > \epsilon } \right]
		\leq \sum_{j=1}^{k_n} \mathbb{E}_{}\left[ \left( \frac{\vert Y_{nj} \vert }{\epsilon } \right) ^\delta \vert Y_{nj} \vert ^2 \right]
		\to 0
	\]
	by taking \(\epsilon ^{-\delta }\) out and then the result follows from the assumption.
\end{proof}

For bounded random variable, we have a simpler form.

\begin{corollary}\label{col:Lyapunov-CLT}
	Let \(\vert X_{ni} \vert \leq C_n\) for all \(1 \leq i \leq k_n\). Then if \(C_n / \sqrt{\Var_{}[S_n] } \to 0\), the \hyperref[def:Lindeberg-condition]{Lindeberg condition} holds. In particular, when \(C_n \eqqcolon C\), it suffices to check \(\Var_{}[S_n] \to \infty \).
\end{corollary}
\begin{proof}
	We see that for every \(1 \leq j \leq k_n\),
	\[
		\vert Y_{nj} \vert
		= \frac{\vert X_{nj} - \mathbb{E}_{}[X_{nj}] \vert }{\sqrt{\Var_{}[S_n] } }
		\leq \frac{2 C_n}{\sqrt{\Var_{}[S_n] } }.
	\]
	In this case, for any \(\delta > 0\), recall that \(\sum_{j=1}^{k_n} \mathbb{E}_{}[Y_{nj}^2] = 1\), hence
	\[
		\sum_{j=1}^{k_n} \mathbb{E}_{}[\vert Y_{nj} \vert ^{2 + \delta }]
		\leq \left( \frac{2C_n}{\sqrt{\Var_{}[S_n] } } \right) ^\delta \sum_{j=1}^{k_n} \mathbb{E}_{}[Y_{nj}^2]
		= \left( \frac{2C_n}{\sqrt{\Var_{}[S_n] } } \right) ^\delta
		\to 0,
	\]
	hence the \hyperref[thm:Lyapunov-CLT]{Lyapunov's condition} holds.
\end{proof}

\begin{eg}[Bernoulli distribution]\label{eg:Lyapunov-CLT}
	For every \(n \geq 1\), let \(X_{ni} \sim \operatorname{Ber}(p_{ni}) \) for all \(1 \leq i \leq k_n\). Since \(X_{ni} \leq 1\), from \autoref{col:Lyapunov-CLT}, it suffices to check
	\[
		\Var_{}[S_n]
		= \sum_{i=1}^{k_n} p_{ni} (1 - p_{ni}) \to \infty .
	\]
	\begin{itemize}
		\item If \(p_{ni} = 1 / i\): then \(\Var_{}[S_n] = \sum_{i=1}^{k_n} 1 / i - \sum_{i=1}^{k_n} 1 / i^2 \to \infty \).
		\item If \(p_{ni} = p_n\): then \(\Var_{}[S_n] = k_n p_n (1 - p_n)\). In particular, for \(p_n = 1 / n\) and \(k_n = n\),
		      \[
			      \Var_{}[S_n]
			      = n \cdot \frac{1}{n} \cdot \frac{n-1}{n}
			      \to 1
			      \neq \infty .
		      \]
		      In general, if \(n p_n \to \lambda > 0\), then the sum \(S_n\) \hyperref[def:converge-in-distribution]{converges} to \(\operatorname{Pois}(\lambda ) \).
	\end{itemize}
\end{eg}

\subsection{Testing the I.I.D.\ Assumption}
With this new tool in hand, i.e., the \hyperref[thm:Lindeberg-CLT]{Lindeberg central limit theorem}, let's see one illustrative application, i.e., \emph{testing the i.i.d.\ assumption}.

\begin{problem*}[Testing the i.i.d.\ assumption]
	Consider collecting a sequence of data \(X_1, \dots , X_n \sim F\) where \(F\) is continuous. We want to test whether \(X_i\)'s are i.i.d.\ from \(F\).
\end{problem*}
\begin{answer}
	To do this, consider that for all \(i \geq 1\), define the \emph{rank} to be\footnote{We don't need to worry about the equality in the indicator since \(F\) is continuous.}
	\[
		R_i = \sum_{j=1}^{i} \mathbbm{1}_{X_j < X_i}.
	\]
	In particular, if \(R_i = i\) (\(R_i = 1\)), then \(X_i\) is the largest (smallest) of \(X_1, \dots , X_i\).
\end{answer}

To see how \(R_i\)'s help us to decide whether \(X_i\)'s are i.i.d., under this hypothesis, we have the following.

\begin{theorem}\label{thm:rank}
	Let \((X_n) \overset{\text{i.i.d.} }{\sim } F\) where \(F\) is continuous. Then \((R_n)\) are independent and \(\mathbb{P} (R_i = r) = 1 / i\) for all \(1 \leq r \leq i\), i.e., \(R_i \sim \mathcal{U} (\{ 1, \dots , i \} )\). Moreover,
	\[
		\frac{6}{\sqrt{n^3} } \left( \sum_{i=1}^{n} R_i - \frac{n(n+1)}{4} \right) \overset{D}{\to} \mathcal{N} (0, 1).
	\]
\end{theorem}
\begin{proof}
	Since \(X_i\)'s are i.i.d.\ and \(F\) it continues (hence no ties),
	\[
		\mathbb{P} (X_{\sigma (1)} < X_{\sigma (2)} < \dots < X_{\sigma (i)})
		= \frac{1}{i!}
	\]
	for any permutation \(\sigma \) of \(\{ 1, \dots , i \} \). This implies \(\mathbb{P} (R_1 = r_1, \dots , R_i = r_i) = 1 / i!\), hence
	\[
		\mathbb{P} (R_i = r)
		= \sum_{r_1, \dots , r_{i-1}} \mathbb{P} (R_1 = r_1 , \dots , R_{i-1} = r_{i-1} , R_i = r)
		= (i - 1)! \cdot \frac{1}{i!}
		= \frac{1}{i}.
	\]
	This proves the first part. Now, observe that \(\mathbb{E}_{}\left[\sum_{i=1}^{n} R_i \right] = \sum_{i=1}^{n} \frac{i}{2} = n (n + 1) / 4\) and
	\[
		\Var_{}\left[\sum_{i=1}^{n} R_i \right]
		= \sum_{i=1}^{n} \frac{i^2 - 1}{12}
		= \frac{n (2 n^{2} + 3 n - 5)}{72}
		\sim \frac{n^3}{36},
	\]
	hence if the \hyperref[thm:Lindeberg-CLT]{Lindebert central limit theorem} holds, then we're done. We check \autoref{col:Lyapunov-CLT}: by noting that \(\lvert R_i \rvert \leq n\), we indeed have \(n / \sqrt{\Var_{}[\sum_{i=1}^{n} R_i]} \to 0\).
\end{proof}

\begin{remark}[Record]
	We may instead consider the \emph{record} \(Z_i = \mathbbm{1}_{R_i = i} \).\footnote{Intuitive, it indicates when \(X_i\) is the largest among \(X_1, \dots , X_{i-1}\).} Since \(R_i \leq i\), \(\mathbb{P} (Z_i = 1) = 1 / i\), i.e., \(Z_i \sim \operatorname{Ber}(1 / i) \). Then the \hyperref[eg:Lyapunov-CLT]{previous example} gives asymptotic normality of \(\sum_{i=1}^{n} Z_i\).
\end{remark}

In either case (\(R_i\) or \(Z_i\)), the above gives us some hints about how to deal with this kind of problems.

\begin{intuition}
	Find some statistics whose distribution is \emph{independent} of the underlying \(F\) under \(H_0\).
\end{intuition}

\section{Testing for Symmetry}
Our next goal is to apply the intuition we get from the i.i.d.\ problem, and apply it to the problem of \hyperref[prb:testing-symmetry]{testing for symmetry}.

\begin{problem}[Testing for symmetry]\label{prb:testing-symmetry}
Let \(X, X_1, \dots  X_n \overset{\text{i.i.d.} }{\sim } F\) where \(F\) is a continuous distribution function. The problem of \emph{testing symmetry} is to test the null hypothesis, \(H_0\), that \(F\) is symmetric about \(0\), i.e., whether \(X \overset{D}{=} -X\).
\end{problem}

\subsection{Motivation: Two-Sample Problem with no I.I.D.\ Assumption}
\hyperref[prb:testing-symmetry]{Symmetry testing} is important since it justifies various tests we use in practice for the classical \hyperref[prb:two-sample]{two-sample problem}, even if the i.i.d.\ assumption is violated.

\begin{problem}[Two-sample problem]\label{prb:two-sample}
Given the data of two random samples obtained from a different given population. The \emph{two-sample problem} considers the task of determining whether the difference between these two populations is statistically significant.
\end{problem}

One classical example of a \hyperref[prb:two-sample]{two-sample problem} is the \emph{treatment effect testing}.

\begin{eg}[Treatment effect]\label{eg:treatment-effect}
	Consider a treatment group and a control group (the two populations), and we observe \(Y_i^{\text{T} }\)'s and \(Y_i^{\text{C} }\)'s from the treatment and the control group, respectively. The goal is to test whether the treatment takes affect. It's common to pair the data together and get \((Y_1^{\text{T} }, Y_1^{\text{C} }), \dots , (Y_n^{\text{T} }, Y_n^{\text{C} })\), and let \(X_i \coloneqq Y_i^{\text{T} } - Y_i^{\text{C} }\) for all \(1 \leq i \leq n\).
\end{eg}

Usually, we assume \(X_1 , \dots , X_n\) are i.i.d.\ with mean \(\mu \) and finite variance, and the null hypothesis we want to test is \(H_0 \colon \mu = 0\). As \(X_i\)'s are i.i.d., we may use the \(t\)-test to the \hyperref[eg:treatment-effect]{treatment effect problem}, namely, we reject \(H_0\) if the \hyperref[def:t-statistic]{\(t\)-statistic} is too large, i.e.,
\[
	T_n
	= \sqrt{n} \frac{\overline{X} _n}{\hat{\sigma} _n}
	= \frac{\sum_{i=1}^{n} X_i}{\sqrt{n} \sqrt{\frac{1}{n} \sum_{i=1}^{n} X_i^2 - \overline{X} _n^2} }
	= \frac{\sum_{i=1}^{n} X_i}{\sqrt{\sum_{i=1}^{n} (X_i - \overline{X} _n)^2} }
	> Z_{\alpha }
\]
for some \(\alpha > 0\). Since \(\Var_{}[X_i] < \infty \), \(T_n \overset{D}{\to} \mathcal{N} (0, 1)\) under \(H_0\).

\begin{problem*}
	In reality, \(X_i\)'s are never i.i.d., but people still use the \(t\)-test. Why?
\end{problem*}
\begin{answer}
	This is because we usually give the treatment in a ``randomized'' way. In addition, we can ``condition'' on the observed data \(\vert X_i \vert \)'s. In this case, we can design a hypothesis testing such that it only depends on statistics that are now i.i.d., and by applying the \hyperref[thm:CLT]{central limit theorem}, we will get something like \(\sum_{i=1}^{n} X_i / \sqrt{\sum_{i=1}^{n} X_i^2} \), pretty much like what we have in the \(t\)-test.
\end{answer}