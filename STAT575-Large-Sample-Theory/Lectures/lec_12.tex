\lecture{12}{22 Feb.\ 9:30}{Asymptotic Joint Distribution by Multivariate CLT}
\begin{theorem}[Multivariate central limit theorem]\label{thm:multivariate-CLT}
	Let \((X_n)\) be i.i.d.\ random vectors in \(\mathbb{R} ^d\) with \(\mathbb{E}_{}[X_i] = \mu \in \mathbb{R} ^d\), \(\Var_{}[X_i] = \Sigma \in \mathbb{R} ^{d \times d}\) for all \(1 \leq i \leq n\). Then
	\[
		\frac{1}{\sqrt{n} } \sum_{i=1}^{n} (X_i - \mu )
		\overset{D}{\to} \mathcal{N} (0, \Sigma ).
	\]
\end{theorem}
\begin{proof}
	Set \(\mu = 0\), and from \hyperref[thm:Cramer-Wold-device]{CramÃ©r-Wold device}, it suffices to show that for any \(t \in \mathbb{R} ^d\),
	\[
		t \cdot \left( \frac{1}{\sqrt{n} } \sum_{i=1}^{n} X_i \right)
		\overset{D}{\to} t \cdot Z
		\sim \mathcal{N} (0, t ^{\top} \Sigma t)
	\]
	where \(Z \sim \mathcal{N} (0, \Sigma )\). Indeed, since from the \hyperref[thm:CLT]{univariate central limit theorem}, the left-hand side \hyperref[def:converge-in-distribution]{converges} to \(\mathcal{N} (0, \Var_{}[t\cdot X_i] )\), with \(\Var_{}[t \cdot X] = t^{\top} \Var_{}[X_i] t = t ^{\top} \Sigma t = \Var_{}[t \cdot Z] \), we're done.
\end{proof}

\subsection{Testing Normality with General Moments}
With \hyperref[thm:multivariate-CLT]{multivariate central limit theorem}, we can now generalize \autoref{col:asymptotic-distribution-odd-sample-central-moment}, i.e., finding the asymptotic distribution of \(\widetilde{M} _k = M_k / \hat{\sigma} _n^k\) for general \(k\). Recall the setup, where we let \((X_n)\) and \(X\) be i.i.d.\ random variable, \(Y_i = X_i - \mu \) (and \(Y = X - \mu \)), \(\sigma ^2 = \Var_{}[X] \), \(\mu _k = \mathbb{E}_{}[Y^k] \), and \(\widetilde{\mu} _k = \mu _k / \sigma ^k\). Let's start with \(k = 1\), i.e., compute the asymptotic law of \(\overline{X} _n / \hat{\sigma} _n\). In this case, we have proved the following.

\begin{prev}
	From \autoref{prop:inference-mean} and \autoref{prop:inference-variance},
	\begin{itemize}
		\item \(\sqrt{n} (\overline{X} _n - \mu ) \overset{D}{\to} \mathcal{N} (0, \sigma ^2)\) from \(\sqrt{n} (\overline{X} _n - \mu ) = \frac{1}{\sqrt{n} } \sum_{i=1}^{n} Y_i\), assuming \(X \in L^2\);
		\item \(\sqrt{n} (\hat{\sigma} _n^2 - \sigma ^2) \overset{D}{\to} \mathcal{N} (0, \mu _4 - \sigma ^4) \) from \(\sqrt{n} (\hat{\sigma} _n^2 - \sigma ^2) = \frac{1}{\sqrt{n} } \sum_{i=1}^{n} (Y_i^2 - \sigma ^2) + o_p(1)\), assuming \(X \in L^4\) and \(\widetilde{\mu} _4 > 1\).\footnote{The latter representation result needs only the assumption of \(X \in L^2\).}
	\end{itemize}
\end{prev}

This together with \hyperref[thm:multivariate-CLT]{multivariate central limit theorem} and \hyperref[thm:Slutsky]{Slutsky's theorem} give the following.

\begin{proposition}\label{prop:joint-asymptotic-distribution-sample-mean-variance}
	If \(X \in L^2\),
	\[
		\sqrt{n} \left( \begin{pmatrix}
				\overline{X} _n   \\
				\hat{\sigma} _n^2 \\
			\end{pmatrix} - \begin{pmatrix}
				\mu       \\
				\sigma ^2 \\
			\end{pmatrix} \right)
		= \frac{1}{\sqrt{n} } \sum_{i=1}^{n} \begin{pmatrix}
			Y_i               \\
			Y_i^2 - \sigma ^2 \\
		\end{pmatrix} + o_p(1).
	\]
	Moreover, if \(X \in L^4\) and \(\widetilde{\mu} _4 = \mu _4 / \sigma ^4 > 1\), then the above \hyperref[def:converge-in-distribution]{converge in distribution} to \(\mathcal{N} (0, \Sigma )\) where
	\[
		\Sigma
		= \Var_{}\left[\begin{pmatrix}
				Y   \\
				Y^2 \\
			\end{pmatrix} \right]
		= \begin{pmatrix}
			\Var_{}[Y]      & \Cov_{}[Y, Y^2] \\
			\Cov_{}[Y, Y^2] & \Var_{}[Y^2]    \\
		\end{pmatrix}
		= \begin{pmatrix}
			\sigma ^2 & \mu _3             \\
			\mu _3    & \mu _4 - \sigma ^4 \\
		\end{pmatrix}.
	\]
\end{proposition}

\begin{remark}[Asymptotically independent]
	We know that when \(X\) is Gaussian, \(\overline{X} _n\) and \(s_n^2\) are independent. Related back to \autoref{col:asymptotic-distribution-odd-sample-central-moment}, when their \hyperref[def:skewness]{skewness} is \(0\), \(\overline{X} _n\) and \(\hat{\sigma} _n^2\) (or \(s_n^2\)) are asymptotically independent, which is again confirmed by \autoref{prop:joint-asymptotic-distribution-sample-mean-variance} here.
\end{remark}

\autoref{prop:joint-asymptotic-distribution-sample-mean-variance} gives an asymptotic distribution of \(\overline{X} _n\) and \(\hat{\sigma} _n^2\), but not \(\hat{\sigma} _n\). This is fine since we can further apply the \hyperref[thm:delta-method]{delta method} with \(g(\overline{X} _n , \hat{\sigma} _ n^2) \coloneqq \overline{X} _n / \hat{\sigma} _n\) to get the distribution of \(\overline{X} _n / \hat{\sigma} _n\). However, let's leave the application of the \hyperref[thm:delta-method]{delta method} to the general \(k\). We note the following.

\begin{note}
	The actual characterization of \(\overline{X} _n\) and \(\hat{\sigma} _n^2\) right before applying \hyperref[thm:CLT]{central limit theorem} is much more useful than the final asymptotic distributions.
\end{note}

Next, we compute the asymptotic law of \(\widetilde{M} _k = M_k / \hat{\sigma} _n^k\) for general \(k > 2\). Following a similar calculation, for \(\hat{\sigma} _n^k\), we can again use the result from \autoref{prop:inference-variance} for \(\hat{\sigma} _n^2\).

\begin{prev}
	From \autoref{thm:asymptotic-distribution-sample-central-moment}, if \(X \in L^{k}\),
	\[
		\sqrt{n} (M_k - \mu _k)
		= \frac{1}{\sqrt{n} } \sum_{i=1}^{n} (Y_i^k - \mu _k - k \mu _{k-1} Y_i) + o_p(1),
	\]
	and \(\sqrt{n} (M_k - \mu _k) \to \mathcal{N} (0, \Var_{}[Y^k - k \mu _{k-1} Y] )\) if \(X \in L^{2k}\) and the variance is strictly positive.
\end{prev}

This implies that for \(X \in L^{k}\) for any \(k > 2\),\footnote{This ``\(Y\)'' will be used in the \hyperref[thm:delta-method]{delta method} later, although this is not exact since \(Y\) should be the random vector corresponding the asymptotic distribution. But this is fine in the end from \hyperref[thm:Slutsky]{Slutsky's theorem}.}
\begin{equation}\label{eq:joint-representation-sample-variance-central-moment}
	Y \coloneqq \sqrt{n} \left( \begin{pmatrix}
			\hat{\sigma} _n^2 \\
			M_k               \\
		\end{pmatrix} - \begin{pmatrix}
			\sigma ^2 \\
			\mu _k    \\
		\end{pmatrix} \right)
	= \frac{1}{\sqrt{n} } \sum_{i=1}^{n}  \begin{pmatrix}
		Y_i^2 - \sigma ^2                 \\
		Y_i^k - \mu _k - k \mu _{k-1} Y_i \\
	\end{pmatrix} + o_p(1),
\end{equation}
which \hyperref[def:converge-in-distribution]{converges} to \(\mathcal{N} (0, \Sigma )\) from \hyperref[thm:multivariate-CLT]{multivariate central limit theorem} when \(X \in L^{2k}\), where
\[
	\Sigma = \begin{pmatrix}
		\Var_{}[Y^2]                        & \Cov_{}[Y^2 , Y^k - k \mu _{k-1} Y] \\
		\Cov_{}[Y^2 , Y^k - k \mu _{k-1} Y] & \Var_{}[Y^k - k \mu _{k-1} Y]       \\
	\end{pmatrix}.
\]

\begin{remark}
	In general \(k\), if \(\mu _{\ell } = 0\) for all odd \(\ell \), then \(M_k\) and \(\hat{\sigma} _n^2\) are asymptotically independent. This is why we get a simplification for odd case in \autoref{col:asymptotic-distribution-odd-sample-central-moment}.
\end{remark}

Putting everything together formally, we have the following result for general \(k\).

\begin{theorem}\label{thm:asymptotic-distribution-sample-standardized-central-moment}
	Let \(X \in L^{k}\) for some \(k > 2\). Then for \(Z = (X - \mu ) / \sigma = Y / \sigma \),
	\[
		\sqrt{n} (\widetilde{M} _k - \widetilde{\mu} _k)
		= \frac{1}{\sqrt{n} } \sum_{i=1}^{n} \left( -\frac{k}{2} \widetilde{\mu} _k (Z_i^2 - 1) + (Z_i^k - \widetilde{\mu} _k - k \widetilde{\mu} _{k-1} Z_i ) \right) + o_p(1).
	\]
	Moreover, if \(X \in L^{2k}\) and \(\widetilde{v} _k \coloneqq \Var_{}\left[- \frac{k}{2} \widetilde{\mu} _k Z^2 + Z^k - k \widetilde{\mu} _{k-1} Z \right] > 0\), then \(\sqrt{n} (\widetilde{M} _k - \widetilde{\mu} _k) \overset{D}{\to} \mathcal{N} (0, \widetilde{v} _k)\).
\end{theorem}
\begin{proof}
	Since \autoref{prop:inference-variance} is for \(\hat{\sigma} _n^2\) but not \(\hat{\sigma} _n^k\), we need to use \hyperref[thm:delta-method]{delta method} by considering \(\widetilde{M} _k = M_k / \hat{\sigma} _n^k = g(\hat{\sigma} _n^2, M_k)\) where \(g(x, y) \coloneqq y / x^{k / 2}\) for \(x > 0\), \(y \in \mathbb{R} \). We see that
	\[
		\nabla g(\sigma ^2, \mu _k)
		= \begin{pmatrix}
			-\frac{k}{2} \mu _k \sigma ^{-k - 2} & \sigma ^{-k} \\
		\end{pmatrix}
		= \begin{pmatrix}
			-\frac{k}{2} \widetilde{\mu} _k \sigma ^{-2} & \sigma ^{-k} \\
		\end{pmatrix}
	\]
	since \(\widetilde{\mu} _k = \mu _k / \sigma ^k\), \(\partial g / \partial x = - k y x^{-k / 2 - 1} / 2\), and  \(\partial g / \partial y = x^{-k / 2}\). From \hyperref[thm:delta-method]{delta method} and \autoref{eq:joint-representation-sample-variance-central-moment} with \(X \in L^k\), with \(\widetilde{\mu} _k = g(\sigma ^2, \mu _k)\), we get \(\sqrt{n} (g(\hat{\sigma} _n^2, M_k) - g(\sigma ^2, \mu _k)) \overset{D}{\to } \nabla g Y\), i.e.,
	\[
		\begin{split}
			\sqrt{n} (\widetilde{M} _k - \widetilde{\mu} _k)
			 & = \nabla g(\sigma ^2, \mu _k) \frac{1}{\sqrt{n} } \sum_{i=1}^{n} \begin{pmatrix}
				                                                                    Y_i^2 - \sigma ^2                 \\
				                                                                    Y_i^k - \mu _k - k \mu _{k-1} Y_i \\
			                                                                    \end{pmatrix} + o_p(1)                                                                                             \\
			 & = \frac{1}{\sqrt{n} } \sum_{i=1}^{n} \left( -\frac{k}{2} \widetilde{\mu} _k \frac{1}{\sigma ^2} (Y_i^2 - \sigma ^2) + \frac{1}{\sigma ^k} (Y_i^k - \mu _k - k \mu _{k-1} Y_i) \right) + o_p(1) \\
			 & = \frac{1}{\sqrt{n} } \sum_{i=1}^{n} \left( -\frac{k}{2} \widetilde{\mu} _k (Z_i^2 - 1) + (Z_i^k - \widetilde{\mu} _k - k \widetilde{\mu} _{k-1} Z_i ) \right) + o_p(1)
		\end{split}
	\]
	by letting \(Z_i \coloneqq (X_i - \mu) / \sigma = Y_i / \sigma \), proving the first claim. Then by the \hyperref[thm:multivariate-CLT]{multivariate central limit theorem} and \hyperref[col:Slutsky]{Slutsky's theorem}, the above further \hyperref[def:converge-in-distribution]{converges in distribution} to \(\mathcal{N} (0, \widetilde{v} _k)\) when
	\[
		\widetilde{v} _k
		\coloneqq \Var_{}\left[-\frac{k}{2} \widetilde{\mu} _k (Z^2 - 1) + (Z^k - \widetilde{\mu} _k - k \widetilde{\mu} _{k-1} Z ) \right]
		= \Var_{}\left[-\frac{k}{2} \widetilde{\mu} _k Z^2 + Z^k - k \widetilde{\mu} _{k-1} Z \right] > 0,
	\]
	as we assumed.
\end{proof}

Compared to \autoref{col:asymptotic-distribution-odd-sample-central-moment} for odd \(k\) and \(\mu _k = 0\), there we only get an asymptotic distribution, not an explicit decomposition. With this explicit formula, we can do more. Consider the following example.

\begin{eg}
	Consider using both \(\widetilde{M} _3\) and \(\widetilde{M} _4\) to test \(H_0\colon X \sim \mathcal{N} \). We see that under \(H_0\),
	\[
		\left( \sqrt{\frac{n}{\widetilde{v} _3}} \widetilde{M} _3 \right) ^2 + \left( \sqrt{\frac{n}{\widetilde{v} _4}} (\widetilde{M} _4 - \widetilde{\mu} _4) \right) ^2
		\overset{D}{\to} \chi _2^2 .
	\]
\end{eg}
\begin{explanation}
	One can write down \(\sqrt{n} (\widetilde{M} _\ell - \widetilde{\mu} _\ell )\) for even \(\ell \), and also \(\sqrt{n} (\widetilde{M} _k - \widetilde{\mu} _k)= \sqrt{n} \widetilde{M} _k\) for odd \(k\), and see that while they both converge to \(\mathcal{N} (0, 1)\), their covariance is \(0\), i.e., asymptotically independent, so the square of them add up to \(\chi _2^2\).
\end{explanation}

Generalizing the above example, for any \(X\) with \(k > 1\) odd and \(\ell > 2\) even, such that every odd central moments vanish with \(\widetilde{v} _k , \widetilde{v} _\ell < \infty \),
\[
	\frac{n}{\widetilde{v} _k} \widetilde{M} _k^2 + \frac{n}{\widetilde{v} _\ell }(\widetilde{M} _\ell - \widetilde{\mu} _\ell )^2 \overset{D}{\to} \chi _2^2 .
\]

\section{A Quick Detour}
We take a slight detour discussing how to asymptotically compare two estimators and how to make the confidence interval (when it depends on too many estimators) more stable.

\subsection{Asymptotic Relative Efficiency}
First, consider the following illustrative example.

\begin{eg}\label{eg:ARE}
	Let \(X_1, \dots , X_n \overset{\text{i.i.d.} }{\sim } \operatorname{Pois}(\theta ) \). To estimate \(\theta \), as \(\theta = \mathbb{E}_{}[X] = \Var_{}[X] \), two natural estimators are \(\overline{X} _n\) and \(\hat{\sigma} _n^2\). To compare them, we see that
	\begin{itemize}
		\item \(\sqrt{n} (\overline{X} _n - \theta ) \overset{D}{\to} \mathcal{N} (0, \sigma ^2)\);
		\item \(\sqrt{n} (\hat{\sigma} _n^2 - \theta ) \overset{D}{\to} \mathcal{N} (0, \mu _4 - \sigma ^4)\).
	\end{itemize}
	As \(\sigma ^2 = \theta \) and \(\mu _4 = 3 \theta ^2 + \theta \), we see that \(\overline{X} _n\) is better since its variance is smaller.
\end{eg}

To further quantify how much better is it, we ask how many data we need to we get a similar precision: consider the problem of estimating a scalar parameter \(\theta \) such that for two estimators \(T_n^1\) and \(T_n^2\),
\[
	\sqrt{n} (T_n^i - \theta )
	\overset{D}{\to} \sigma _i (\theta ) Z \sim \mathcal{N} (0, \sigma _i^2(\theta ))
\]
Our goal is to find a single number which compares these two estimators. Firstly, for \(n\) large enough,
\[
	\mathbb{P} (\theta \in I_n^{i})
	\coloneqq \mathbb{P} \left( \theta \in T_n^i \pm Z_{\alpha / 2} \frac{\sigma _i(\theta )}{\sqrt{n} } \right)
	\cong 1 - \alpha
\]
where \(I_n^{i} \coloneqq T_n^i \pm Z_{\alpha / 2} \sigma _i(\theta ) / \sqrt{n} \). Let \(n_i(\gamma )\) be the value of \(n\) such that \(\vert I_n^{i} \vert = \gamma \), for \(\gamma \) small enough,
\[
	\gamma \cong 2 Z_{\alpha / 2} \frac{\sigma _i(\theta )}{\sqrt{n_i (\gamma )} }
	\implies n_i(\gamma ) \cong \left( \frac{2 Z_{\alpha / 2}}{\gamma } \sigma _i(\theta ) \right)^2 ,
\]
i.e., \(n_1(\gamma ) / n_2(\gamma ) \cong \sigma _1^2(\theta ) / \sigma _2^2(\theta )\). We called this the \hyperref[def:asymptotic-relative-efficiency-estimator]{asymptotic relative efficiency} \(\ARE_\theta (T^1, T^2) \).

\begin{definition}[Asymptotic relative efficiency for estimator]\label{def:asymptotic-relative-efficiency-estimator}
	The \emph{asymptotic relative efficiency} between two estimators \(T^1_n\) and \(T^2_n\) for \(\theta \) such that \(\sqrt{n} (T_n^i - \theta ) \overset{D}{\to} \mathcal{N} (0, \sigma _i^2(\theta ))\) is defined as
	\[
		\ARE_\theta (T^1, T^2)
		= \frac{\sigma _1(\theta )^2}{\sigma _2(\theta )^2}.
	\]
\end{definition}

\begin{intuition}
	We can read \(\ARE_\theta (T^1, T^2) < 1\) as \(n_1 < n_2\) and infer \(T^1\) is better than \(T^2\).
\end{intuition}

\begin{note}
	\autoref{def:asymptotic-relative-efficiency-estimator} is different from the convention, where we usually define the \emph{asymptotic relative efficiency of \(T^1\) w.r.t.\ \(T^2\)} as \(\ARE_\theta (T^1, T^2) = (\sigma _2(\theta ) / \sigma _1(\theta ))^2\). But it's just the convention.
\end{note}

\subsection{Variance Stabilizing Transformation}
Continuing on the previous \hyperref[eg:ARE]{example}, say we use \(\overline{X} _n\) as the estimator of \(\theta \). We have
\[
	\sqrt{n} (\overline{X} _n - \theta ) \overset{D}{\to} \sqrt{\theta } Z \sim \sqrt{\theta } \mathcal{N} (0, 1) = \mathcal{N} (0, \theta ) .
\]
As the asymptotic distribution depends on \(\theta \), we don't directly get a confidence interval.

\begin{prev}
	We will usually write \(\sqrt{n} / \sqrt{\theta } (\overline{X} _n - \theta ) \overset{D}{\to} Z \sim \mathcal{N} (0, 1)\), replace \(\sqrt{\theta } \) by \(\sqrt{\overline{X} _n} \), and apply \hyperref[thm:continuous-mapping]{continuous mapping theorem} and \hyperref[thm:Slutsky]{Slutsky's theorem} to get a confidence interval.
\end{prev}

We see that our usual approach relies on (\hyperref[def:consistent]{consistently}) estimating the variance of the asymptotic distribution, which is potentially ``unstable'' for small \(n\). To get around this, observe that from the \hyperref[thm:delta-method]{delta method} with some \(g \colon \mathbb{R} \to \mathbb{R} \) differentiable at \(\theta \) and \(g^{\prime} (\theta ) \neq 0\),
\[
	\sqrt{n} (g(\overline{X} _n) - g(\theta )) \overset{D}{\to} g^{\prime} (\theta ) \sqrt{\theta } Z.
\]
This suggests that if we can select \(g\) such that \(g^{\prime} (\theta ) \sqrt{\theta } = c > 0\) is some constant for every \(\theta > 0\), our goal is achieved since now we have
\[
	\frac{\sqrt{n}}{c} (g(\overline{X} _n) - g(\theta )) \overset{D}{\to} \mathcal{N} (0, 1).
\]
In this case, we get an asymptotic confidence interval for \(g(\theta )\) with confidence level \(1 - \alpha \) as
\[
	\left( g(\overline{X} _n) - Z_{\alpha / 2} \frac{c}{\sqrt{n} } , g(\overline{X} _n) + Z_{\alpha / 2} \frac{c}{\sqrt{n} } \right),
\]
and hence an asymptotic confidence interval for \(\theta \) with confidence level \(1 - \alpha \) is just
\[
	\left( g^{-1} \left( g(\overline{X} _n) - Z_{\alpha / 2} \frac{c}{\sqrt{n} } \right) , g^{-1} \left( g(\overline{X} _n) + Z_{\alpha / 2} \frac{c}{\sqrt{n} } \right) \right),
\]
This is the so-called \emph{variance stabilizing transformation}.

\begin{claim}
	For \(c = 1 / 2\), \(g(\theta ) = \sqrt{\theta } \) suffices. Hence, in this case, \(g^{-1} (u) = u^2\).
\end{claim}
\begin{explanation}
	Since for \(g^{\prime} (\theta ) = \frac{1}{2 \sqrt{\theta } }\), we have \(g(\theta ) = \sqrt{\theta } \).
\end{explanation}

Lastly, we note that the above can be easily generalized.

\begin{remark}
	Consider estimating a scalar parameter \(\theta \) in some open interval \(\Theta \), where we replace:
	\begin{itemize}
		\item \(\sqrt{\theta }\) by \(h(\theta )\), a positive function;\footnote{We don't need continuity since we don't need \(h(T_n)\) when doing the variance stabilizing transformation.}
		\item \(\sqrt{n}\) by \(b_n\), a positive divergent strictly increasing sequence;
		\item \(\overline{X} _n\) by \(T_n\), a \hyperref[def:consistent]{consistent} estimator of \(\theta \).
	\end{itemize}
	In this way, letting \(g^{\prime} (\theta ) h(\theta ) = c > 0\) for all \(\theta \in \Theta \) asserts \(g^{\prime} (\theta ) > 0\) for all \(\theta \in \Theta \), hence \(g\) is strictly increasing and its usual inverse \(g^{-1} \) is well-defined.
\end{remark}

\begin{note}
	Variance stabilizing transformation doesn't have any theoretical guarantees. Rather, it's just our guess that by making the variance independent of \(n\), it'll become better in terms of stability.
\end{note}