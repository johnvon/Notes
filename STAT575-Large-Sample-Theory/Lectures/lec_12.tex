\lecture{12}{22 Feb.\ 9:30}{Asymptotic Joint Distribution by Multivariate CLT}
\begin{proof}
	Set \(\mu = 0\), and it suffices to show that for any \(t \in \mathbb{R} ^d\),
	\[
		t \cdot \left( \frac{1}{\sqrt{n} } \sum_{i=1}^{n} X_i \right)
		\overset{D}{\to} t \cdot Z
		\sim \mathcal{N} (0, t ^{\top} \Sigma t)
	\]
	where \(Z \sim \mathcal{N} (0, \Sigma )\). We see that from the \hyperref[thm:CLT]{univariate central limit theorem}, the left-hand side is
	\[
		\frac{1}{\sqrt{n} } \sum_{i=1}^{n} t \cdot X_i
		\overset{D}{\to} \mathcal{N} (0, \Var_{}[t\cdot X_i] ),
	\]
	and since \(\Var_{}[t \cdot X] = t^{\top} \Var_{}[X_i] t = t ^{\top} \Sigma t = \Var_{}[t \cdot Z] \), hence we're done.
\end{proof}

\subsection{Testing Normality with General Moments}
With \hyperref[thm:multivariate-CLT]{multivariate central limit theorem}, we continue on the problem of finding the asymptotic distribution of \(\widetilde{M} _k = M_k / \hat{\sigma} _n^k\) for general \(k\). Recall the setup, where we let \((X_n)\) and \(X\) be i.i.d.\ random variable, \(Y_i = X_i - \mu \) (and \(Y = X - \mu \)), \(\sigma ^2 = \Var_{}[X] \), \(\mu _k = \mathbb{E}_{}[Y^k] \), and \(\widetilde{\mu} _k = \mu _k / \sigma ^k\). Let's start with \(k = 1\), i.e., compute the asymptotic law of \(\overline{X} _n / \hat{\sigma} _n\). In this case, we have proved the following.

\begin{prev}
	From \autoref{prop:inference-mean} and \autoref{prop:inference-variance},
	\begin{itemize}
		\item \(\sqrt{n} (\overline{X} _n - \mu ) \overset{D}{\to} \mathcal{N} (0, \sigma ^2)\) from \(\sqrt{n} (\overline{X} _n - \mu ) = \frac{1}{\sqrt{n} } \sum_{i=1}^{n} Y_i\), assuming \(X \in L^2\);
		\item \(\sqrt{n} (\hat{\sigma} _n^2 - \sigma ^2) \overset{D}{\to} \mathcal{N} (0, \mu _4 - \sigma ^4) \) from \(\sqrt{n} (\hat{\sigma} _n^2 - \sigma ^2) = \frac{1}{\sqrt{n} } \sum_{i=1}^{n} (Y_i^2 - \sigma ^2) + o_p(1)\), assuming \(X \in L^2\).\footnote{The former asymptotic distribution result needs the assumption of \(X \in L^4\) and \(\widetilde{\mu} _4 > 1\).}
	\end{itemize}
\end{prev}

We see that we have results for \(\overline{X} _n\) and \(\hat{\sigma} _n^2\), but not \(\hat{\sigma} _n\). This is fine since we can
\begin{enumerate}
	\item compute the joint distribution of \(\overline{X} _n\) and \(\hat{\sigma} _n^2\);
	\item apply the \hyperref[thm:delta-method]{delta method} with \(g(\overline{X} _n , \hat{\sigma} _ n^2) \coloneqq \overline{X} _n / \hat{\sigma} _n\) to get the distribution of \(\overline{X} _n / \hat{\sigma} _n\).
\end{enumerate}
Specifically, we have the following.

\begin{proposition}\label{prop:joint-asymptotic-distribution-sample-mean-variance}
	If \(X \in L^2\),
	\[
		\sqrt{n} \left( \begin{pmatrix}
				\overline{X} _n   \\
				\hat{\sigma} _n^2 \\
			\end{pmatrix} - \begin{pmatrix}
				\mu       \\
				\sigma ^2 \\
			\end{pmatrix} \right)
		= \frac{1}{\sqrt{n} } \sum_{i=1}^{n} \begin{pmatrix}
			Y_i               \\
			Y_i^2 - \sigma ^2 \\
		\end{pmatrix} + o_p(1).
	\]
	Moreover, if \(X \in L^4\) and \(\widetilde{\mu} _4 = \mu _4 - \sigma ^4 > 1\), then
	\[
		\sqrt{n} \left( \begin{pmatrix}
				\overline{X} _n   \\
				\hat{\sigma} _n^2 \\
			\end{pmatrix} - \begin{pmatrix}
				\mu       \\
				\sigma ^2 \\
			\end{pmatrix} \right)
		\overset{D}{\to} \mathcal{N} \left( \begin{pmatrix}
			0 \\
			0 \\
		\end{pmatrix}, \Var_{}\left[\begin{pmatrix}
				Y   \\
				Y^2 \\
			\end{pmatrix} \right] \right)
	\]
	by the \hyperref[thm:multivariate-CLT]{multivariate central limit theorem} with
	\[
		\Var_{}\left[\begin{pmatrix}
				Y   \\
				Y^2 \\
			\end{pmatrix} \right]
		= \begin{pmatrix}
			\Var_{}[Y]      & \Cov_{}[Y, Y^2] \\
			\Cov_{}[Y, Y^2] & \Var_{}[Y^2]    \\
		\end{pmatrix}
		= \begin{pmatrix}
			\sigma ^2 & \mu _3             \\
			\mu _3    & \mu _4 - \sigma ^4 \\
		\end{pmatrix}.
	\]
\end{proposition}

\begin{remark}[Asymptotically independent]
	Related back to \autoref{col:asymptotic-distribution-odd-sample-central-moment}, when their \hyperref[def:skewness]{skewness} is \(0\), \(\overline{X} _n\) and \(\hat{\sigma} _n^2\) (or \(s_n^2\)) are asymptotically independent.
\end{remark}

Let's leave the application of the \hyperref[thm:delta-method]{delta method} to the general \(k\). We note the following.

\begin{note}
	The actual characterization of \(\overline{X} _n\) and \(\hat{\sigma} _n^2\) right before applying \hyperref[thm:CLT]{central limit theorem} is much more useful than the final asymptotic distributions.
\end{note}

Next, we compute the asymptotic law of \(\widetilde{M} _k = M_k / \hat{\sigma} _n^k\) for general \(k > 2\). Following a similar calculation, for \(\hat{\sigma} _n^k\), we can again use the result for \(\hat{\sigma} _n^2\), i.e., \autoref{prop:inference-variance}.

\begin{prev}
	From \autoref{thm:asymptotic-distribution-sample-central-moment}, if \(X \in L^{k}\),
	\[
		\sqrt{n} (M_k - \mu _k)
		= \frac{1}{\sqrt{n} } \sum_{i=1}^{n} (Y_i^k - \mu _k - k \mu _{k-1} Y_i) + o_p(1),
	\]
	and \(\sqrt{n} (M_k - \mu _k) \to \mathcal{N} (0, \Var_{}[Y^k - k \mu _{k-1} Y] )\) if \(X \in L^{2k}\) and the variance is strictly positive.
\end{prev}

This implies that for \(X \in L^{k}\) for any \(k > 2\),
\[
	\begin{split}
		Y \coloneqq \sqrt{n} \left( \begin{pmatrix}
				                            \hat{\sigma} _n^2 \\
				                            M_k               \\
			                            \end{pmatrix} - \begin{pmatrix}
				                                            \sigma ^2 \\
				                                            \mu _k    \\
			                                            \end{pmatrix} \right)
		= \frac{1}{\sqrt{n} } & \sum_{i=1}^{n}  \begin{pmatrix}
			                                        Y_i^2 - \sigma ^2                 \\
			                                        Y_i^k - \mu _k - k \mu _{k-1} Y_i \\
		                                        \end{pmatrix} + o_p(1)                                                                                                     \\
		                      & \overset{D}{\to} \mathcal{N} \left( \begin{pmatrix}
				                                                            0 \\
				                                                            0 \\
			                                                            \end{pmatrix} , \begin{pmatrix}
				                                                                            \Var_{}[Y^2]                        & \Cov_{}[Y^2 , Y^k - k \mu _{k-1} Y] \\
				                                                                            \Cov_{}[Y^2 , Y^k - k \mu _{k-1} Y] & \Var_{}[Y^k - k \mu _{k-1} Y]       \\
			                                                                            \end{pmatrix} \right)
	\end{split}
\]
where the \hyperref[thm:multivariate-CLT]{multivariate central limit theorem} can be applied when \(X \in L^{2k}\) and \(\Var_{}[Y^k - k \mu _{k-1} Y] > 0\). This ``\(Y\)'' will be used in the \hyperref[thm:delta-method]{delta method} later.\footnote{This is not exact since \(Y\) should be the random vector corresponding the asymptotic distribution on the right-hand side. But this is fine in the end as we will soon see.}

\begin{remark}
	In general \(k\), if \(\mu _{\ell } = 0\) for all odd \(\ell \), then \(M_k\) and \(\hat{\sigma} _n^2\) are asymptotically independent. This is why we get a simplification for odd case in \autoref{col:asymptotic-distribution-odd-sample-central-moment}.
\end{remark}

Putting everything together formally, we have the following result for general \(k\).

\begin{theorem}\label{thm:asymptotic-distribution-sample-standardized-central-moment}
	Let \(X \in L^{k}\) for some \(k > 2\). Then for \(Z = (X - \mu ) / \sigma = Y / \sigma \),
	\[
		\sqrt{n} (\widetilde{M} _k - \widetilde{\mu} _k)
		= \frac{1}{\sqrt{n} } \sum_{i=1}^{n} \left( -\frac{k}{2} \widetilde{\mu} _k (Z_i^2 - 1) + (Z_i^k - \widetilde{\mu} _k - k \widetilde{\mu} _{k-1} Z_i ) \right) + o_p(1).
	\]
	Moreover, if \(X \in L^{2k}\) and \(\widetilde{v} _k \coloneqq \Var_{}\left[- \frac{k}{2} \widetilde{\mu} _k Z^2 + Z^k - k \widetilde{\mu} _{k-1} Z \right] > 0\), then \(\sqrt{n} (\widetilde{M} _k - \widetilde{\mu} _k) \overset{D}{\to} \mathcal{N} (0, \widetilde{v} _k)\).
\end{theorem}
\begin{proof}
	Since \autoref{prop:inference-variance} is for \(\hat{\sigma} _n^2\) but not \(\hat{\sigma} _n^k\), we need to use \hyperref[thm:delta-method]{delta method} by considering
	\[
		\widetilde{M} _k
		= \frac{M_k}{\hat{\sigma} _n^k}
		\eqqcolon g(\hat{\sigma} _n^2, M_k)
	\]
	where \(g(x, y) = y / x^{k / 2}\) for \(x > 0\), \(y \in \mathbb{R} \). We see that
	\[
		\nabla g(\sigma ^2, \mu _k)
		= \begin{pmatrix}
			-\frac{k}{2} \mu _k \sigma ^{-k - 2} & \sigma ^{-k} \\
		\end{pmatrix}
		= \begin{pmatrix}
			-\frac{k}{2} \widetilde{\mu} _k \sigma ^{-2} & \sigma ^{-k} \\
		\end{pmatrix}
	\]
	since \(\widetilde{\mu} _k = \mu _k / \sigma ^k\), \(\partial g / \partial x = - k y x^{-k / 2 - 1} / 2\), and  \(\partial g / \partial y = x^{-k / 2}\). From \hyperref[thm:delta-method]{delta method} and the above calculation, with \(\widetilde{\mu} _k = g(\sigma ^2, \mu _k)\), we get \(\sqrt{n} (g(\hat{\sigma} _n^2, M_k) - g(\sigma ^2, \mu _k)) \overset{D}{\to } \nabla g Y\), i.e.,
	\[
		\begin{split}
			\sqrt{n} (\widetilde{M} _k - \widetilde{\mu} _k)
			 & = \nabla g(\sigma ^2, \mu _k) \frac{1}{\sqrt{n} } \sum_{i=1}^{n} \begin{pmatrix}
				                                                                    Y_i^2 - \sigma ^2                 \\
				                                                                    Y_i^k - \mu _k - k \mu _{k-1} Y_i \\
			                                                                    \end{pmatrix} + o_p(1)                                                                                             \\
			 & = \frac{1}{\sqrt{n} } \sum_{i=1}^{n} \left( -\frac{k}{2} \widetilde{\mu} _k \frac{1}{\sigma ^2} (Y_i^2 - \sigma ^2) + \frac{1}{\sigma ^k} (Y_i^k - \mu _k - k \mu _{k-1} Y_i) \right) + o_p(1) \\
			 & = \frac{1}{\sqrt{n} } \sum_{i=1}^{n} \left( -\frac{k}{2} \widetilde{\mu} _k (Z_i^2 - 1) + (Z_i^k - \widetilde{\mu} _k - k \widetilde{\mu} _{k-1} Z_i ) \right) + o_p(1)
		\end{split}
	\]
	by letting \(Z_i \coloneqq (X_i - \mu) / \sigma = Y_i / \sigma \). Then by the \hyperref[thm:multivariate-CLT]{multivariate central limit theorem} and \hyperref[col:Slutsky]{Slutsky's theorem}, the above further converges to \(\mathcal{N} (0, \widetilde{v} _k)\) when
	\[
		\widetilde{v} _k
		\coloneqq \Var_{}\left[-\frac{k}{2} \widetilde{\mu} _k (Z^2 - 1) + (Z^k - \widetilde{\mu} _k - k \widetilde{\mu} _{k-1} Z ) \right]
		= \Var_{}\left[-\frac{k}{2} \widetilde{\mu} _k Z^2 + Z^k - k \widetilde{\mu} _{k-1} Z \right] > 0,
	\]
	as we assumed.
\end{proof}

Compared to the last time  when we do this for odd \(k\) and \(\mu _k = 0\) (\autoref{col:asymptotic-distribution-odd-sample-central-moment}), we only get an asymptotic distribution, not an explicit decomposition.

\begin{note}
	It's more convenient to use \hyperref[thm:delta-method]{delta method} in this way, i.e., not use the actual \(Y\) corresponding to the distribution, but use the term in the limiting sequence with \(o_p(1)\).
\end{note}

With this explicit formula, we now see one example.

\begin{eg}
	Consider using both \(\widetilde{M} _3\) and \(\widetilde{M} _4\) to test \(H_0\colon X \sim \mathcal{N} \). We see that under \(H_0\),
	\[
		\left( \sqrt{\frac{n}{\widetilde{v} _3}} \widetilde{M} _3 \right) ^2 + \left( \sqrt{\frac{n}{\widetilde{v} _4}} (\widetilde{M} _4 - \widetilde{\mu} _4) \right) ^2
		\overset{D}{\to} \chi _2^2 .
	\]
\end{eg}
\begin{explanation}
	One can write down \(\sqrt{n} (\widetilde{M} _\ell - \widetilde{\mu} _\ell )\) for even \(\ell \), and also \(\sqrt{n} (\widetilde{M} _k - \widetilde{\mu} _k)= \sqrt{n} \widetilde{M} _k\) for odd \(k\), and see that while they both converge to \(\mathcal{N} (0, 1)\), their covariance is \(0\), i.e., asymptotically independent, so the square of them add up to \(\chi _2^2\).
\end{explanation}

Generalizing the above example, for any \(X\) with \(k > 1\) odd and \(\ell > 2\) even, such that every odd central moments vanish with \(\widetilde{v} _k , \widetilde{v} _\ell < \infty \),
\[
	\frac{n}{\widetilde{v} _k} \widetilde{M} _k^2 + \frac{n}{\widetilde{v} _\ell }(\widetilde{M} _\ell - \widetilde{\mu} _\ell )^2 \overset{D}{\to} \chi _2^2
\]
from exactly the same argument.

\subsection{Asymptotic Relative Efficiency}
Assume \(X_1, \dots , X_n \overset{\text{i.i.d.} }{\sim } \operatorname{Pois}(\theta ) \). To estimate \(\theta \), as \(\theta = \mathbb{E}_{}[X] = \Var_{}[X] \), two natural estimators are \(\overline{X} _n\) and \(\hat{\sigma} _n^2\). To compare them, we see that
\begin{itemize}
	\item \(\sqrt{n} (\overline{X} _n - \theta ) \overset{D}{\to} \mathcal{N} (0, \sigma ^2)\);
	\item \(\sqrt{n} (\hat{\sigma} _n^2 - \theta ) \overset{D}{\to} \mathcal{N} (0, \mu _4 - \sigma ^4)\).
\end{itemize}
As \(\sigma ^2 = \theta \) and \(\mu _4 = 3 \theta ^2 + \theta \), we see that \(\overline{X} _n\) is better since its variance is smaller. To further quantify how much better is it, we can ask how many data we need such that we get a similar precision. Consider
\[
	\sqrt{n} (T_n^i - \theta )
	\overset{D}{\to} \mathcal{N} (0, \sigma _i^2(\theta ))
\]
for two statistics \(T_i\), \(i = 1, 2\). This implies
\[
	\mathbb{P} \left( \theta \in T_n^i \pm Z_{\alpha / 2} \frac{\sigma _i(\theta )}{\sqrt{n} } \right) \cong 1 - \alpha .
\]
Let \(I_n^{(i)} \coloneqq T_n^i \pm Z_{\alpha / 2} \sigma _i(\theta ) / \sqrt{n} \), and let \(n_i\) be the value of \(n\) such that \(\vert I_n^{(i)} \vert = \gamma \),
\[
	\gamma = 2 Z_{\alpha / 2} \frac{\sigma _i(\theta )}{\sqrt{n_i} }
	\implies n_i = \left( \frac{2 Z_{\alpha / 2}}{\gamma } \sigma _i(\theta ) \right)^2 ,
\]
i.e., \(n_1 / n_2 = \sigma _1(\theta )^2 / \sigma _2(\theta )^2\). We called this the \hyperref[def:asymptotic-relative-efficiency]{asymptotic relative efficiency} \(\operatorname{ARE}(T^1, T^2) \).

\begin{definition}[Asymptotic relative efficiency]\label{def:asymptotic-relative-efficiency}
	The \emph{asymptotic relative efficiency} between two statistics \(T^1_n\) and \(T^2_n\) for \(\theta \) such that \(\sqrt{n} (T_n^i - \theta ) \overset{D}{\to} \mathcal{N} (0, \sigma _i^2(\theta ))\) is defined as
	\[
		\operatorname{ARE}(T^1, T^2) = \frac{\sigma _1(\theta )^2}{\sigma _2(\theta )^2}.
	\]
\end{definition}

\subsection{Variance Stabilizing Transformation}
Now, say we use \(\overline{X} _n\) as the estimator of \(\theta \). We have \(\sqrt{n} (\overline{X} _n - \theta ) \overset{D}{\to} \sqrt{\theta } \mathcal{N} (0, 1) = \mathcal{N} (0, \theta )\).

\begin{note}
	As the asymptotic distribution depends on \(\theta \), we don't directly get a confidence interval.
\end{note}

To get around this, we first write
\[
	\sqrt{n} (\overline{X} _n - \theta )
	\overset{D}{\to} \sqrt{\theta } Z \sim \mathcal{N} (0, \theta )
\]
for \(Z \sim \mathcal{N} (0, 1)\). Then, instead of writing this as
\[
	\frac{\sqrt{n} }{\sqrt{\theta } } (\overline{X} _n - \theta ) \overset{D}{\to} Z
\]
and replace \(\sqrt{\theta } \) on by some estimator, observe that from \hyperref[thm:delta-method]{delta method} with some \(g\),
\[
	\sqrt{n} (g(\overline{X} _n) - g(\theta )) \overset{D}{\to} g^{\prime} (\theta ) \sqrt{\theta } Z
\]
Suppose \(g^{\prime} (\theta ) \sqrt{\theta } = c \) is some constant for every \(\theta > 0\), our goal is also achieved, i.e.,
\[
	\frac{\sqrt{n}}{c} (g(\overline{X} _n) - g(\theta )) \overset{D}{\to} \mathcal{N} (0, 1).
\]

\begin{claim}
	There exists \(g\) such that \(c = 1 / 2\).
\end{claim}
\begin{explanation}
	Since for \(g^{\prime} (\theta ) = \frac{1}{2 \sqrt{\theta } }\), we have \(g(\theta ) = \sqrt{\theta } \).
\end{explanation}

Then, the asymptotic confidence interval for \(g(\theta )\) with confidence level \(1 - \alpha \) is just
\[
	\left( g(\overline{X} _n) - Z_{\alpha / 2} \frac{c}{\sqrt{n} } , g(\overline{X} _n) + Z_{\alpha / 2} \frac{c}{\sqrt{n} } \right),
\]
and hence an asymptotic confidence interval for \(\theta \) with confidence level \(1 - \alpha \) is just
\[
	\left( g^{-1} \left( g(\overline{X} _n) - Z_{\alpha / 2} \frac{c}{\sqrt{n} } \right) , g^{-1} \left( g(\overline{X} _n) + Z_{\alpha / 2} \frac{c}{\sqrt{n} } \right) \right),
\]
In our case, \(g^{-1} (u) = u^2\). This is the so-called \emph{variance stabilizing transformation}, and can be easily generalized.