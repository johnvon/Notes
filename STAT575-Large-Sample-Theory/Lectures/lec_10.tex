\chapter{Fundamental Theorems of Probability}
\lecture{10}{15 Feb.\ 9:30}{Law of Large Number and Central Limit Theorem}
With the tools we developed, we can now prove the fundamental theorems of probability and see some applications to inferences.

\section{The Law of Large Number and The Central Limit Theorem}
In this section, we will study the \hyperref[thm:WLLN]{weak law of large number} and the \hyperref[thm:CLT]{central limit theorem}.

\subsection{Weak Law of Large Number}
The first result, the \hyperref[thm:WLLN]{weak law of large number}, states that the sample mean \hyperref[def:converge-in-probability]{converges} to the mean.

\begin{theorem}[Khintchin's weak law of large number]\label{thm:WLLN}
	Let \(X\) and \((X_n)\) be i.i.d.\ random vectors with \(X \in L^1\), i.e., \(\mathbb{E}_{}[\vert X \vert ] < \infty \). Then \(\overline{X} _n \overset{p}{\to } \mathbb{E}_{}[X] \).
\end{theorem}
\begin{proof}
	Since \(c \coloneqq \mathbb{E}_{}[X] \) is a constant, it suffices to show that \(\phi _{\overline{X} _n}(t) \to \phi _{c} (t) = e^{itc}\) for all \(t\) from \autoref{col:weak-probability-constant}. Firstly, let \(\overline{X} _n = S_n / n\), we have
	\[
		\phi _{\overline{X} _n}(t)
		= \mathbb{E}_{}[e^{it S_n / n}]
		= \phi _{S_n}(t / n)
		= \prod_{i=1}^{n} \phi _{X_i}(t / n)
		\eqqcolon \left( \phi (t / n) \right) ^n
	\]
	where we let \(\phi _{X_i} \eqqcolon \phi \) since \((X_n)\) are i.i.d. From the fundamental theorem of calculus, with the fact that the first moment of \(X\) exists, \(\phi \) is differentiable such that
	\[
		\left( \phi (t / n) \right) ^n
		= \left( 1 + \frac{t}{n} \int_{0}^{1} \phi ^{\prime} (u t / n) \,\mathrm{d}u  \right) ^n .
	\]
	Since \((1 + a_n)^n \to e^c\) if \(n a_n \to c\), it remains to show \(\int_{0}^{1} \phi ^{\prime} (u t / n) \,\mathrm{d}u \to ic\). First, \(\phi ^{\prime} (t)\) is continuous at \(0\) from \autoref{col:characteristic-function-derivative-uniform-continuous},\footnote{We see that assuming \(\phi \) is differentiable at \(0\) such that \(\phi ^{\prime} (0) = ic\) is enough.} as \(n \to \infty \)
	\[
		\phi ^{\prime} (u t / n) \to \phi ^{\prime} (0) = i \mathbb{E}_{}[X] = ic.
	\]
	With the fact that \(\sup _t \vert \phi ^{\prime} (t) \vert \leq \mathbb{E}_{}[\vert X \vert ] \), the \href{https://en.wikipedia.org/wiki/Dominated_convergence_theorem}{bounded convergence theorem} implies
	\[
		\int_{0}^{1} \phi ^{\prime} (ut / n) \,\mathrm{d}u
		\to \int_{0}^{1} ic \,\mathrm{d}u
		= ic
	\]
	since we can now pass the limit inside the integral.
\end{proof}

Although we will not show, but the stronger version holds, i.e., it \hyperref[def:converge-almost-surely]{converges almost surely}.

\begin{theorem}[Strong law of large number]\label{thm:SLLN}
	Let \(X\) and \((X_n)\) be i.i.d.\ random vectors with \(X \in L^1\). Then \(\overline{X} _n \overset{\text{a.s.} }{\to } \mathbb{E}_{}[X] \).
\end{theorem}

\subsection{Central Limit Theorem}
In terms of the distributional result, we need higher-order moments. In particular, if the second moment exists, then we can generalize we have done as in the proof of \autoref{thm:characteristic-function-derivative}.

\begin{prev}
	If \(g\) is continuously differentiable at \(0\), then for \(x\) around \(0\),
	\[
		g(x)
		= g(0) + g^{\prime} (0) x + x \int_{0}^{1} g^{\prime} (ux) - g^{\prime} (0) \,\mathrm{d}u .
	\]
\end{prev}

\begin{note}\label{note:lec10}
	If in addition, \(g^{\prime} \) is also continuously differentiable at \(0\), then for \(x\) around \(0\),
	\begin{align*}
		g(x)
		 & = g(0) + g^{\prime} (0) x + x \int_{0}^{1} \int_{0}^{ux} g^{\prime\prime} (y) \,\mathrm{d}y \,\mathrm{d} u                                                               \\
		 & = g(0) + g^{\prime} (0) x + x^2 \int_{0}^{1} \int_{0}^{1} g^{\prime\prime} (xuv) u \,\mathrm{d}v \,\mathrm{d} u. \tag*{\(y = ux v\), \(\mathrm{d} y = ux \mathrm{d} v\)} \\
	\end{align*}
\end{note}

We now state the theorem.

\begin{theorem}[Lindeberg-Lévy central limit theorem]\label{thm:CLT}
	Let \((X_n)\) be i.i.d.\ random variables (i.e., \(d = 1\)) with \(\mathbb{E}_{}[X_i] \eqqcolon \mu \), \(\Var_{}[X_i] \eqqcolon \sigma ^2 < \infty \) for all \(1 \leq i \leq n\). Then
	\[
		\frac{\overline{X} _n - \mu }{\sigma / \sqrt{n} } \overset{D}{\to } \mathcal{N} (0, 1).
	\]
\end{theorem}
\begin{proof}
	Without loss of generality, let \(\mu = 0\), \(\sigma = 1\). Since \(\frac{\overline{X} _n - \mu }{\sigma / \sqrt{n} } = \frac{S_n - n \mu }{\sigma \sqrt{n} }\), it's enough to show that \(\phi _{S_n / \sqrt{n} }(t) \to e^{-t^2 / 2}\) for any \(t \in \mathbb{R} \) from \hyperref[thm:Levy-Cramer-continuity]{Lévy-Cramer continuity theorem} and \autoref{eq:normal-characteristic}. Firstly,
	\[
		\phi _{S_n / \sqrt{n} } (t)
		= \mathbb{E}_{}[e^{i t S_n / \sqrt{n} }]
		= \phi _{S_n} (t / \sqrt{n} )
		= \left( \phi (t / \sqrt{n} ) \right) ^n
	\]
	where we let \(\phi _{X_n} \eqqcolon \phi \) since \((X_n)\) are i.i.d. By applying the above \hyperref[note:lec10]{note}, we further have
	\[
		\begin{split}
			\left( \phi (t / \sqrt{n} ) \right) ^n
			 & = \left( \phi (0) + \phi ^{\prime} (0) \frac{t}{\sqrt{n} } + \frac{t^2}{n} \int_{0}^{1} \int_{0}^{1} u \phi ^{\prime\prime} (u v t / \sqrt{n} ) \,\mathrm{d}u  \,\mathrm{d}v  \right) ^n \\
			 & = \left( 1 + \frac{t^2}{n}\int_{0}^{1} \int_{0}^{1} u \phi ^{\prime\prime} (u v t / \sqrt{n} ) \,\mathrm{d}u  \,\mathrm{d}v \right) ^n
		\end{split}
	\]
	since \(\phi (0) = 1\) and \(\phi ^{\prime} (0) = i \mu = 0\). It remains to show that the double integral converges to \(- 1 / 2\) since it'll imply \((\phi (t / \sqrt{n} ))^n \to e^{-t^2 / 2}\). We see that as \(n \to \infty \), the integrand
	\[
		u \phi ^{\prime\prime} (u v t / \sqrt{n} )
		\to u \phi ^{\prime\prime} (0)
		= u (i^2 \mathbb{E}_{}[X^2])
		= - u (\Var_{}[X] + (\mathbb{E}_{}[X] )^2 )
		= - u (1 + 0)
		= -u.
	\]
	Hence, from the \href{https://en.wikipedia.org/wiki/Dominated_convergence_theorem}{bounded convergence theorem},
	\[
		\int_{0}^{1} \int_{0}^{1} u \phi ^{\prime\prime} (u t / \sqrt{n} ) \,\mathrm{d}u \,\mathrm{d}v
		\to \int_{0}^{1} \int_{0}^{1} -u \,\mathrm{d}u \,\mathrm{d}v
		= -\frac{1}{2},
	\]
	which shows the result.
\end{proof}

\begin{remark}
	From the \hyperref[thm:CLT]{central limit theorem}, we can indeed deduce the \hyperref[thm:WLLN]{weak law of large number}. But since the former requires more conditions, hence \hyperref[thm:WLLN]{weak law of large number} still has its own merit.
\end{remark}

\section{Inference for Population Mean and Variance}
We now apply what we proved to the problem of inferences.

\subsection{Population Mean}
Firstly, let's consider the applications for mean estimation. Let \(X, X_1, \dots , X_n\) be i.i.d.\ samples such that \(\mathbb{E}_{}[X] = \mu < \infty \), \(\Var_{}[X] = \sigma ^2\). If, also, \(X_i\)'s are Gaussian, \(\overline{X} _n \sim \mathcal{N} (\mu , \sigma ^2 / n)\), i.e.,
\[
	\frac{\overline{X} _n - \mu }{\sigma / \sqrt{n} } \sim \mathcal{N} (0, 1),
\]
In this case, a natural estimator of \(\mu \) is \(\overline{X} _n\), and we have the distribution of \(\overline{X} _n - \mu \), i.e., we know how our estimator perform in terms of distribution, which can in turn provides a confidence interval.

\begin{intuition}
	We want to make the distribution, specifically, its variance (denominator at the left-hand side) independent of parameters to get a corresponding confidence interval.
\end{intuition}

Right now, our confidence interval depends on \(\sigma \). To solve this, consider replacing \(\sigma \) by the sample standard deviation \(s_n\), then
\[
	T_n \coloneqq \frac{\overline{X} _n - \mu }{s_n / \sqrt{n} } \sim t_{n-1} \overset{\operatorname{TV}}{\to } \mathcal{N} (0, 1)
\]
as \(n \to \infty \), where \(T_n\) follows \(t\)-distribution with \(n-1\) degrees of freedom.

\begin{notation}
	We let \(s_n^2 \coloneqq \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \overline{X} _n)^2\) and \(\hat{\sigma} _n^2 \coloneqq \frac{1}{n} \sum_{i=1}^{n} (X_i - \overline{X} _n)^2\).
\end{notation}

We see that when \(X\) is Gaussian, an asymptotically valid \(100 (1 - \alpha )\%\) confidence interval for \(\mu \) is
\[
	\overline{X} \pm Z_{\alpha / 2} \frac{s_n}{\sqrt{n} }.
\]
The first question we will address is that, what if \(X_i\)'s are not Gaussian, and can we replace \(s_n\) by \(\hat{\sigma} _n\).

\begin{proposition}\label{prop:inference-mean}
	If \(X \in L^2\), then \(\hat{\sigma} _n^2\) and \(s_n^2\) are both \hyperref[def:consistent]{consistent estimators} of \(\sigma ^2\). Furthermore, \(T_n \overset{D}{\to} \mathcal{N} (0, 1)\), and the same holds if \(s_n\) is replaced by \(\hat{\sigma} _n\) in the definition of \(T_n\).
\end{proposition}
\begin{proof}
	Indeed, by letting \(Y_i \coloneqq X_i - \mu \) for all \(i\) (and also \(Y = X - \mu \)),
	\[
		\hat{\sigma} _n^2
		= \frac{1}{n} \sum_{i=1}^{n} \left( X_i - \overline{X} _n\right) ^2
		= \frac{1}{n} \sum_{i=1}^{n} \left( Y_i - \overline{Y} _n\right) ^2
		= \frac{1}{n} \sum_{i=1}^{n} Y_i^2 - (\overline{Y} _n)^2
		\overset{p}{\to} \sigma ^2 + 0
	\]
	as \(n \to \infty \) since \(\frac{1}{n} \sum_{i=1}^{n} Y_i^2 \overset{p}{\to } \mathbb{E}_{}[Y^2] = \Var_{}[X] = \sigma ^2\),\footnote{We need to check whether the first moment exists.} and \((\overline{Y} _n)^2 \overset{p}{\to } (\mathbb{E}_{}[Y])^2 = 0\), both from \hyperref[thm:WLLN]{weak law of large number}. This implies that \(s_n^2\) is also a \hyperref[def:consistent]{consistent estimator} of \(\sigma ^2\) since
	\[
		s_n^2 = \frac{n}{n-1} \hat{\sigma} _n^2 \overset{p}{\to} 1 \cdot \sigma ^2 = \sigma ^2,
	\]
	again from \hyperref[col:Slutsky]{Slutsky's theorem}. The distributional result follows directly from \hyperref[thm:CLT]{central limit theorem} for \(\frac{\overline{X} _n - \mu }{\sigma / \sqrt{n} } \overset{D}{\to} \mathcal{N} (0, 1)\) and \hyperref[col:Slutsky]{Slutsky's theorem}.
\end{proof}

\autoref{prop:inference-mean} says that for mean estimation, even if the data is not Gaussian, we're fine.

\begin{corollary}\label{col:inference-mean-CI}
	If \(X \in L^2\), then \(\overline{X} _n \pm Z_{\alpha / 2} s_n / \sqrt{n} \) and \(\overline{X} _n \pm Z_{\alpha / 2} \hat{\sigma} _n / \sqrt{n} \) are both asymptotically valid \(100 (1 - \alpha )\%\) confidence intervals for \(\mu \).
\end{corollary}

\subsection{Population Variance}
Next, let's consider variance estimation and further assume that \(\sigma ^2 < \infty \). Again, let \(X, X_1, \dots , X_n\) be i.i.d.\ Gaussian random samples,
\[
	(n-1) \frac{s_n^2}{\sigma ^2}
	\overset{D}{=} \sum_{i=1}^{n-1} Z_i^2
\]
where \((Z_{n-1}) \overset{\text{i.i.d.} }{\sim } \mathcal{N} (0, 1)\). Firstly, since \(\mathbb{E}_{}[Z_i^2] = \Var_{}[Z_i] + (\mathbb{E}_{}[Z_i] )^2 = 1\), and \(\Var_{}[Z_i^2] = \mathbb{E}_{}[Z_i^4] - (\mathbb{E}_{}[Z_i^2] )^2 = 3 - 1 = 2\). Standardizing, from the normal approximation to the chi-square distribution,
\[
	\frac{(n-1) \frac{s_n^2}{\sigma ^2} - (n - 1)}{\sqrt{2 (n-1)} }
	\overset{D}{=} \frac{\sum_{i=1}^{n-1} Z_i^2 - (n-1)}{\sqrt{2 (n-1)} }
	\overset{D}{\to } \mathcal{N} (0, 1),
\]
i.e., as \(n \to \infty \),
\[
	\sqrt{n-1} \left( \frac{s_n^2}{\sigma ^2} - 1 \right)  \overset{D}{\to } \mathcal{N} (0, 2)
	\iff \sqrt{n} \left( \frac{s_n^2}{\sigma ^2} - 1 \right)  \overset{D}{\to } \mathcal{N} (0, 2)
	\iff \sqrt{n} (s_n^2 - \sigma ^2) \overset{D}{\to } \mathcal{N} (0, 2 \sigma ^4),
\]
and an asymptotically valid \(100 (1 - \alpha )\%\) confidence interval for \(\sigma ^2\) is
\[
	\frac{s_n^2}{1 \pm Z_{\alpha / 2} \sqrt{2 / n} }.
\]
Let's again ask what will happen when \(X_i\)'s are not Gaussian anymore.

\begin{proposition}\label{prop:inference-variance}
	If \(X \in L^2\), then the following hold when \(\hat{\sigma} _n^2\) is replaced by \(s_n^2\). Firstly,
	\[
		\sqrt{n} (\hat{\sigma} _n^2 - \sigma ^2)
		= \frac{1}{\sqrt{n} } \sum_{i=1}^{n} (Y_i^2 - \sigma ^2) + o_p(1).
	\]
	Moreover, if \(X \in L^4\) and \(\mathbb{E}_{}[( (X-\mu ) / \sigma ) ^4 ] > 1\), then \(\sqrt{n} (\hat{\sigma} _n^2 - \sigma ^2) \overset{D}{\to} \mathcal{N} (0, \mathbb{E}_{}[(X-\mu )^4 ] - \sigma ^4)\).
\end{proposition}
\begin{proof}
	We see that from the same calculation as above, with \(Y_i \coloneqq X_i - \mu \) (and also \(Y = X - \mu \)),
	\[
		\begin{split}
			\hat{\sigma} _n^2 - \frac{\hat{\sigma} _n^2}{n} = \frac{1}{n} \sum_{i=1}^{n} Y_i^2 - \overline{Y} _n^2
			\implies & \hat{\sigma} _n^2 - \sigma ^2 - \frac{\hat{\sigma} _n^2}{n} = \frac{1}{n} \sum_{i=1}^{n} (Y_i^2 - \sigma ^2)- \overline{Y} _n^2                                                           \\
			\implies & \sqrt{n}(\hat{\sigma } _n^2 - \sigma ^2) = \frac{1}{\sqrt{n} } \sum_{i=1}^{n} (Y_i^2 - \sigma ^2) - \frac{(\sqrt{n} \overline{Y} _n)^2}{\sqrt{n} } + \frac{\hat{\sigma} _n^2}{\sqrt{n} }.
		\end{split}
	\]
	As \(n \to \infty \), \(\hat{\sigma} _n^2 / \sqrt{n} \overset{p}{\to } 0\), with the fact that \(\sqrt{n} \overline{Y} _n\) \hyperref[def:converge-in-distribution]{converges in distribution} from the \hyperref[thm:CLT]{central limit theorem}, it's also \hyperref[def:bounded-in-probability]{bounded in probability} from \autoref{prop:convergence-in-distirbution-bounded-in-probability}, hence
	\[
		\frac{(\sqrt{n} \overline{Y} _n) ^2}{\sqrt{n} }
		= o(1) O_p(1)
		= o_p(1).
	\]
	This proves the first claim. Now, from the \hyperref[thm:CLT]{central limit theorem}, assuming \(\Var_{}[Y_i^2] < \infty \), we have
	\[
		\frac{1}{\sqrt{n} } \sum_{i=1}^{n} (Y_i^2 - \sigma ^2)
		= \frac{1}{\sqrt{n} } \sum_{i=1}^{n} (Y_i^2 - \mathbb{E}_{}[Y_i^2] )
		\overset{D}{\to } \mathcal{N} (0, \Var_{}[Y_i^2] ),
	\]
	which implies \(\sqrt{n} (\hat{\sigma} _n^2 - \sigma ^2) \overset{D}{\to } \mathcal{N} (0, \Var_{}[Y^2] )\) from what we have shown, where
	\[
		\Var_{}[Y^2]
		= \mathbb{E}_{}[(X - \mu )^4]  - \left( \mathbb{E}_{}[(X-\mu )^2] \right) ^2
		= \sigma ^4 \mathbb{E}_{}\left[\left( \frac{X-\mu }{\sigma } \right) ^4 \right] - \sigma ^4
		= \sigma ^4 \left( \mathbb{E}_{}\left[\left( \frac{X-\mu }{\sigma } \right) ^4 \right] - 1 \right),
	\]
	which proves the result with our assumption. Finally, we note that
	\[
		\sqrt{n} (\hat{\sigma} _n^2 - s_n^2) = \frac{\sqrt{n} }{n-1} \hat{\sigma} _n^2 \overset{p}{\to} 0 \cdot \sigma ^2 = 0,
	\]
	hence the same results holds for replacing \(\hat{\sigma} _n^2\) by \(s_n^2\) as well.
\end{proof}

The quantity (and a related one) in our assumption deserves a special name.

\begin{definition}[Kurtosis]\label{def:Kurtosis}
	The \emph{Kurtosis} of a random variable \(X\) is defined as \(\mathbb{E}_{}[( (X-\mu ) / \sigma )^4]\).
\end{definition}

\begin{definition}[Skewness]\label{def:skewness}
	The \emph{skewness} of a random variable \(X\) is defined as \(\mathbb{E}_{}[( (X-\mu ) / \sigma )^3]\).
\end{definition}

\begin{eg}[Kurtosis for Gaussian]
	The \hyperref[def:Kurtosis]{Kurtosis} of the standard Gaussian is \(3\).
\end{eg}

Let \(Z = (X-\mu ) / \sigma \), we note that \autoref{prop:inference-variance} requires \(\mathbb{E}_{}[Z^4] > 1\). However, from Jensen's inequality, \(\mathbb{E}_{}[Z^4] \geq \left( \mathbb{E}_{}[Z^2] \right) ^2 \geq 1\), hence indeed, the assumption might not be true in general.

\begin{eg}
	If \(\mathbb{E}_{}[Z^4] = 1\),
	\[
		\Var_{}[Y^2] = 0
		\iff \mathbb{P} (Y^2 = \mathbb{E}_{}[Y^2] ) = 1
		\iff \mathbb{P} (Y = \pm \sigma ) = 1
		\iff \mathbb{P} (X = \mu \pm \sigma ) = 1,
	\]
	i.e., the violation might happen for \(X\) being concentrated on two points.
\end{eg}

The takeaway is when \(X\) is not a normal, and if the \hyperref[def:Kurtosis]{Kurtosis} of \(X\) is different from the normal, then the distribution of \(\sqrt{n} (\hat{\sigma} _n^2 - \sigma ^2)\) is different. Specifically, if the \hyperref[def:Kurtosis]{Kurtosis} exists and is not equal to \(1\), then an asymptotically valid \(100 (1 - \alpha )\%\) confidence interval for \(\sigma ^2\) is
\[
	\frac{\hat{\sigma} _n^2}{1 \pm Z_{\alpha / 2} \sqrt{(\mathbb{E}_{}[((X - \mu ) / \sigma )^4]  - 1) / n} }.
\]
However, if we don't know the \hyperref[def:Kurtosis]{Kurtosis} of \(X\), we can't say anything about the confidence interval.

\begin{intuition}
	By \hyperref[thm:Slutsky]{Slutsky's theorem}, if we have a \hyperref[def:consistent]{consistent estimator} of the \hyperref[def:Kurtosis]{Kurtosis}, we can then use it instead and get a desired asymptotic confidence interval.
\end{intuition}